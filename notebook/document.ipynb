{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd6f5cb",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1333692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc3d4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'author': 'Manya Sharma', 'date_created': '2025-10-17', 'pages': 1}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the content of the document.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"author\": \"Manya Sharma\",\n",
    "        \"date_created\": \"2025-10-17\",\n",
    "        \"pages\":1}\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9a4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4791a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72864eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### textloader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8ebe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\", \n",
    "    glob=\"**/*.txt\",  ##pattern to match files\n",
    "    loader_cls=TextLoader,  ##loader class to use\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress= False\n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e146ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 0}, page_content='Jake VanderPlas\\nPython \\nData Science \\nHandbook\\nESSENTIAL TOOLS FOR WORKING WITH DATA\\npowered by\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 1}, page_content='www.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 2}, page_content='Jake VanderPlas\\nPython Data Science Handbook\\nEssential Tools for Working with Data\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 3}, page_content='978-1-491-91205-8\\n[LSI]\\nPython Data Science Handbook\\nby Jake VanderPlas\\nCopyright © 2017 Jake VanderPlas. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐\\ntutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Dawn Schanafelt\\nProduction Editor: Kristen Brown\\nCopyeditor: Jasmine Kwityn\\nProofreader: Rachel Monaghan\\nIndexer: WordCo Indexing Services, Inc.\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nDecember 2016:\\n First Edition\\nRevision History for the First Edition\\n2016-11-17: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491912058 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Python Data Science Handbook, the\\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 4}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\n1. IPython: Beyond Normal Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nShell or Notebook?                                                                                                             2\\nLaunching the IPython Shell                                                                                        2\\nLaunching the Jupyter Notebook                                                                                 2\\nHelp and Documentation in IPython                                                                             3\\nAccessing Documentation with ?                                                                                 3\\nAccessing Source Code with ??                                                                                     5\\nExploring Modules with Tab Completion                                                                  6\\nKeyboard Shortcuts in the IPython Shell                                                                       8\\nNavigation Shortcuts                                                                                                      8\\nText Entry Shortcuts                                                                                                      9\\nCommand History Shortcuts                                                                                       9\\nMiscellaneous Shortcuts                                                                                              10\\nIPython Magic Commands                                                                                            10\\nPasting Code Blocks: %paste and %cpaste                                                               11\\nRunning External Code: %run                                                                                   12\\nTiming Code Execution: %timeit                                                                               12\\nHelp on Magic Functions: ?, %magic, and %lsmagic                                              13\\nInput and Output History                                                                                              13\\nIPython’s In and Out Objects                                                                                      13\\nUnderscore Shortcuts and Previous Outputs                                                           15\\nSuppressing Output                                                                                                     15\\nRelated Magic Commands                                                                                          16\\nIPython and Shell Commands                                                                                       16\\nQuick Introduction to the Shell                                                                                 16\\nShell Commands in IPython                                                                                      18\\niii\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 5}, page_content='Passing Values to and from the Shell                                                                         18\\nShell-Related Magic Commands                                                                                   19\\nErrors and Debugging                                                                                                     20\\nControlling Exceptions: %xmode                                                                              20\\nDebugging: When Reading Tracebacks Is Not Enough                                          22\\nProfiling and Timing Code                                                                                             25\\nTiming Code Snippets: %timeit and %time                                                             25\\nProfiling Full Scripts: %prun                                                                                      27\\nLine-by-Line Profiling with %lprun                                                                          28\\nProfiling Memory Use: %memit and %mprun                                                        29\\nMore IPython Resources                                                                                                 30\\nWeb Resources                                                                                                              30\\nBooks                                                                                                                              31\\n2. Introduction to NumPy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  33\\nUnderstanding Data Types in Python                                                                           34\\nA Python Integer Is More Than Just an Integer                                                       35\\nA Python List Is More Than Just a List                                                                     37\\nFixed-Type Arrays in Python                                                                                     38\\nCreating Arrays from Python Lists                                                                            39\\nCreating Arrays from Scratch                                                                                     39\\nNumPy Standard Data Types                                                                                     41\\nThe Basics of NumPy Arrays                                                                                          42\\nNumPy Array Attributes                                                                                             42\\nArray Indexing: Accessing Single Elements                                                             43\\nArray Slicing: Accessing Subarrays                                                                            44\\nReshaping of Arrays                                                                                                     47\\nArray Concatenation and Splitting                                                                            48\\nComputation on NumPy Arrays: Universal Functions                                              50\\nThe Slowness of Loops                                                                                                50\\nIntroducing UFuncs                                                                                                     51\\nExploring NumPy’s UFuncs                                                                                        52\\nAdvanced Ufunc Features                                                                                           56\\nUfuncs: Learning More                                                                                                58\\nAggregations: Min, Max, and Everything in Between                                                58\\nSumming the Values in an Array                                                                               59\\nMinimum and Maximum                                                                                           59\\nExample: What Is the Average Height of US Presidents?                                       61\\nComputation on Arrays: Broadcasting                                                                         63\\nIntroducing Broadcasting                                                                                           63\\nRules of Broadcasting                                                                                                  65\\nBroadcasting in Practice                                                                                              68\\niv \\n| \\nTable of Contents\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 6}, page_content='Comparisons, Masks, and Boolean Logic                                                                     70\\nExample: Counting Rainy Days                                                                                 70\\nComparison Operators as ufuncs                                                                              71\\nWorking with Boolean Arrays                                                                                    73\\nBoolean Arrays as Masks                                                                                             75\\nFancy Indexing                                                                                                                 78\\nExploring Fancy Indexing                                                                                           79\\nCombined Indexing                                                                                                     80\\nExample: Selecting Random Points                                                                           81\\nModifying Values with Fancy Indexing                                                                    82\\nExample: Binning Data                                                                                                83\\nSorting Arrays                                                                                                                  85\\nFast Sorting in NumPy: np.sort and np.argsort                                                       86\\nPartial Sorts: Partitioning                                                                                            88\\nExample: k-Nearest Neighbors                                                                                   88\\nStructured Data: NumPy’s Structured Arrays                                                              92\\nCreating Structured Arrays                                                                                         94\\nMore Advanced Compound Types                                                                            95\\nRecordArrays: Structured Arrays with a Twist                                                        96\\nOn to Pandas                                                                                                                 96\\n3. Data Manipulation with Pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97\\nInstalling and Using Pandas                                                                                           97\\nIntroducing Pandas Objects                                                                                           98\\nThe Pandas Series Object                                                                                            99\\nThe Pandas DataFrame Object                                                                                 102\\nThe Pandas Index Object                                                                                          105\\nData Indexing and Selection                                                                                        107\\nData Selection in Series                                                                                             107\\nData Selection in DataFrame                                                                                    110\\nOperating on Data in Pandas                                                                                       115\\nUfuncs: Index Preservation                                                                                      115\\nUFuncs: Index Alignment                                                                                         116\\nUfuncs: Operations Between DataFrame and Series                                            118\\nHandling Missing Data                                                                                                 119\\nTrade-Offs in Missing Data Conventions                                                               120\\nMissing Data in Pandas                                                                                             120\\nOperating on Null Values                                                                                          124\\nHierarchical Indexing                                                                                                   128\\nA Multiply Indexed Series                                                                                         128\\nMethods of MultiIndex Creation                                                                             131\\nIndexing and Slicing a MultiIndex                                                                          134\\nTable of Contents \\n| \\nv\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 7}, page_content='Rearranging Multi-Indices                                                                                        137\\nData Aggregations on Multi-Indices                                                                       140\\nCombining Datasets: Concat and Append                                                                 141\\nRecall: Concatenation of NumPy Arrays                                                                142\\nSimple Concatenation with pd.concat                                                                    142\\nCombining Datasets: Merge and Join                                                                         146\\nRelational Algebra                                                                                                      146\\nCategories of Joins                                                                                                      147\\nSpecification of the Merge Key                                                                                 149\\nSpecifying Set Arithmetic for Joins                                                                         152\\nOverlapping Column Names: The suffixes Keyword                                           153\\nExample: US States Data                                                                                           154\\nAggregation and Grouping                                                                                          158\\nPlanets Data                                                                                                                159\\nSimple Aggregation in Pandas                                                                                 159\\nGroupBy: Split, Apply, Combine                                                                             161\\nPivot Tables                                                                                                                     170\\nMotivating Pivot Tables                                                                                             170\\nPivot Tables by Hand                                                                                                 171\\nPivot Table Syntax                                                                                                      171\\nExample: Birthrate Data                                                                                            174\\nVectorized String Operations                                                                                       178\\nIntroducing Pandas String Operations                                                                   178\\nTables of Pandas String Methods                                                                             180\\nExample: Recipe Database                                                                                        184\\nWorking with Time Series                                                                                            188\\nDates and Times in Python                                                                                       188\\nPandas Time Series: Indexing by Time                                                                   192\\nPandas Time Series Data Structures                                                                        192\\nFrequencies and Offsets                                                                                            195\\nResampling, Shifting, and Windowing                                                                   196\\nWhere to Learn More                                                                                                202\\nExample: Visualizing Seattle Bicycle Counts                                                          202\\nHigh-Performance Pandas: eval() and query()                                                         208\\nMotivating query() and eval(): Compound Expressions                                      209\\npandas.eval() for Efficient Operations                                                                    210\\nDataFrame.eval() for Column-Wise Operations                                                   211\\nDataFrame.query() Method                                                                                      213\\nPerformance: When to Use These Functions                                                         214\\nFurther Resources                                                                                                          215\\nvi \\n| \\nTable of Contents\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 8}, page_content='4. Visualization with Matplotlib. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  217\\nGeneral Matplotlib Tips                                                                                                218\\nImporting matplotlib                                                                                                 218\\nSetting Styles                                                                                                               218\\nshow() or No show()? How to Display Your Plots                                                218\\nSaving Figures to File                                                                                                 221\\nTwo Interfaces for the Price of One                                                                            222\\nSimple Line Plots                                                                                                           224\\nAdjusting the Plot: Line Colors and Styles                                                             226\\nAdjusting the Plot: Axes Limits                                                                                228\\nLabeling Plots                                                                                                              230\\nSimple Scatter Plots                                                                                                       233\\nScatter Plots with plt.plot                                                                                          233\\nScatter Plots with plt.scatter                                                                                      235\\nplot Versus scatter: A Note on Efficiency                                                               237\\nVisualizing Errors                                                                                                          237\\nBasic Errorbars                                                                                                           238\\nContinuous Errors                                                                                                     239\\nDensity and Contour Plots                                                                                           241\\nVisualizing a Three-Dimensional Function                                                           241\\nHistograms, Binnings, and Density                                                                            245\\nTwo-Dimensional Histograms and Binnings                                                        247\\nCustomizing Plot Legends                                                                                            249\\nChoosing Elements for the Legend                                                                          251\\nLegend for Size of Points                                                                                           252\\nMultiple Legends                                                                                                        254\\nCustomizing Colorbars                                                                                                 255\\nCustomizing Colorbars                                                                                             256\\nExample: Handwritten Digits                                                                                   261\\nMultiple Subplots                                                                                                           262\\nplt.axes: Subplots by Hand                                                                                        263\\nplt.subplot: Simple Grids of Subplots                                                                      264\\nplt.subplots: The Whole Grid in One Go                                                               265\\nplt.GridSpec: More Complicated Arrangements                                                   266\\nText and Annotation                                                                                                     268\\nExample: Effect of Holidays on US Births                                                              269\\nTransforms and Text Position                                                                                  270\\nArrows and Annotation                                                                                            272\\nCustomizing Ticks                                                                                                         275\\nMajor and Minor Ticks                                                                                             276\\nHiding Ticks or Labels                                                                                               277\\nReducing or Increasing the Number of Ticks                                                        278\\nTable of Contents \\n| \\nvii\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 9}, page_content='Fancy Tick Formats                                                                                                    279\\nSummary of Formatters and Locators                                                                    281\\nCustomizing Matplotlib: Configurations and Stylesheets                                       282\\nPlot Customization by Hand                                                                                    282\\nChanging the Defaults: rcParams                                                                            284\\nStylesheets                                                                                                                   285\\nThree-Dimensional Plotting in Matplotlib                                                                290\\nThree-Dimensional Points and Lines                                                                      291\\nThree-Dimensional Contour Plots                                                                          292\\nWireframes and Surface Plots                                                                                  293\\nSurface Triangulations                                                                                               295\\nGeographic Data with Basemap                                                                                   298\\nMap Projections                                                                                                         300\\nDrawing a Map Background                                                                                     304\\nPlotting Data on Maps                                                                                               307\\nExample: California Cities                                                                                        308\\nExample: Surface Temperature Data                                                                       309\\nVisualization with Seaborn                                                                                           311\\nSeaborn Versus Matplotlib                                                                                        312\\nExploring Seaborn Plots                                                                                            313\\nExample: Exploring Marathon Finishing Times                                                   322\\nFurther Resources                                                                                                          329\\nMatplotlib Resources                                                                                                 329\\nOther Python Graphics Libraries                                                                             330\\n5. Machine Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  331\\nWhat Is Machine Learning?                                                                                         332\\nCategories of Machine Learning                                                                              332\\nQualitative Examples of Machine Learning Applications                                    333\\nSummary                                                                                                                     342\\nIntroducing Scikit-Learn                                                                                              343\\nData Representation in Scikit-Learn                                                                       343\\nScikit-Learn’s Estimator API                                                                                     346\\nApplication: Exploring Handwritten Digits                                                           354\\nSummary                                                                                                                     359\\nHyperparameters and Model Validation                                                                    359\\nThinking About Model Validation                                                                          359\\nSelecting the Best Model                                                                                           363\\nLearning Curves                                                                                                         370\\nValidation in Practice: Grid Search                                                                         373\\nSummary                                                                                                                     375\\nFeature Engineering                                                                                                      375\\nviii \\n| \\nTable of Contents\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 10}, page_content='Categorical Features                                                                                                   376\\nText Features                                                                                                               377\\nImage Features                                                                                                            378\\nDerived Features                                                                                                         378\\nImputation of Missing Data                                                                                      381\\nFeature Pipelines                                                                                                        381\\nIn Depth: Naive Bayes Classification                                                                          382\\nBayesian Classification                                                                                              383\\nGaussian Naive Bayes                                                                                                383\\nMultinomial Naive Bayes                                                                                          386\\nWhen to Use Naive Bayes                                                                                         389\\nIn Depth: Linear Regression                                                                                        390\\nSimple Linear Regression                                                                                          390\\nBasis Function Regression                                                                                        392\\nRegularization                                                                                                             396\\nExample: Predicting Bicycle Traffic                                                                         400\\nIn-Depth: Support Vector Machines                                                                           405\\nMotivating Support Vector Machines                                                                     405\\nSupport Vector Machines: Maximizing the Margin                                              407\\nExample: Face Recognition                                                                                       416\\nSupport Vector Machine Summary                                                                         420\\nIn-Depth: Decision Trees and Random Forests                                                        421\\nMotivating Random Forests: Decision Trees                                                         421\\nEnsembles of Estimators: Random Forests                                                            426\\nRandom Forest Regression                                                                                       428\\nExample: Random Forest for Classifying Digits                                                    430\\nSummary of Random Forests                                                                                   432\\nIn Depth: Principal Component Analysis                                                                  433\\nIntroducing Principal Component Analysis                                                          433\\nPCA as Noise Filtering                                                                                              440\\nExample: Eigenfaces                                                                                                   442\\nPrincipal Component Analysis Summary                                                              445\\nIn-Depth: Manifold Learning                                                                                      445\\nManifold Learning: “HELLO”                                                                                  446\\nMultidimensional Scaling (MDS)                                                                            447\\nMDS as Manifold Learning                                                                                      450\\nNonlinear Embeddings: Where MDS Fails                                                            452\\nNonlinear Manifolds: Locally Linear Embedding                                                 453\\nSome Thoughts on Manifold Methods                                                                   455\\nExample: Isomap on Faces                                                                                        456\\nExample: Visualizing Structure in Digits                                                                460\\nIn Depth: k-Means Clustering                                                                                     462\\nTable of Contents \\n| \\nix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 11}, page_content='Introducing k-Means                                                                                                 463\\nk-Means Algorithm: Expectation–Maximization                                                  465\\nExamples                                                                                                                      470\\nIn Depth: Gaussian Mixture Models                                                                           476\\nMotivating GMM: Weaknesses of k-Means                                                           477\\nGeneralizing E–M: Gaussian Mixture Models                                                       480\\nGMM as Density Estimation                                                                                    484\\nExample: GMM for Generating New Data                                                             488\\nIn-Depth: Kernel Density Estimation                                                                         491\\nMotivating KDE: Histograms                                                                                   491\\nKernel Density Estimation in Practice                                                                    496\\nExample: KDE on a Sphere                                                                                       498\\nExample: Not-So-Naive Bayes                                                                                  501\\nApplication: A Face Detection Pipeline                                                                      506\\nHOG Features                                                                                                             506\\nHOG in Action: A Simple Face Detector                                                                507\\nCaveats and Improvements                                                                                      512\\nFurther Machine Learning Resources                                                                         514\\nMachine Learning in Python                                                                                    514\\nGeneral Machine Learning                                                                                       515\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  517\\nx \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 12}, page_content='Preface\\nWhat Is Data Science?\\nThis is a book about doing data science with Python, which immediately begs the\\nquestion: what is data science? It’s a surprisingly hard definition to nail down, espe‐\\ncially given how ubiquitous the term has become. Vocal critics have variously dis‐\\nmissed the term as a superfluous label (after all, what science doesn’t involve data?) or\\na simple buzzword that only exists to salt résumés and catch the eye of overzealous\\ntech recruiters.\\nIn my mind, these critiques miss something important. Data science, despite its hype-\\nladen veneer, is perhaps the best label we have for the cross-disciplinary set of skills\\nthat are becoming increasingly important in many applications across industry and\\nacademia. This cross-disciplinary piece is key: in my mind, the best existing defini‐\\ntion of data science is illustrated by Drew Conway’s Data Science Venn Diagram, first\\npublished on his blog in September 2010 (see Figure P-1).\\nFigure P-1. Drew Conway’s Data Science Venn Diagram\\nxi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 13}, page_content='While some of the intersection labels are a bit tongue-in-cheek, this diagram captures\\nthe essence of what I think people mean when they say “data science”: it is fundamen‐\\ntally an interdisciplinary subject. Data science comprises three distinct and overlap‐\\nping areas: the skills of a statistician who knows how to model and summarize\\ndatasets (which are growing ever larger); the skills of a computer scientist who can\\ndesign and use algorithms to efficiently store, process, and visualize this data; and the\\ndomain expertise—what we might think of as “classical” training in a subject—neces‐\\nsary both to formulate the right questions and to put their answers in context.\\nWith this in mind, I would encourage you to think of data science not as a new\\ndomain of knowledge to learn, but as a new set of skills that you can apply within\\nyour current area of expertise. Whether you are reporting election results, forecasting\\nstock returns, optimizing online ad clicks, identifying microorganisms in microscope\\nphotos, seeking new classes of astronomical objects, or working with data in any\\nother field, the goal of this book is to give you the ability to ask and answer new ques‐\\ntions about your chosen subject area.\\nWho Is This Book For?\\nIn my teaching both at the University of Washington and at various tech-focused\\nconferences and meetups, one of the most common questions I have heard is this:\\n“how should I learn Python?” The people asking are generally technically minded\\nstudents, developers, or researchers, often with an already strong background in writ‐\\ning code and using computational and numerical tools. Most of these folks don’t want\\nto learn Python per se, but want to learn the language with the aim of using it as a\\ntool for data-intensive and computational science. While a large patchwork of videos,\\nblog posts, and tutorials for this audience is available online, I’ve long been frustrated\\nby the lack of a single good answer to this question; that is what inspired this book.\\nThe book is not meant to be an introduction to Python or to programming in gen‐\\neral; I assume the reader has familiarity with the Python language, including defining\\nfunctions, assigning variables, calling methods of objects, controlling the flow of a\\nprogram, and other basic tasks. Instead, it is meant to help Python users learn to use\\nPython’s data science stack—libraries such as IPython, NumPy, Pandas, Matplotlib,\\nScikit-Learn, and related tools—to effectively store, manipulate, and gain insight\\nfrom data.\\nWhy Python?\\nPython has emerged over the last couple decades as a first-class tool for scientific\\ncomputing tasks, including the analysis and visualization of large datasets. This may\\nhave come as a surprise to early proponents of the Python language: the language\\nitself was not specifically designed with data analysis or scientific computing in mind.\\nxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 14}, page_content='The usefulness of Python for data science stems primarily from the large and active\\necosystem of third-party packages: NumPy for manipulation of homogeneous array-\\nbased data, Pandas for manipulation of heterogeneous and labeled data, SciPy for\\ncommon scientific computing tasks, Matplotlib for publication-quality visualizations,\\nIPython for interactive execution and sharing of code, Scikit-Learn for machine\\nlearning, and many more tools that will be mentioned in the following pages.\\nIf you are looking for a guide to the Python language itself, I would suggest the sister\\nproject to this book, A Whirlwind Tour of the Python Language. This short report pro‐\\nvides a tour of the essential features of the Python language, aimed at data scientists\\nwho already are familiar with one or more other programming languages.\\nPython 2 Versus Python 3\\nThis book uses the syntax of Python 3, which contains language enhancements that\\nare not compatible with the 2.x series of Python. Though Python 3.0 was first released\\nin 2008, adoption has been relatively slow, particularly in the scientific and web devel‐\\nopment communities. This is primarily because it took some time for many of the\\nessential third-party packages and toolkits to be made compatible with the new lan‐\\nguage internals. Since early 2014, however, stable releases of the most important tools\\nin the data science ecosystem have been fully compatible with both Python 2 and 3,\\nand so this book will use the newer Python 3 syntax. However, the vast majority of\\ncode snippets in this book will also work without modification in Python 2: in cases\\nwhere a Py2-incompatible syntax is used, I will make every effort to note it explicitly.\\nOutline of This Book\\nEach chapter of this book focuses on a particular package or tool that contributes a\\nfundamental piece of the Python data science story.\\nIPython and Jupyter (Chapter 1)\\nThese packages provide the computational environment in which many Python-\\nusing data scientists work.\\nNumPy (Chapter 2)\\nThis library provides the ndarray object for efficient storage and manipulation of\\ndense data arrays in Python.\\nPandas (Chapter 3)\\nThis library provides the DataFrame object for efficient storage and manipulation\\nof labeled/columnar data in Python.\\nMatplotlib (Chapter 4)\\nThis library provides capabilities for a flexible range of data visualizations in\\nPython.\\nPreface \\n| \\nxiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 15}, page_content='Scikit-Learn (Chapter 5)\\nThis library provides efficient and clean Python implementations of the most\\nimportant and established machine learning algorithms.\\nThe PyData world is certainly much larger than these five packages, and is growing\\nevery day. With this in mind, I make every attempt through these pages to provide\\nreferences to other interesting efforts, projects, and packages that are pushing the\\nboundaries of what can be done in Python. Nevertheless, these five are currently fun‐\\ndamental to much of the work being done in the Python data science space, and I\\nexpect they will remain important even as the ecosystem continues growing around\\nthem.\\nUsing Code Examples\\nSupplemental material (code examples, figures, etc.) is available for download at\\nhttps://github.com/jakevdp/PythonDataScienceHandbook. This book is here to help\\nyou get your job done. In general, if example code is offered with this book, you may\\nuse it in your programs and documentation. You do not need to contact us for per‐\\nmission unless you’re reproducing a significant portion of the code. For example,\\nwriting a program that uses several chunks of code from this book does not require\\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example\\ncode from this book into your product’s documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example, “Python Data Science Handbook by\\nJake VanderPlas (O’Reilly). Copyright 2017 Jake VanderPlas, 978-1-491-91205-8.”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nInstallation Considerations\\nInstalling Python and the suite of libraries that enable scientific computing is\\nstraightforward. This section will outline some of the considerations to keep in mind\\nwhen setting up your computer.\\nThough there are various ways to install Python, the one I would suggest for use in\\ndata science is the Anaconda distribution, which works similarly whether you use\\nWindows, Linux, or Mac OS X. The Anaconda distribution comes in two flavors:\\n• Miniconda gives you the Python interpreter itself, along with a command-line\\ntool called conda that operates as a cross-platform package manager geared\\nxiv \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 16}, page_content='toward Python packages, similar in spirit to the apt or yum tools that Linux users\\nmight be familiar with.\\n• Anaconda includes both Python and conda, and additionally bundles a suite of\\nother preinstalled packages geared toward scientific computing. Because of the\\nsize of this bundle, expect the installation to consume several gigabytes of disk\\nspace.\\nAny of the packages included with Anaconda can also be installed manually on top of\\nMiniconda; for this reason I suggest starting with Miniconda.\\nTo get started, download and install the Miniconda package (make sure to choose a\\nversion with Python 3), and then install the core packages used in this book:\\n[~]$ conda install numpy pandas scikit-learn matplotlib seaborn ipython-notebook\\nThroughout the text, we will also make use of other, more specialized tools in\\nPython’s scientific ecosystem; installation is usually as easy as typing conda install\\npackagename. For more information on conda, including information about creating\\nand using conda environments (which I would highly recommend), refer to conda’s\\nonline documentation.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nO’Reilly Safari\\nSafari (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nPreface \\n| \\nxv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 17}, page_content='Members have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://bit.ly/python-data-sci-handbook.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nxvi \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 18}, page_content='CHAPTER 1\\nIPython: Beyond Normal Python\\nThere are many options for development environments for Python, and I’m often\\nasked which one I use in my own work. My answer sometimes surprises people: my\\npreferred environment is IPython plus a text editor (in my case, Emacs or Atom\\ndepending on my mood). IPython (short for Interactive Python) was started in 2001\\nby Fernando Perez as an enhanced Python interpreter, and has since grown into a\\nproject aiming to provide, in Perez’s words, “Tools for the entire lifecycle of research\\ncomputing.” If Python is the engine of our data science task, you might think of IPy‐\\nthon as the interactive control panel.\\nAs well as being a useful interactive interface to Python, IPython also provides a\\nnumber of useful syntactic additions to the language; we’ll cover the most useful of\\nthese additions here. In addition, IPython is closely tied with the Jupyter project,\\nwhich provides a browser-based notebook that is useful for development, collabora‐\\ntion, sharing, and even publication of data science results. The IPython notebook is\\nactually a special case of the broader Jupyter notebook structure, which encompasses\\nnotebooks for Julia, R, and other programming languages. As an example of the use‐\\nfulness of the notebook format, look no further than the page you are reading: the\\nentire manuscript for this book was composed as a set of IPython notebooks.\\nIPython is about using Python effectively for interactive scientific and data-intensive\\ncomputing. This chapter will start by stepping through some of the IPython features\\nthat are useful to the practice of data science, focusing especially on the syntax it\\noffers beyond the standard features of Python. Next, we will go into a bit more depth\\non some of the more useful “magic commands” that can speed up common tasks in\\ncreating and using data science code. Finally, we will touch on some of the features of\\nthe notebook that make it useful in understanding data and sharing results.\\n1'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 19}, page_content=\"Shell or Notebook?\\nThere are two primary means of using IPython that we’ll discuss in this chapter: the\\nIPython shell and the IPython notebook. The bulk of the material in this chapter is\\nrelevant to both, and the examples will switch between them depending on what is\\nmost convenient. In the few sections that are relevant to just one or the other, I will\\nexplicitly state that fact. Before we start, some words on how to launch the IPython\\nshell and IPython notebook.\\nLaunching the IPython Shell\\nThis chapter, like most of this book, is not designed to be absorbed passively. I recom‐\\nmend that as you read through it, you follow along and experiment with the tools and\\nsyntax we cover: the muscle-memory you build through doing this will be far more\\nuseful than the simple act of reading about it. Start by launching the IPython inter‐\\npreter by typing ipython on the command line; alternatively, if you’ve installed a dis‐\\ntribution like Anaconda or EPD, there may be a launcher specific to your system\\n(we’ll discuss this more fully in “Help and Documentation in IPython” on page 3).\\nOnce you do this, you should see a prompt like the following:\\nIPython 4.0.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython's features.\\n%quickref -> Quick reference.\\nhelp      -> Python's own help system.\\nobject?   -> Details about 'object', use 'object??' for extra details.\\nIn [1]:\\nWith that, you’re ready to follow along.\\nLaunching the Jupyter Notebook\\nThe Jupyter notebook is a browser-based graphical interface to the IPython shell, and\\nbuilds on it a rich set of dynamic display capabilities. As well as executing Python/\\nIPython statements, the notebook allows the user to include formatted text, static and\\ndynamic visualizations, mathematical equations, JavaScript widgets, and much more.\\nFurthermore, these documents can be saved in a way that lets other people open them\\nand execute the code on their own systems.\\nThough the IPython notebook is viewed and edited through your web browser win‐\\ndow, it must connect to a running Python process in order to execute code. To start\\nthis process (known as a “kernel”), run the following command in your system shell:\\n$ jupyter notebook\\nThis command will launch a local web server that will be visible to your browser. It\\nimmediately spits out a log showing what it is doing; that log will look something like\\nthis:\\n2 \\n| \\nChapter 1: IPython: Beyond Normal Python\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 20}, page_content='$ jupyter notebook\\n[NotebookApp] Serving notebooks from local directory: /Users/jakevdp/...\\n[NotebookApp] 0 active kernels\\n[NotebookApp] The IPython Notebook is running at: http://localhost:8888/\\n[NotebookApp] Use Control-C to stop this server and shut down all kernels...\\nUpon issuing the command, your default browser should automatically open and\\nnavigate to the listed local URL; the exact address will depend on your system. If the\\nbrowser does not open automatically, you can open a window and manually open this\\naddress (http://localhost:8888/ in this example).\\nHelp and Documentation in IPython\\nIf you read no other section in this chapter, read this one: I find the tools discussed\\nhere to be the most transformative contributions of IPython to my daily workflow.\\nWhen a technologically minded person is asked to help a friend, family member, or\\ncolleague with a computer problem, most of the time it’s less a matter of knowing the\\nanswer as much as knowing how to quickly find an unknown answer. In data science\\nit’s the same: searchable web resources such as online documentation, mailing-list\\nthreads, and Stack Overflow answers contain a wealth of information, even (espe‐\\ncially?) if it is a topic you’ve found yourself searching before. Being an effective prac‐\\ntitioner of data science is less about memorizing the tool or command you should use\\nfor every possible situation, and more about learning to effectively find the informa‐\\ntion you don’t know, whether through a web search engine or another means.\\nOne of the most useful functions of IPython/Jupyter is to shorten the gap between the\\nuser and the type of documentation and search that will help them do their work\\neffectively. While web searches still play a role in answering complicated questions,\\nan amazing amount of information can be found through IPython alone. Some\\nexamples of the questions IPython can help answer in a few keystrokes:\\n• How do I call this function? What arguments and options does it have?\\n• What does the source code of this Python object look like?\\n• What is in this package I imported? What attributes or methods does this object\\nhave?\\nHere we’ll discuss IPython’s tools to quickly access this information, namely the ?\\ncharacter to explore documentation, the ?? characters to explore source code, and the\\nTab key for autocompletion.\\nAccessing Documentation with ?\\nThe Python language and its data science ecosystem are built with the user in mind,\\nand one big part of that is access to documentation. Every Python object contains the\\nHelp and Documentation in IPython \\n| \\n3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 21}, page_content='reference to a string, known as a docstring, which in most cases will contain a concise\\nsummary of the object and how to use it. Python has a built-in help() function that\\ncan access this information and print the results. For example, to see the documenta‐\\ntion of the built-in len function, you can do the following:\\nIn [1]: help(len)\\nHelp on built-in function len in module builtins:\\nlen(...)\\n    len(object) -> integer\\n    Return the number of items of a sequence or mapping.\\nDepending on your interpreter, this information may be displayed as inline text, or in\\nsome separate pop-up window.\\nBecause finding help on an object is so common and useful, IPython introduces the ?\\ncharacter as a shorthand for accessing this documentation and other relevant\\ninformation:\\nIn [2]: len?\\nType:        builtin_function_or_method\\nString form: <built-in function len>\\nNamespace:   Python builtin\\nDocstring:\\nlen(object) -> integer\\nReturn the number of items of a sequence or mapping.\\nThis notation works for just about anything, including object methods:\\nIn [3]: L = [1, 2, 3]\\nIn [4]: L.insert?\\nType:        builtin_function_or_method\\nString form: <built-in method insert of list object at 0x1024b8ea8>\\nDocstring:   L.insert(index, object) -- insert object before index\\nor even objects themselves, with the documentation from their type:\\nIn [5]: L?\\nType:        list\\nString form: [1, 2, 3]\\nLength:      3\\nDocstring:\\nlist() -> new empty list\\nlist(iterable) -> new list initialized from iterable\\'s items\\nImportantly, this will even work for functions or other objects you create yourself!\\nHere we’ll define a small function with a docstring:\\nIn [6]: def square(a):\\n  ....:     \"\"\"Return the square of a.\"\"\"\\n4 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 22}, page_content='....:     return a ** 2\\n  ....:\\nNote that to create a docstring for our function, we simply placed a string literal in\\nthe first line. Because docstrings are usually multiple lines, by convention we used\\nPython’s triple-quote notation for multiline strings.\\nNow we’ll use the ? mark to find this docstring:\\nIn [7]: square?\\nType:        function\\nString form: <function square at 0x103713cb0>\\nDefinition:  square(a)\\nDocstring:   Return the square of a.\\nThis quick access to documentation via docstrings is one reason you should get in the\\nhabit of always adding such inline documentation to the code you write!\\nAccessing Source Code with ??\\nBecause the Python language is so easily readable, you can usually gain another level\\nof insight by reading the source code of the object you’re curious about. IPython pro‐\\nvides a shortcut to the source code with the double question mark (??):\\nIn [8]: square??\\nType:        function\\nString form: <function square at 0x103713cb0>\\nDefinition:  square(a)\\nSource:\\ndef square(a):\\n    \"Return the square of a\"\\n    return a ** 2\\nFor simple functions like this, the double question mark can give quick insight into\\nthe under-the-hood details.\\nIf you play with this much, you’ll notice that sometimes the ?? suffix doesn’t display\\nany source code: this is generally because the object in question is not implemented in\\nPython, but in C or some other compiled extension language. If this is the case, the ??\\nsuffix gives the same output as the ? suffix. You’ll find this particularly with many of\\nPython’s built-in objects and types, for example len from above:\\nIn [9]: len??\\nType:        builtin_function_or_method\\nString form: <built-in function len>\\nNamespace:   Python builtin\\nDocstring:\\nlen(object) -> integer\\nReturn the number of items of a sequence or mapping.\\nHelp and Documentation in IPython \\n| \\n5'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 23}, page_content='Using ? and/or ?? gives a powerful and quick interface for finding information about\\nwhat any Python function or module does.\\nExploring Modules with Tab Completion\\nIPython’s other useful interface is the use of the Tab key for autocompletion and\\nexploration of the contents of objects, modules, and namespaces. In the examples that\\nfollow, we’ll use <TAB> to indicate when the Tab key should be pressed.\\nTab completion of object contents\\nEvery Python object has various attributes and methods associated with it. Like with\\nthe help function discussed before, Python has a built-in dir function that returns a\\nlist of these, but the tab-completion interface is much easier to use in practice. To see\\na list of all available attributes of an object, you can type the name of the object fol‐\\nlowed by a period (.) character and the Tab key:\\nIn [10]: L.<TAB>\\nL.append   L.copy     L.extend   L.insert   L.remove   L.sort\\nL.clear    L.count    L.index    L.pop      L.reverse\\nTo narrow down the list, you can type the first character or several characters of the\\nname, and the Tab key will find the matching attributes and methods:\\nIn [10]: L.c<TAB>\\nL.clear  L.copy   L.count\\nIn [10]: L.co<TAB>\\nL.copy   L.count\\nIf there is only a single option, pressing the Tab key will complete the line for you. For\\nexample, the following will instantly be replaced with L.count:\\nIn [10]: L.cou<TAB>\\nThough Python has no strictly enforced distinction between public/external\\nattributes and private/internal attributes, by convention a preceding underscore is\\nused to denote such methods. For clarity, these private methods and special methods\\nare omitted from the list by default, but it’s possible to list them by explicitly typing\\nthe underscore:\\nIn [10]: L._<TAB>\\nL.__add__           L.__gt__            L.__reduce__\\nL.__class__         L.__hash__          L.__reduce_ex__\\nFor brevity, we’ve only shown the first couple lines of the output. Most of these are\\nPython’s special double-underscore methods (often nicknamed “dunder” methods).\\n6 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 24}, page_content='Tab completion when importing\\nTab completion is also useful when importing objects from packages. Here we’ll use it\\nto find all possible imports in the itertools package that start with co:\\nIn [10]: from itertools import co<TAB>\\ncombinations                   compress\\ncombinations_with_replacement  count\\nSimilarly, you can use tab completion to see which imports are available on your sys‐\\ntem (this will change depending on which third-party scripts and modules are visible\\nto your Python session):\\nIn [10]: import <TAB>\\nDisplay all 399 possibilities? (y or n)\\nCrypto              dis                 py_compile\\nCython              distutils           pyclbr\\n...                 ...                 ...\\ndifflib             pwd                 zmq\\nIn [10]: import h<TAB>\\nhashlib             hmac                http\\nheapq               html                husl\\n(Note that for brevity, I did not print here all 399 importable packages and modules\\non my system.)\\nBeyond tab completion: Wildcard matching\\nTab completion is useful if you know the first few characters of the object or attribute\\nyou’re looking for, but is little help if you’d like to match characters at the middle or\\nend of the word. For this use case, IPython provides a means of wildcard matching\\nfor names using the * character.\\nFor example, we can use this to list every object in the namespace that ends with\\nWarning:\\nIn [10]: *Warning?\\nBytesWarning                  RuntimeWarning\\nDeprecationWarning            SyntaxWarning\\nFutureWarning                 UnicodeWarning\\nImportWarning                 UserWarning\\nPendingDeprecationWarning     Warning\\nResourceWarning\\nNotice that the * character matches any string, including the empty string.\\nSimilarly, suppose we are looking for a string method that contains the word find\\nsomewhere in its name. We can search for it this way:\\nHelp and Documentation in IPython \\n| \\n7'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 25}, page_content='In [10]: str.*find*?\\nstr.find\\nstr.rfind\\nI find this type of flexible wildcard search can be very useful for finding a particular\\ncommand when I’m getting to know a new package or reacquainting myself with a\\nfamiliar one.\\nKeyboard Shortcuts in the IPython Shell\\nIf you spend any amount of time on the computer, you’ve probably found a use for\\nkeyboard shortcuts in your workflow. Most familiar perhaps are Cmd-C and Cmd-V\\n(or Ctrl-C and Ctrl-V) for copying and pasting in a wide variety of programs and sys‐\\ntems. Power users tend to go even further: popular text editors like Emacs, Vim, and\\nothers provide users an incredible range of operations through intricate combina‐\\ntions of keystrokes.\\nThe IPython shell doesn’t go this far, but does provide a number of keyboard short‐\\ncuts for fast navigation while you’re typing commands. These shortcuts are not in fact\\nprovided by IPython itself, but through its dependency on the GNU Readline library:\\nthus, some of the following shortcuts may differ depending on your system configu‐\\nration. Also, while some of these shortcuts do work in the browser-based notebook,\\nthis section is primarily about shortcuts in the IPython shell.\\nOnce you get accustomed to these, they can be very useful for quickly performing\\ncertain commands without moving your hands from the “home” keyboard position.\\nIf you’re an Emacs user or if you have experience with Linux-style shells, the follow‐\\ning will be very familiar. We’ll group these shortcuts into a few categories: navigation\\nshortcuts, text entry shortcuts, command history shortcuts, and miscellaneous shortcuts.\\nNavigation Shortcuts\\nWhile the use of the left and right arrow keys to move backward and forward in the\\nline is quite obvious, there are other options that don’t require moving your hands\\nfrom the “home” keyboard position:\\nKeystroke\\nAction\\nCtrl-a\\nMove cursor to the beginning of the line\\nCtrl-e\\nMove cursor to the end of the line\\nCtrl-b (or the left arrow key)\\nMove cursor back one character\\nCtrl-f (or the right arrow key) Move cursor forward one character\\n8 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 26}, page_content=\"Text Entry Shortcuts\\nWhile everyone is familiar with using the Backspace key to delete the previous char‐\\nacter, reaching for the key often requires some minor finger gymnastics, and it only\\ndeletes a single character at a time. In IPython there are several shortcuts for remov‐\\ning some portion of the text you’re typing. The most immediately useful of these are\\nthe commands to delete entire lines of text. You’ll know these have become second\\nnature if you find yourself using a combination of Ctrl-b and Ctrl-d instead of reach‐\\ning for the Backspace key to delete the previous character!\\nKeystroke\\nAction\\nBackspace key Delete previous character in line\\nCtrl-d\\nDelete next character in line\\nCtrl-k\\nCut text from cursor to end of line\\nCtrl-u\\nCut text from beginning fo line to cursor\\nCtrl-y\\nYank (i.e., paste) text that was previously cut\\nCtrl-t\\nTranspose (i.e., switch) previous two characters\\nCommand History Shortcuts\\nPerhaps the most impactful shortcuts discussed here are the ones IPython provides\\nfor navigating the command history. This command history goes beyond your cur‐\\nrent IPython session: your entire command history is stored in a SQLite database in\\nyour IPython profile directory. The most straightforward way to access these is with\\nthe up and down arrow keys to step through the history, but other options exist as\\nwell:\\nKeystroke\\nAction\\nCtrl-p (or the up arrow key)\\nAccess previous command in history\\nCtrl-n (or the down arrow key)\\nAccess next command in history\\nCtrl-r\\nReverse-search through command history\\nThe reverse-search can be particularly useful. Recall that in the previous section we\\ndefined a function called square. Let’s reverse-search our Python history from a new\\nIPython shell and find this definition again. When you press Ctrl-r in the IPython\\nterminal, you’ll see the following prompt:\\nIn [1]:\\n(reverse-i-search)`':\\nIf you start typing characters at this prompt, IPython will auto-fill the most recent\\ncommand, if any, that matches those characters:\\nKeyboard Shortcuts in the IPython Shell \\n| \\n9\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 27}, page_content='In [1]:\\n(reverse-i-search)`sqa\\': square??\\nAt any point, you can add more characters to refine the search, or press Ctrl-r again\\nto search further for another command that matches the query. If you followed along\\nin the previous section, pressing Ctrl-r twice more gives:\\nIn [1]:\\n(reverse-i-search)`sqa\\': def square(a):\\n    \"\"\"Return the square of a\"\"\"\\n    return a ** 2\\nOnce you have found the command you’re looking for, press Return and the search\\nwill end. We can then use the retrieved command, and carry on with our session:\\nIn [1]: def square(a):\\n    \"\"\"Return the square of a\"\"\"\\n    return a ** 2\\nIn [2]: square(2)\\nOut[2]: 4\\nNote that you can also use Ctrl-p/Ctrl-n or the up/down arrow keys to search\\nthrough history, but only by matching characters at the beginning of the line. That is,\\nif you type def and then press Ctrl-p, it would find the most recent command (if any)\\nin your history that begins with the characters def.\\nMiscellaneous Shortcuts\\nFinally, there are a few miscellaneous shortcuts that don’t fit into any of the preceding\\ncategories, but are nevertheless useful to know:\\nKeystroke Action\\nCtrl-l\\nClear terminal screen\\nCtrl-c\\nInterrupt current Python command\\nCtrl-d\\nExit IPython session\\nThe Ctrl-c shortcut in particular can be useful when you inadvertently start a very\\nlong-running job.\\nWhile some of the shortcuts discussed here may seem a bit tedious at first, they\\nquickly become automatic with practice. Once you develop that muscle memory, I\\nsuspect you will even find yourself wishing they were available in other contexts.\\nIPython Magic Commands\\nThe previous two sections showed how IPython lets you use and explore Python effi‐\\nciently and interactively. Here we’ll begin discussing some of the enhancements that\\n10 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 28}, page_content='IPython adds on top of the normal Python syntax. These are known in IPython as\\nmagic commands, and are prefixed by the % character. These magic commands are\\ndesigned to succinctly solve various common problems in standard data analysis.\\nMagic commands come in two flavors: line magics, which are denoted by a single %\\nprefix and operate on a single line of input, and cell magics, which are denoted by a\\ndouble %% prefix and operate on multiple lines of input. We’ll demonstrate and dis‐\\ncuss a few brief examples here, and come back to more focused discussion of several\\nuseful magic commands later in the chapter.\\nPasting Code Blocks: %paste and %cpaste\\nWhen you’re working in the IPython interpreter, one common gotcha is that pasting\\nmultiline code blocks can lead to unexpected errors, especially when indentation and\\ninterpreter markers are involved. A common case is that you find some example code\\non a website and want to paste it into your interpreter. Consider the following simple\\nfunction:\\n>>> def donothing(x):\\n...     return x\\nThe code is formatted as it would appear in the Python interpreter, and if you copy\\nand paste this directly into IPython you get an error:\\nIn [2]: >>> def donothing(x):\\n   ...:     ...     return x\\n   ...:\\n  File \"<ipython-input-20-5a66c8964687>\", line 2\\n    ...     return x\\n                 ^\\nSyntaxError: invalid syntax\\nIn the direct paste, the interpreter is confused by the additional prompt characters.\\nBut never fear—IPython’s %paste magic function is designed to handle this exact type\\nof multiline, marked-up input:\\nIn [3]: %paste\\n>>> def donothing(x):\\n...     return x\\n## -- End pasted text --\\nThe %paste command both enters and executes the code, so now the function is\\nready to be used:\\nIn [4]: donothing(10)\\nOut[4]: 10\\nA command with a similar intent is %cpaste, which opens up an interactive multiline\\nprompt in which you can paste one or more chunks of code to be executed in a batch:\\nIPython Magic Commands \\n| \\n11'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 29}, page_content='In [5]: %cpaste\\nPasting code; enter \\'--\\' alone on the line to stop or use Ctrl-D.\\n:>>> def donothing(x):\\n:...     return x\\n:--\\nThese magic commands, like others we’ll see, make available functionality that would\\nbe difficult or impossible in a standard Python interpreter.\\nRunning External Code: %run\\nAs you begin developing more extensive code, you will likely find yourself working in\\nboth IPython for interactive exploration, as well as a text editor to store code that you\\nwant to reuse. Rather than running this code in a new window, it can be convenient\\nto run it within your IPython session. This can be done with the %run magic.\\nFor example, imagine you’ve created a myscript.py file with the following contents:\\n#-------------------------------------\\n# file: myscript.py\\ndef square(x):\\n    \"\"\"square a number\"\"\"\\n    return x ** 2\\nfor N in range(1, 4):\\n    print(N, \"squared is\", square(N))\\nYou can execute this from your IPython session as follows:\\nIn [6]: %run myscript.py\\n1 squared is 1\\n2 squared is 4\\n3 squared is 9\\nNote also that after you’ve run this script, any functions defined within it are available\\nfor use in your IPython session:\\nIn [7]: square(5)\\nOut[7]: 25\\nThere are several options to fine-tune how your code is run; you can see the docu‐\\nmentation in the normal way, by typing %run? in the IPython interpreter.\\nTiming Code Execution: %timeit\\nAnother example of a useful magic function is %timeit, which will automatically\\ndetermine the execution time of the single-line Python statement that follows it. For\\nexample, we may want to check the performance of a list comprehension:\\nIn [8]: %timeit L = [n ** 2 for n in range(1000)]\\n1000 loops, best of 3: 325 µs per loop\\n12 \\n| \\nChapter 1: IPython: Beyond Normal Python\\nwww.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 30}, page_content='The benefit of %timeit is that for short commands it will automatically perform mul‐\\ntiple runs in order to attain more robust results. For multiline statements, adding a\\nsecond % sign will turn this into a cell magic that can handle multiple lines of input.\\nFor example, here’s the equivalent construction with a for loop:\\nIn [9]: %%timeit\\n   ...: L = []\\n   ...: for n in range(1000):\\n   ...:     L.append(n ** 2)\\n   ...:\\n1000 loops, best of 3: 373 µs per loop\\nWe can immediately see that list comprehensions are about 10% faster than the\\nequivalent for loop construction in this case. We’ll explore %timeit and other\\napproaches to timing and profiling code in “Profiling and Timing Code” on page 25.\\nHelp on Magic Functions: ?, %magic, and %lsmagic\\nLike normal Python functions, IPython magic functions have docstrings, and this\\nuseful documentation can be accessed in the standard manner. So, for example, to\\nread the documentation of the %timeit magic, simply type this:\\nIn [10]: %timeit?\\nDocumentation for other functions can be accessed similarly. To access a general\\ndescription of available magic functions, including some examples, you can type this:\\nIn [11]: %magic\\nFor a quick and simple list of all available magic functions, type this:\\nIn [12]: %lsmagic\\nFinally, I’ll mention that it is quite straightforward to define your own magic func‐\\ntions if you wish. We won’t discuss it here, but if you are interested, see the references\\nlisted in “More IPython Resources” on page 30.\\nInput and Output History\\nPreviously we saw that the IPython shell allows you to access previous commands\\nwith the up and down arrow keys, or equivalently the Ctrl-p/Ctrl-n shortcuts. Addi‐\\ntionally, in both the shell and the notebook, IPython exposes several ways to obtain\\nthe output of previous commands, as well as string versions of the commands them‐\\nselves. We’ll explore those here.\\nIPython’s In and Out Objects\\nBy now I imagine you’re quite familiar with the In[1]:/Out[1]: style prompts used\\nby IPython. But it turns out that these are not just pretty decoration: they give a clue\\nInput and Output History \\n| \\n13'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 31}, page_content=\"as to how you can access previous inputs and outputs in your current session. Imag‐\\nine you start a session that looks like this:\\nIn [1]: import math\\nIn [2]: math.sin(2)\\nOut[2]: 0.9092974268256817\\nIn [3]: math.cos(2)\\nOut[3]: -0.4161468365471424\\nWe’ve imported the built-in math package, then computed the sine and the cosine of\\nthe number 2. These inputs and outputs are displayed in the shell with In/Out labels,\\nbut there’s more—IPython actually creates some Python variables called In and Out\\nthat are automatically updated to reflect this history:\\nIn [4]: print(In)\\n['', 'import math', 'math.sin(2)', 'math.cos(2)', 'print(In)']\\nIn [5]: Out\\nOut[5]: {2: 0.9092974268256817, 3: -0.4161468365471424}\\nThe In object is a list, which keeps track of the commands in order (the first item in\\nthe list is a placeholder so that In[1] can refer to the first command):\\nIn [6]: print(In[1])\\nimport math\\nThe Out object is not a list but a dictionary mapping input numbers to their outputs\\n(if any):\\nIn [7]: print(Out[2])\\n0.9092974268256817\\nNote that not all operations have outputs: for example, import statements and print\\nstatements don’t affect the output. The latter may be surprising, but makes sense if\\nyou consider that print is a function that returns None; for brevity, any command\\nthat returns None is not added to Out.\\nWhere this can be useful is if you want to interact with past results. For example, let’s\\ncheck the sum of sin(2) ** 2 and cos(2) ** 2 using the previously computed\\nresults:\\nIn [8]: Out[2] ** 2 + Out[3] ** 2\\nOut[8]: 1.0\\nThe result is 1.0 as we’d expect from the well-known trigonometric identity. In this\\ncase, using these previous results probably is not necessary, but it can become very\\nhandy if you execute a very expensive computation and want to reuse the result!\\n14 \\n| \\nChapter 1: IPython: Beyond Normal Python\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 32}, page_content='Underscore Shortcuts and Previous Outputs\\nThe standard Python shell contains just one simple shortcut for accessing previous\\noutput; the variable _ (i.e., a single underscore) is kept updated with the previous out‐\\nput; this works in IPython as well:\\nIn [9]: print(_)\\n1.0\\nBut IPython takes this a bit further—you can use a double underscore to access the\\nsecond-to-last output, and a triple underscore to access the third-to-last output (skip‐\\nping any commands with no output):\\nIn [10]: print(__)\\n-0.4161468365471424\\nIn [11]: print(___)\\n0.9092974268256817\\nIPython stops there: more than three underscores starts to get a bit hard to count,\\nand at that point it’s easier to refer to the output by line number.\\nThere is one more shortcut we should mention, however—a shorthand for Out[X] is\\n_X (i.e., a single underscore followed by the line number):\\nIn [12]: Out[2]\\nOut[12]: 0.9092974268256817\\nIn [13]: _2\\nOut[13]: 0.9092974268256817\\nSuppressing Output\\nSometimes you might wish to suppress the output of a statement (this is perhaps\\nmost common with the plotting commands that we’ll explore in Chapter 4). Or\\nmaybe the command you’re executing produces a result that you’d prefer not to store\\nin your output history, perhaps so that it can be deallocated when other references are\\nremoved. The easiest way to suppress the output of a command is to add a semicolon\\nto the end of the line:\\nIn [14]: math.sin(2) + math.cos(2);\\nNote that the result is computed silently, and the output is neither displayed on the\\nscreen or stored in the Out dictionary:\\nIn [15]: 14 in Out\\nOut[15]: False\\nInput and Output History \\n| \\n15'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 33}, page_content='Related Magic Commands\\nFor accessing a batch of previous inputs at once, the %history magic command is\\nvery helpful. Here is how you can print the first four inputs:\\nIn [16]: %history -n 1-4\\n   1: import math\\n   2: math.sin(2)\\n   3: math.cos(2)\\n   4: print(In)\\nAs usual, you can type %history? for more information and a description of options\\navailable. Other similar magic commands are %rerun (which will re-execute some\\nportion of the command history) and %save (which saves some set of the command\\nhistory to a file). For more information, I suggest exploring these using the ? help\\nfunctionality discussed in “Help and Documentation in IPython” on page 3.\\nIPython and Shell Commands\\nWhen working interactively with the standard Python interpreter, one of the frustra‐\\ntions you’ll face is the need to switch between multiple windows to access Python\\ntools and system command-line tools. IPython bridges this gap, and gives you a syn‐\\ntax for executing shell commands directly from within the IPython terminal. The\\nmagic happens with the exclamation point: anything appearing after ! on a line will\\nbe executed not by the Python kernel, but by the system command line.\\nThe following assumes you’re on a Unix-like system, such as Linux or Mac OS X.\\nSome of the examples that follow will fail on Windows, which uses a different type of\\nshell by default (though with the 2016 announcement of native Bash shells on Win‐\\ndows, soon this may no longer be an issue!). If you’re unfamiliar with shell com‐\\nmands, I’d suggest reviewing the Shell Tutorial put together by the always excellent\\nSoftware Carpentry Foundation.\\nQuick Introduction to the Shell\\nA full intro to using the shell/terminal/command line is well beyond the scope of this\\nchapter, but for the uninitiated we will offer a quick introduction here. The shell is a\\nway to interact textually with your computer. Ever since the mid-1980s, when Micro‐\\nsoft and Apple introduced the first versions of their now ubiquitous graphical operat‐\\ning systems, most computer users have interacted with their operating system\\nthrough familiar clicking of menus and drag-and-drop movements. But operating\\nsystems existed long before these graphical user interfaces, and were primarily con‐\\ntrolled through sequences of text input: at the prompt, the user would type a com‐\\nmand, and the computer would do what the user told it to. Those early prompt\\n16 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 34}, page_content='systems are the precursors of the shells and terminals that most active data scientists\\nstill use today.\\nSomeone unfamiliar with the shell might ask why you would bother with this, when\\nyou can accomplish many results by simply clicking on icons and menus. A shell user\\nmight reply with another question: why hunt icons and click menus when you can\\naccomplish things much more easily by typing? While it might sound like a typical\\ntech preference impasse, when moving beyond basic tasks it quickly becomes clear\\nthat the shell offers much more control of advanced tasks, though admittedly the\\nlearning curve can intimidate the average computer user.\\nAs an example, here is a sample of a Linux/OS X shell session where a user explores,\\ncreates, and modifies directories and files on their system (osx:~ $ is the prompt,\\nand everything after the $ sign is the typed command; text that is preceded by a # is\\nmeant just as description, rather than something you would actually type in):\\nosx:~ $ echo \"hello world\"              # echo is like Python\\'s print function\\nhello world\\nosx:~ $ pwd                             # pwd = print working directory\\n/home/jake                              # this is the \"path\" that we\\'re in\\nosx:~ $ ls                              # ls = list working directory contents\\nnotebooks  projects\\nosx:~ $ cd projects/                    # cd = change directory\\nosx:projects $ pwd\\n/home/jake/projects\\nosx:projects $ ls\\ndatasci_book   mpld3   myproject.txt\\nosx:projects $ mkdir myproject          # mkdir = make new directory\\nosx:projects $ cd myproject/\\nosx:myproject $ mv ../myproject.txt ./  # mv = move file. Here we\\'re moving the\\n                                        # file myproject.txt from one directory\\n                                        # up (../) to the current directory (./)\\nosx:myproject $ ls\\nmyproject.txt\\nNotice that all of this is just a compact way to do familiar operations (navigating a\\ndirectory structure, creating a directory, moving a file, etc.) by typing commands\\nrather than clicking icons and menus. Note that with just a few commands (pwd, ls,\\ncd, mkdir, and cp) you can do many of the most common file operations. It’s when\\nyou go beyond these basics that the shell approach becomes really powerful.\\nIPython and Shell Commands \\n| \\n17'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 35}, page_content='Shell Commands in IPython\\nYou can use any command that works at the command line in IPython by prefixing it\\nwith the ! character. For example, the ls, pwd, and echo commands can be run as\\nfollows:\\nIn [1]: !ls\\nmyproject.txt\\nIn [2]: !pwd\\n/home/jake/projects/myproject\\nIn [3]: !echo \"printing from the shell\"\\nprinting from the shell\\nPassing Values to and from the Shell\\nShell commands can not only be called from IPython, but can also be made to inter‐\\nact with the IPython namespace. For example, you can save the output of any shell\\ncommand to a Python list using the assignment operator:\\nIn [4]: contents = !ls\\nIn [5]: print(contents)\\n[\\'myproject.txt\\']\\nIn [6]: directory = !pwd\\nIn [7]: print(directory)\\n[\\'/Users/jakevdp/notebooks/tmp/myproject\\']\\nNote that these results are not returned as lists, but as a special shell return type\\ndefined in IPython:\\nIn [8]: type(directory)\\nIPython.utils.text.SList\\nThis looks and acts a lot like a Python list, but has additional functionality, such as\\nthe grep and fields methods and the s, n, and p properties that allow you to search,\\nfilter, and display the results in convenient ways. For more information on these, you\\ncan use IPython’s built-in help features.\\nCommunication in the other direction—passing Python variables into the shell—is\\npossible through the {varname} syntax:\\nIn [9]: message = \"hello from Python\"\\nIn [10]: !echo {message}\\nhello from Python\\n18 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 36}, page_content='The curly braces contain the variable name, which is replaced by the variable’s con‐\\ntents in the shell command.\\nShell-Related Magic Commands\\nIf you play with IPython’s shell commands for a while, you might notice that you can‐\\nnot use !cd to navigate the filesystem:\\nIn [11]: !pwd\\n/home/jake/projects/myproject\\nIn [12]: !cd ..\\nIn [13]: !pwd\\n/home/jake/projects/myproject\\nThe reason is that shell commands in the notebook are executed in a temporary sub‐\\nshell. If you’d like to change the working directory in a more enduring way, you can\\nuse the %cd magic command:\\nIn [14]: %cd ..\\n/home/jake/projects\\nIn fact, by default you can even use this without the % sign:\\nIn [15]: cd myproject\\n/home/jake/projects/myproject\\nThis is known as an automagic function, and this behavior can be toggled with the\\n%automagic magic function.\\nBesides %cd, other available shell-like magic functions are %cat, %cp, %env, %ls, %man,\\n%mkdir, %more, %mv, %pwd, %rm, and %rmdir, any of which can be used without the %\\nsign if automagic is on. This makes it so that you can almost treat the IPython\\nprompt as if it’s a normal shell:\\nIn [16]: mkdir tmp\\nIn [17]: ls\\nmyproject.txt  tmp/\\nIn [18]: cp myproject.txt tmp/\\nIn [19]: ls tmp\\nmyproject.txt\\nIn [20]: rm -r tmp\\nThis access to the shell from within the same terminal window as your Python ses‐\\nsion means that there is a lot less switching back and forth between interpreter and\\nshell as you write your Python code.\\nShell-Related Magic Commands \\n| \\n19'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 37}, page_content='Errors and Debugging\\nCode development and data analysis always require a bit of trial and error, and\\nIPython contains tools to streamline this process. This section will briefly cover some\\noptions for controlling Python’s exception reporting, followed by exploring tools for\\ndebugging errors in code.\\nControlling Exceptions: %xmode\\nMost of the time when a Python script fails, it will raise an exception. When the inter‐\\npreter hits one of these exceptions, information about the cause of the error can be\\nfound in the traceback, which can be accessed from within Python. With the %xmode\\nmagic function, IPython allows you to control the amount of information printed\\nwhen the exception is raised. Consider the following code:\\nIn[1]: def func1(a, b):\\n           return a / b\\n       def func2(x):\\n           a = x\\n           b = x - 1\\n           return func1(a, b)\\nIn[2]: func2(1)\\n---------------------------------------------------------------------------\\nZeroDivisionError                         Traceback (most recent call last)\\n<ipython-input-2-b2e110f6fc8f^gt; in <module>()\\n----> 1 func2(1)\\n<ipython-input-1-d849e34d61fb> in func2(x)\\n      5     a = x\\n      6     b = x - 1\\n----> 7     return func1(a, b)\\n<ipython-input-1-d849e34d61fb> in func1(a, b)\\n      1 def func1(a, b):\\n----> 2     return a / b\\n      3\\n      4 def func2(x):\\n      5     a = x\\nZeroDivisionError: division by zero\\nCalling func2 results in an error, and reading the printed trace lets us see exactly what\\nhappened. By default, this trace includes several lines showing the context of each\\n20 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 38}, page_content='step that led to the error. Using the %xmode magic function (short for exception mode),\\nwe can change what information is printed.\\n%xmode takes a single argument, the mode, and there are three possibilities: Plain,\\nContext, and Verbose. The default is Context, and gives output like that just shown.\\nPlain is more compact and gives less information:\\nIn[3]: %xmode Plain\\nException reporting mode: Plain\\nIn[4]: func2(1)\\n------------------------------------------------------------\\nTraceback (most recent call last):\\n  File \"<ipython-input-4-b2e110f6fc8f>\", line 1, in <module>\\n    func2(1)\\n  File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2\\n    return func1(a, b)\\n  File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1\\n    return a / b\\nZeroDivisionError: division by zero\\nThe Verbose mode adds some extra information, including the arguments to any\\nfunctions that are called:\\nIn[5]: %xmode Verbose\\nException reporting mode: Verbose\\nIn[6]: func2(1)\\n---------------------------------------------------------------------------\\nZeroDivisionError                         Traceback (most recent call last)\\n<ipython-input-6-b2e110f6fc8f> in <module>()\\n----> 1 func2(1)\\n        global func2 = <function func2 at 0x103729320>\\n<ipython-input-1-d849e34d61fb> in func2(x=1)\\n      5     a = x\\n      6     b = x - 1\\n----> 7     return func1(a, b)\\n        global func1 = <function func1 at 0x1037294d0>\\n        a = 1\\n        b = 0\\nErrors and Debugging \\n| \\n21'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 39}, page_content='<ipython-input-1-d849e34d61fb> in func1(a=1, b=0)\\n      1 def func1(a, b):\\n----> 2     return a / b\\n        a = 1\\n        b = 0\\n      3\\n      4 def func2(x):\\n      5     a = x\\nZeroDivisionError: division by zero\\nThis extra information can help you narrow in on why the exception is being raised.\\nSo why not use the Verbose mode all the time? As code gets complicated, this kind of\\ntraceback can get extremely long. Depending on the context, sometimes the brevity of\\nDefault mode is easier to work with.\\nDebugging: When Reading Tracebacks Is Not Enough\\nThe standard Python tool for interactive debugging is pdb, the Python debugger. This\\ndebugger lets the user step through the code line by line in order to see what might be\\ncausing a more difficult error. The IPython-enhanced version of this is ipdb, the\\nIPython debugger.\\nThere are many ways to launch and use both these debuggers; we won’t cover them\\nfully here. Refer to the online documentation of these two utilities to learn more.\\nIn IPython, perhaps the most convenient interface to debugging is the %debug magic\\ncommand. If you call it after hitting an exception, it will automatically open an inter‐\\nactive debugging prompt at the point of the exception. The ipdb prompt lets you\\nexplore the current state of the stack, explore the available variables, and even run\\nPython commands!\\nLet’s look at the most recent exception, then do some basic tasks—print the values of\\na and b, and type quit to quit the debugging session:\\nIn[7]: %debug\\n> <ipython-input-1-d849e34d61fb>(2)func1()\\n      1 def func1(a, b):\\n----> 2     return a / b\\n      3\\nipdb> print(a)\\n1\\nipdb> print(b)\\n0\\nipdb> quit\\n22 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 40}, page_content='The interactive debugger allows much more than this, though—we can even step up\\nand down through the stack and explore the values of variables there:\\nIn[8]: %debug\\n> <ipython-input-1-d849e34d61fb>(2)func1()\\n      1 def func1(a, b):\\n----> 2     return a / b\\n      3\\nipdb> up\\n> <ipython-input-1-d849e34d61fb>(7)func2()\\n      5     a = x\\n      6     b = x - 1\\n----> 7     return func1(a, b)\\nipdb> print(x)\\n1\\nipdb> up\\n> <ipython-input-6-b2e110f6fc8f>(1)<module>()\\n----> 1 func2(1)\\nipdb> down\\n> <ipython-input-1-d849e34d61fb>(7)func2()\\n      5     a = x\\n      6     b = x - 1\\n----> 7     return func1(a, b)\\nipdb> quit\\nThis allows you to quickly find out not only what caused the error, but also what\\nfunction calls led up to the error.\\nIf you’d like the debugger to launch automatically whenever an exception is raised,\\nyou can use the %pdb magic function to turn on this automatic behavior:\\nIn[9]: %xmode Plain\\n       %pdb on\\n       func2(1)\\nException reporting mode: Plain\\nAutomatic pdb calling has been turned ON\\nTraceback (most recent call last):\\n  File \"<ipython-input-9-569a67d2d312>\", line 3, in <module>\\n    func2(1)\\n  File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2\\n    return func1(a, b)\\nErrors and Debugging \\n| \\n23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 41}, page_content='File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1\\n    return a / b\\nZeroDivisionError: division by zero\\n> <ipython-input-1-d849e34d61fb>(2)func1()\\n      1 def func1(a, b):\\n----> 2     return a / b\\n      3\\nipdb> print(b)\\n0\\nipdb> quit\\nFinally, if you have a script that you’d like to run from the beginning in interactive\\nmode, you can run it with the command %run -d, and use the next command to step\\nthrough the lines of code interactively.\\nPartial list of debugging commands\\nThere are many more available commands for interactive debugging than we’ve listed\\nhere; the following table contains a description of some of the more common and\\nuseful ones:\\nCommand\\nDescription\\nlist\\nShow the current location in the file\\nh(elp)\\nShow a list of commands, or find help on a specific command\\nq(uit)\\nQuit the debugger and the program\\nc(ontinue)\\nQuit the debugger; continue in the program\\nn(ext)\\nGo to the next step of the program\\n<enter>\\nRepeat the previous command\\np(rint)\\nPrint variables\\ns(tep)\\nStep into a subroutine\\nr(eturn)\\nReturn out of a subroutine\\nFor more information, use the help command in the debugger, or take a look at\\nipdb’s online documentation.\\n24 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 42}, page_content='Profiling and Timing Code\\nIn the process of developing code and creating data processing pipelines, there are\\noften trade-offs you can make between various implementations. Early in developing\\nyour algorithm, it can be counterproductive to worry about such things. As Donald\\nKnuth famously quipped, “We should forget about small efficiencies, say about 97%\\nof the time: premature optimization is the root of all evil.”\\nBut once you have your code working, it can be useful to dig into its efficiency a bit.\\nSometimes it’s useful to check the execution time of a given command or set of com‐\\nmands; other times it’s useful to dig into a multiline process and determine where the\\nbottleneck lies in some complicated series of operations. IPython provides access to a\\nwide array of functionality for this kind of timing and profiling of code. Here we’ll\\ndiscuss the following IPython magic commands:\\n%time\\nTime the execution of a single statement\\n%timeit\\nTime repeated execution of a single statement for more accuracy\\n%prun\\nRun code with the profiler\\n%lprun\\nRun code with the line-by-line profiler\\n%memit\\nMeasure the memory use of a single statement\\n%mprun\\nRun code with the line-by-line memory profiler\\nThe last four commands are not bundled with IPython—you’ll need to install the\\nline_profiler and memory_profiler extensions, which we will discuss in the fol‐\\nlowing sections.\\nTiming Code Snippets: %timeit and %time\\nWe saw the %timeit line magic and %%timeit cell magic in the introduction to magic\\nfunctions in “IPython Magic Commands” on page 10; %%timeit can be used to time\\nthe repeated execution of snippets of code:\\nIn[1]: %timeit sum(range(100))\\n100000 loops, best of 3: 1.54 µs per loop\\nProfiling and Timing Code \\n| \\n25'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 43}, page_content='Note that because this operation is so fast, %timeit automatically does a large number\\nof repetitions. For slower commands, %timeit will automatically adjust and perform\\nfewer repetitions:\\nIn[2]: %%timeit\\n       total = 0\\n       for i in range(1000):\\n           for j in range(1000):\\n               total += i * (-1) ** j\\n1 loops, best of 3: 407 ms per loop\\nSometimes repeating an operation is not the best option. For example, if we have a\\nlist that we’d like to sort, we might be misled by a repeated operation. Sorting a pre-\\nsorted list is much faster than sorting an unsorted list, so the repetition will skew the\\nresult:\\nIn[3]: import random\\n       L = [random.random() for i in range(100000)]\\n       %timeit L.sort()\\n100 loops, best of 3: 1.9 ms per loop\\nFor this, the %time magic function may be a better choice. It also is a good choice for\\nlonger-running commands, when short, system-related delays are unlikely to affect\\nthe result. Let’s time the sorting of an unsorted and a presorted list:\\nIn[4]: import random\\n       L = [random.random() for i in range(100000)]\\n       print(\"sorting an unsorted list:\")\\n       %time L.sort()\\nsorting an unsorted list:\\nCPU times: user 40.6 ms, sys: 896 µs, total: 41.5 ms\\nWall time: 41.5 ms\\nIn[5]: print(\"sorting an already sorted list:\")\\n       %time L.sort()\\nsorting an already sorted list:\\nCPU times: user 8.18 ms, sys: 10 µs, total: 8.19 ms\\nWall time: 8.24 ms\\nNotice how much faster the presorted list is to sort, but notice also how much longer\\nthe timing takes with %time versus %timeit, even for the presorted list! This is a\\nresult of the fact that %timeit does some clever things under the hood to prevent sys‐\\ntem calls from interfering with the timing. For example, it prevents cleanup of unused\\nPython objects (known as garbage collection) that might otherwise affect the timing.\\nFor this reason, %timeit results are usually noticeably faster than %time results.\\nFor %time as with %timeit, using the double-percent-sign cell-magic syntax allows\\ntiming of multiline scripts:\\n26 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 44}, page_content='In[6]: %%time\\n       total = 0\\n       for i in range(1000):\\n           for j in range(1000):\\n               total += i * (-1) ** j\\nCPU times: user 504 ms, sys: 979 µs, total: 505 ms\\nWall time: 505 ms\\nFor more information on %time and %timeit, as well as their available options, use\\nthe IPython help functionality (i.e., type %time? at the IPython prompt).\\nProfiling Full Scripts: %prun\\nA program is made of many single statements, and sometimes timing these state‐\\nments in context is more important than timing them on their own. Python contains\\na built-in code profiler (which you can read about in the Python documentation), but\\nIPython offers a much more convenient way to use this profiler, in the form of the\\nmagic function %prun.\\nBy way of example, we’ll define a simple function that does some calculations:\\nIn[7]: def sum_of_lists(N):\\n           total = 0\\n           for i in range(5):\\n               L = [j ^ (j >> i) for j in range(N)]\\n               total += sum(L)\\n           return total\\nNow we can call %prun with a function call to see the profiled results:\\nIn[8]: %prun sum_of_lists(1000000)\\nIn the notebook, the output is printed to the pager, and looks something like this:\\n14 function calls in 0.714 seconds\\n   Ordered by: internal time\\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n        5    0.599    0.120    0.599    0.120 <ipython-input-19>:4(<listcomp>)\\n        5    0.064    0.013    0.064    0.013 {built-in method sum}\\n        1    0.036    0.036    0.699    0.699 <ipython-input-19>:1(sum_of_lists)\\n        1    0.014    0.014    0.714    0.714 <string>:1(<module>)\\n        1    0.000    0.000    0.714    0.714 {built-in method exec}\\nThe result is a table that indicates, in order of total time on each function call, where\\nthe execution is spending the most time. In this case, the bulk of execution time is in\\nthe list comprehension inside sum_of_lists. From here, we could start thinking\\nabout what changes we might make to improve the performance in the algorithm.\\nProfiling and Timing Code \\n| \\n27'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 45}, page_content='For more information on %prun, as well as its available options, use the IPython help\\nfunctionality (i.e., type %prun? at the IPython prompt).\\nLine-by-Line Profiling with %lprun\\nThe function-by-function profiling of %prun is useful, but sometimes it’s more conve‐\\nnient to have a line-by-line profile report. This is not built into Python or IPython,\\nbut there is a line_profiler package available for installation that can do this. Start\\nby using Python’s packaging tool, pip, to install the line_profiler package:\\n$ pip install line_profiler\\nNext, you can use IPython to load the line_profiler IPython extension, offered as\\npart of this package:\\nIn[9]: %load_ext line_profiler\\nNow the %lprun command will do a line-by-line profiling of any function—in this\\ncase, we need to tell it explicitly which functions we’re interested in profiling:\\nIn[10]: %lprun -f sum_of_lists sum_of_lists(5000)\\nAs before, the notebook sends the result to the pager, but it looks something like this:\\nTimer unit: 1e-06 s\\nTotal time: 0.009382 s\\nFile: <ipython-input-19-fa2be176cc3e>\\nFunction: sum_of_lists at line 1\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     1                                           def sum_of_lists(N):\\n     2         1            2      2.0      0.0      total = 0\\n     3         6            8      1.3      0.1      for i in range(5):\\n     4         5         9001   1800.2     95.9          L = [j ^ (j >> i) ...\\n     5         5          371     74.2      4.0          total += sum(L)\\n     6         1            0      0.0      0.0      return total\\nThe information at the top gives us the key to reading the results: the time is reported\\nin microseconds and we can see where the program is spending the most time. At this\\npoint, we may be able to use this information to modify aspects of the script and\\nmake it perform better for our desired use case.\\nFor more information on %lprun, as well as its available options, use the IPython help\\nfunctionality (i.e., type %lprun? at the IPython prompt).\\n28 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 46}, page_content='Profiling Memory Use: %memit and %mprun\\nAnother aspect of profiling is the amount of memory an operation uses. This can be\\nevaluated with another IPython extension, the memory_profiler. As with the\\nline_profiler, we start by pip-installing the extension:\\n$ pip install memory_profiler\\nThen we can use IPython to load the extension:\\nIn[12]: %load_ext memory_profiler\\nThe memory profiler extension contains two useful magic functions: the %memit\\nmagic (which offers a memory-measuring equivalent of %timeit) and the %mprun\\nfunction (which offers a memory-measuring equivalent of %lprun). The %memit func‐\\ntion can be used rather simply:\\nIn[13]: %memit sum_of_lists(1000000)\\npeak memory: 100.08 MiB, increment: 61.36 MiB\\nWe see that this function uses about 100 MB of memory.\\nFor a line-by-line description of memory use, we can use the %mprun magic. Unfortu‐\\nnately, this magic works only for functions defined in separate modules rather than\\nthe notebook itself, so we’ll start by using the %%file magic to create a simple module\\ncalled mprun_demo.py, which contains our sum_of_lists function, with one addition\\nthat will make our memory profiling results more clear:\\nIn[14]: %%file mprun_demo.py\\n        def sum_of_lists(N):\\n            total = 0\\n            for i in range(5):\\n                L = [j ^ (j >> i) for j in range(N)]\\n                total += sum(L)\\n                del L # remove reference to L\\n            return total\\nOverwriting mprun_demo.py\\nWe can now import the new version of this function and run the memory line\\nprofiler:\\nIn[15]: from mprun_demo import sum_of_lists\\n        %mprun -f sum_of_lists sum_of_lists(1000000)\\nThe result, printed to the pager, gives us a summary of the memory use of the func‐\\ntion, and looks something like this:\\nProfiling and Timing Code \\n| \\n29'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 47}, page_content='Filename: ./mprun_demo.py\\nLine #    Mem usage    Increment   Line Contents\\n================================================\\n     4     71.9 MiB      0.0 MiB           L = [j ^ (j >> i) for j in range(N)]\\nFilename: ./mprun_demo.py\\nLine #    Mem usage    Increment   Line Contents\\n================================================\\n     1     39.0 MiB      0.0 MiB   def sum_of_lists(N):\\n     2     39.0 MiB      0.0 MiB       total = 0\\n     3     46.5 MiB      7.5 MiB       for i in range(5):\\n     4     71.9 MiB     25.4 MiB           L = [j ^ (j >> i) for j in range(N)]\\n     5     71.9 MiB      0.0 MiB           total += sum(L)\\n     6     46.5 MiB    -25.4 MiB           del L # remove reference to L\\n     7     39.1 MiB     -7.4 MiB       return total\\nHere the Increment column tells us how much each line affects the total memory\\nbudget: observe that when we create and delete the list L, we are adding about 25 MB\\nof memory usage. This is on top of the background memory usage from the Python\\ninterpreter itself.\\nFor more information on %memit and %mprun, as well as their available options, use\\nthe IPython help functionality (i.e., type %memit? at the IPython prompt).\\nMore IPython Resources\\nIn this chapter, we’ve just scratched the surface of using IPython to enable data sci‐\\nence tasks. Much more information is available both in print and on the Web, and\\nhere we’ll list some other resources that you may find helpful.\\nWeb Resources\\nThe IPython website\\nThe IPython website links to documentation, examples, tutorials, and a variety of\\nother resources.\\nThe nbviewer website\\nThis site shows static renderings of any IPython notebook available on the Inter‐\\nnet. The front page features some example notebooks that you can browse to see\\nwhat other folks are using IPython for!\\n30 \\n| \\nChapter 1: IPython: Beyond Normal Python'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 48}, page_content='A Gallery of Interesting IPython Notebooks\\nThis ever-growing list of notebooks, powered by nbviewer, shows the depth and\\nbreadth of numerical analysis you can do with IPython. It includes everything\\nfrom short examples and tutorials to full-blown courses and books composed in\\nthe notebook format!\\nVideo tutorials\\nSearching the Internet, you will find many video-recorded tutorials on IPython.\\nI’d especially recommend seeking tutorials from the PyCon, SciPy, and PyData\\nconferences by Fernando Perez and Brian Granger, two of the primary creators\\nand maintainers of IPython and Jupyter.\\nBooks\\nPython for Data Analysis\\nWes McKinney’s book includes a chapter that covers using IPython as a data sci‐\\nentist. Although much of the material overlaps what we’ve discussed here,\\nanother perspective is always helpful.\\nLearning IPython for Interactive Computing and Data Visualization\\nThis short book by Cyrille Rossant offers a good introduction to using IPython\\nfor data analysis.\\nIPython Interactive Computing and Visualization Cookbook\\nAlso by Cyrille Rossant, this book is a longer and more advanced treatment of\\nusing IPython for data science. Despite its name, it’s not just about IPython—it\\nalso goes into some depth on a broad range of data science topics.\\nFinally, a reminder that you can find help on your own: IPython’s ?-based help func‐\\ntionality (discussed in “Help and Documentation in IPython” on page 3) can be very\\nuseful if you use it well and use it often. As you go through the examples here and\\nelsewhere, you can use it to familiarize yourself with all the tools that IPython has to\\noffer.\\nMore IPython Resources \\n| \\n31'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 49}, page_content='www.allitebooks.com'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 50}, page_content='CHAPTER 2\\nIntroduction to NumPy\\nThis chapter, along with Chapter 3, outlines techniques for effectively loading, stor‐\\ning, and manipulating in-memory data in Python. The topic is very broad: datasets\\ncan come from a wide range of sources and a wide range of formats, including collec‐\\ntions of documents, collections of images, collections of sound clips, collections of\\nnumerical measurements, or nearly anything else. Despite this apparent heterogene‐\\nity, it will help us to think of all data fundamentally as arrays of numbers.\\nFor example, images—particularly digital images—can be thought of as simply two-\\ndimensional arrays of numbers representing pixel brightness across the area. Sound\\nclips can be thought of as one-dimensional arrays of intensity versus time. Text can be\\nconverted in various ways into numerical representations, perhaps binary digits rep‐\\nresenting the frequency of certain words or pairs of words. No matter what the data\\nare, the first step in making them analyzable will be to transform them into arrays of\\nnumbers. (We will discuss some specific examples of this process later in “Feature\\nEngineering” on page 375.)\\nFor this reason, efficient storage and manipulation of numerical arrays is absolutely\\nfundamental to the process of doing data science. We’ll now take a look at the special‐\\nized tools that Python has for handling such numerical arrays: the NumPy package\\nand the Pandas package (discussed in Chapter 3.)\\nThis chapter will cover NumPy in detail. NumPy (short for Numerical Python) pro‐\\nvides an efficient interface to store and operate on dense data buffers. In some ways,\\nNumPy arrays are like Python’s built-in list type, but NumPy arrays provide much\\nmore efficient storage and data operations as the arrays grow larger in size. NumPy\\narrays form the core of nearly the entire ecosystem of data science tools in Python, so\\ntime spent learning to use NumPy effectively will be valuable no matter what aspect\\nof data science interests you.\\n33'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 51}, page_content=\"If you followed the advice outlined in the preface and installed the Anaconda stack,\\nyou already have NumPy installed and ready to go. If you’re more the do-it-yourself\\ntype, you can go to the NumPy website and follow the installation instructions found\\nthere. Once you do, you can import NumPy and double-check the version:\\nIn[1]: import numpy\\n       numpy.__version__\\nOut[1]: '1.11.1'\\nFor the pieces of the package discussed here, I’d recommend NumPy version 1.8 or\\nlater. By convention, you’ll find that most people in the SciPy/PyData world will\\nimport NumPy using np as an alias:\\nIn[2]: import numpy as np\\nThroughout this chapter, and indeed the rest of the book, you’ll find that this is the\\nway we will import and use NumPy.\\nReminder About Built-In Documentation\\nAs you read through this chapter, don’t forget that IPython gives you the ability to\\nquickly explore the contents of a package (by using the tab-completion feature) as\\nwell as the documentation of various functions (using the ? character). Refer back to\\n“Help and Documentation in IPython” on page 3 if you need a refresher on this.\\nFor example, to display all the contents of the numpy namespace, you can type this:\\nIn [3]: np.<TAB>\\nAnd to display NumPy’s built-in documentation, you can use this:\\nIn [4]: np?\\nMore detailed documentation, along with tutorials and other resources, can be found\\nat http://www.numpy.org.\\nUnderstanding Data Types in Python\\nEffective data-driven science and computation requires understanding how data is\\nstored and manipulated. This section outlines and contrasts how arrays of data are\\nhandled in the Python language itself, and how NumPy improves on this. Under‐\\nstanding this difference is fundamental to understanding much of the material\\nthroughout the rest of the book.\\nUsers of Python are often drawn in by its ease of use, one piece of which is dynamic\\ntyping. While a statically typed language like C or Java requires each variable to be\\n34 \\n| \\nChapter 2: Introduction to NumPy\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 52}, page_content='explicitly declared, a dynamically typed language like Python skips this specification.\\nFor example, in C you might specify a particular operation as follows:\\n/* C code */\\nint result = 0;\\nfor(int i=0; i<100; i++){\\n    result += i;\\n}\\nWhile in Python the equivalent operation could be written this way:\\n# Python code\\nresult = 0\\nfor i in range(100):\\n    result += i\\nNotice the main difference: in C, the data types of each variable are explicitly\\ndeclared, while in Python the types are dynamically inferred. This means, for exam‐\\nple, that we can assign any kind of data to any variable:\\n# Python code\\nx = 4\\nx = \"four\"\\nHere we’ve switched the contents of x from an integer to a string. The same thing in C\\nwould lead (depending on compiler settings) to a compilation error or other uninten‐\\nded consequences:\\n/* C code */\\nint x = 4;\\nx = \"four\";  // FAILS\\nThis sort of flexibility is one piece that makes Python and other dynamically typed\\nlanguages convenient and easy to use. Understanding how this works is an important\\npiece of learning to analyze data efficiently and effectively with Python. But what this\\ntype flexibility also points to is the fact that Python variables are more than just their\\nvalue; they also contain extra information about the type of the value. We’ll explore\\nthis more in the sections that follow.\\nA Python Integer Is More Than Just an Integer\\nThe standard Python implementation is written in C. This means that every Python\\nobject is simply a cleverly disguised C structure, which contains not only its value, but\\nother information as well. For example, when we define an integer in Python, such as\\nx = 10000, x is not just a “raw” integer. It’s actually a pointer to a compound C struc‐\\nture, which contains several values. Looking through the Python 3.4 source code, we\\nfind that the integer (long) type definition effectively looks like this (once the C mac‐\\nros are expanded):\\nUnderstanding Data Types in Python \\n| \\n35'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 53}, page_content='struct _longobject {\\n    long ob_refcnt;\\n    PyTypeObject *ob_type;\\n    size_t ob_size;\\n    long ob_digit[1];\\n};\\nA single integer in Python 3.4 actually contains four pieces:\\n• ob_refcnt, a reference count that helps Python silently handle memory alloca‐\\ntion and deallocation\\n• ob_type, which encodes the type of the variable\\n• ob_size, which specifies the size of the following data members\\n• ob_digit, which contains the actual integer value that we expect the Python vari‐\\nable to represent\\nThis means that there is some overhead in storing an integer in Python as compared\\nto an integer in a compiled language like C, as illustrated in Figure 2-1.\\nFigure 2-1. The difference between C and Python integers\\nHere PyObject_HEAD is the part of the structure containing the reference count, type\\ncode, and other pieces mentioned before.\\nNotice the difference here: a C integer is essentially a label for a position in memory\\nwhose bytes encode an integer value. A Python integer is a pointer to a position in\\nmemory containing all the Python object information, including the bytes that con‐\\ntain the integer value. This extra information in the Python integer structure is what\\nallows Python to be coded so freely and dynamically. All this additional information\\nin Python types comes at a cost, however, which becomes especially apparent in\\nstructures that combine many of these objects.\\n36 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 54}, page_content='A Python List Is More Than Just a List\\nLet’s consider now what happens when we use a Python data structure that holds\\nmany Python objects. The standard mutable multielement container in Python is the\\nlist. We can create a list of integers as follows:\\nIn[1]: L = list(range(10))\\n       L\\nOut[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\nIn[2]: type(L[0])\\nOut[2]: int\\nOr, similarly, a list of strings:\\nIn[3]: L2 = [str(c) for c in L]\\n       L2\\nOut[3]: [\\'0\\', \\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\']\\nIn[4]: type(L2[0])\\nOut[4]: str\\nBecause of Python’s dynamic typing, we can even create heterogeneous lists:\\nIn[5]: L3 = [True, \"2\", 3.0, 4]\\n       [type(item) for item in L3]\\nOut[5]: [bool, str, float, int]\\nBut this flexibility comes at a cost: to allow these flexible types, each item in the list\\nmust contain its own type info, reference count, and other information—that is, each\\nitem is a complete Python object. In the special case that all variables are of the same\\ntype, much of this information is redundant: it can be much more efficient to store\\ndata in a fixed-type array. The difference between a dynamic-type list and a fixed-type\\n(NumPy-style) array is illustrated in Figure 2-2.\\nAt the implementation level, the array essentially contains a single pointer to one con‐\\ntiguous block of data. The Python list, on the other hand, contains a pointer to a\\nblock of pointers, each of which in turn points to a full Python object like the Python\\ninteger we saw earlier. Again, the advantage of the list is flexibility: because each list\\nelement is a full structure containing both data and type information, the list can be\\nfilled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil‐\\nity, but are much more efficient for storing and manipulating data.\\nUnderstanding Data Types in Python \\n| \\n37'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 55}, page_content=\"Figure 2-2. The difference between C and Python lists\\nFixed-Type Arrays in Python\\nPython offers several different options for storing data in efficient, fixed-type data\\nbuffers. The built-in array module (available since Python 3.3) can be used to create\\ndense arrays of a uniform type:\\nIn[6]: import array\\n       L = list(range(10))\\n       A = array.array('i', L)\\n       A\\nOut[6]: array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nHere 'i' is a type code indicating the contents are integers.\\nMuch more useful, however, is the ndarray object of the NumPy package. While\\nPython’s array object provides efficient storage of array-based data, NumPy adds to\\nthis efficient operations on that data. We will explore these operations in later sec‐\\ntions; here we’ll demonstrate several ways of creating a NumPy array.\\nWe’ll start with the standard NumPy import, under the alias np:\\nIn[7]: import numpy as np\\n38 \\n| \\nChapter 2: Introduction to NumPy\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 56}, page_content=\"Creating Arrays from Python Lists\\nFirst, we can use np.array to create arrays from Python lists:\\nIn[8]: # integer array:\\n       np.array([1, 4, 2, 5, 3])\\nOut[8]: array([1, 4, 2, 5, 3])\\nRemember that unlike Python lists, NumPy is constrained to arrays that all contain\\nthe same type. If types do not match, NumPy will upcast if possible (here, integers are\\nupcast to floating point):\\nIn[9]: np.array([3.14, 4, 2, 3])\\nOut[9]: array([ 3.14,  4.  ,  2.  ,  3.  ])\\nIf we want to explicitly set the data type of the resulting array, we can use the dtype\\nkeyword:\\nIn[10]: np.array([1, 2, 3, 4], dtype='float32')\\nOut[10]: array([ 1.,  2.,  3.,  4.], dtype=float32)\\nFinally, unlike Python lists, NumPy arrays can explicitly be multidimensional; here’s\\none way of initializing a multidimensional array using a list of lists:\\nIn[11]: # nested lists result in multidimensional arrays\\n        np.array([range(i, i + 3) for i in [2, 4, 6]])\\nOut[11]: array([[2, 3, 4],\\n                [4, 5, 6],\\n                [6, 7, 8]])\\nThe inner lists are treated as rows of the resulting two-dimensional array.\\nCreating Arrays from Scratch\\nEspecially for larger arrays, it is more efficient to create arrays from scratch using rou‐\\ntines built into NumPy. Here are several examples:\\nIn[12]: # Create a length-10 integer array filled with zeros\\n        np.zeros(10, dtype=int)\\nOut[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\\nIn[13]: # Create a 3x5 floating-point array filled with 1s\\n        np.ones((3, 5), dtype=float)\\nOut[13]: array([[ 1.,  1.,  1.,  1.,  1.],\\n                [ 1.,  1.,  1.,  1.,  1.],\\n                [ 1.,  1.,  1.,  1.,  1.]])\\nIn[14]: # Create a 3x5 array filled with 3.14\\n        np.full((3, 5), 3.14)\\nUnderstanding Data Types in Python \\n| \\n39\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 57}, page_content='Out[14]: array([[ 3.14,  3.14,  3.14,  3.14,  3.14],\\n                [ 3.14,  3.14,  3.14,  3.14,  3.14],\\n                [ 3.14,  3.14,  3.14,  3.14,  3.14]])\\nIn[15]: # Create an array filled with a linear sequence\\n        # Starting at 0, ending at 20, stepping by 2\\n        # (this is similar to the built-in range() function)\\n        np.arange(0, 20, 2)\\nOut[15]: array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\\nIn[16]: # Create an array of five values evenly spaced between 0 and 1\\n        np.linspace(0, 1, 5)\\nOut[16]: array([ 0.  ,  0.25,  0.5 ,  0.75,  1.  ])\\nIn[17]: # Create a 3x3 array of uniformly distributed\\n        # random values between 0 and 1\\n        np.random.random((3, 3))\\nOut[17]: array([[ 0.99844933,  0.52183819,  0.22421193],\\n                [ 0.08007488,  0.45429293,  0.20941444],\\n                [ 0.14360941,  0.96910973,  0.946117  ]])\\nIn[18]: # Create a 3x3 array of normally distributed random values\\n        # with mean 0 and standard deviation 1\\n        np.random.normal(0, 1, (3, 3))\\nOut[18]: array([[ 1.51772646,  0.39614948, -0.10634696],\\n                [ 0.25671348,  0.00732722,  0.37783601],\\n                [ 0.68446945,  0.15926039, -0.70744073]])\\nIn[19]: # Create a 3x3 array of random integers in the interval [0, 10)\\n        np.random.randint(0, 10, (3, 3))\\nOut[19]: array([[2, 3, 4],\\n                [5, 7, 8],\\n                [0, 5, 0]])\\nIn[20]: # Create a 3x3 identity matrix\\n        np.eye(3)\\nOut[20]: array([[ 1.,  0.,  0.],\\n                [ 0.,  1.,  0.],\\n                [ 0.,  0.,  1.]])\\nIn[21]: # Create an uninitialized array of three integers\\n        # The values will be whatever happens to already exist at that\\n        # memory location\\n        np.empty(3)\\nOut[21]: array([ 1.,  1.,  1.])\\n40 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 58}, page_content=\"NumPy Standard Data Types\\nNumPy arrays contain values of a single type, so it is important to have detailed\\nknowledge of those types and their limitations. Because NumPy is built in C, the\\ntypes will be familiar to users of C, Fortran, and other related languages.\\nThe standard NumPy data types are listed in Table 2-1. Note that when constructing\\nan array, you can specify them using a string:\\nnp.zeros(10, dtype='int16')\\nOr using the associated NumPy object:\\nnp.zeros(10, dtype=np.int16)\\nTable 2-1. Standard NumPy data types\\nData type\\nDescription\\nbool_\\nBoolean (True or False) stored as a byte\\nint_\\nDefault integer type (same as C long; normally either int64 or int32)\\nintc\\nIdentical to C int (normally int32 or int64)\\nintp\\nInteger used for indexing (same as C ssize_t; normally either int32 or int64)\\nint8\\nByte (–128 to 127)\\nint16\\nInteger (–32768 to 32767)\\nint32\\nInteger (–2147483648 to 2147483647)\\nint64\\nInteger (–9223372036854775808 to 9223372036854775807)\\nuint8\\nUnsigned integer (0 to 255)\\nuint16\\nUnsigned integer (0 to 65535)\\nuint32\\nUnsigned integer (0 to 4294967295)\\nuint64\\nUnsigned integer (0 to 18446744073709551615)\\nfloat_\\nShorthand for float64\\nfloat16\\nHalf-precision float: sign bit, 5 bits exponent, 10 bits mantissa\\nfloat32\\nSingle-precision float: sign bit, 8 bits exponent, 23 bits mantissa\\nfloat64\\nDouble-precision float: sign bit, 11 bits exponent, 52 bits mantissa\\ncomplex_\\nShorthand for complex128\\ncomplex64\\nComplex number, represented by two 32-bit floats\\ncomplex128\\nComplex number, represented by two 64-bit floats\\nMore advanced type specification is possible, such as specifying big or little endian\\nnumbers; for more information, refer to the NumPy documentation. NumPy also\\nsupports compound data types, which will be covered in “Structured Data: NumPy’s\\nStructured Arrays” on page 92.\\nUnderstanding Data Types in Python \\n| \\n41\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 59}, page_content='The Basics of NumPy Arrays\\nData manipulation in Python is nearly synonymous with NumPy array manipulation:\\neven newer tools like Pandas (Chapter 3) are built around the NumPy array. This sec‐\\ntion will present several examples using NumPy array manipulation to access data\\nand subarrays, and to split, reshape, and join the arrays. While the types of operations\\nshown here may seem a bit dry and pedantic, they comprise the building blocks of\\nmany other examples used throughout the book. Get to know them well!\\nWe’ll cover a few categories of basic array manipulations here:\\nAttributes of arrays\\nDetermining the size, shape, memory consumption, and data types of arrays\\nIndexing of arrays\\nGetting and setting the value of individual array elements\\nSlicing of arrays\\nGetting and setting smaller subarrays within a larger array\\nReshaping of arrays\\nChanging the shape of a given array\\nJoining and splitting of arrays\\nCombining multiple arrays into one, and splitting one array into many\\nNumPy Array Attributes\\nFirst let’s discuss some useful array attributes. We’ll start by defining three random\\narrays: a one-dimensional, two-dimensional, and three-dimensional array. We’ll use\\nNumPy’s random number generator, which we will seed with a set value in order to\\nensure that the same random arrays are generated each time this code is run:\\nIn[1]: import numpy as np\\n       np.random.seed(0)  # seed for reproducibility\\n       x1 = np.random.randint(10, size=6)  # One-dimensional array\\n       x2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array\\n       x3 = np.random.randint(10, size=(3, 4, 5))  # Three-dimensional array\\nEach array has attributes ndim (the number of dimensions), shape (the size of each\\ndimension), and size (the total size of the array):\\nIn[2]: print(\"x3 ndim: \", x3.ndim)\\n       print(\"x3 shape:\", x3.shape)\\n       print(\"x3 size: \", x3.size)\\nx3 ndim:  3\\nx3 shape: (3, 4, 5)\\nx3 size:  60\\n42 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 60}, page_content='Another useful attribute is the dtype, the data type of the array (which we discussed\\npreviously in “Understanding Data Types in Python” on page 34):\\nIn[3]: print(\"dtype:\", x3.dtype)\\ndtype: int64\\nOther attributes include itemsize, which lists the size (in bytes) of each array ele‐\\nment, and nbytes, which lists the total size (in bytes) of the array:\\nIn[4]: print(\"itemsize:\", x3.itemsize, \"bytes\")\\n       print(\"nbytes:\", x3.nbytes, \"bytes\")\\nitemsize: 8 bytes\\nnbytes: 480 bytes\\nIn general, we expect that nbytes is equal to itemsize times size.\\nArray Indexing: Accessing Single Elements\\nIf you are familiar with Python’s standard list indexing, indexing in NumPy will feel\\nquite familiar. In a one-dimensional array, you can access the ith value (counting from\\nzero) by specifying the desired index in square brackets, just as with Python lists:\\nIn[5]: x1\\nOut[5]: array([5, 0, 3, 3, 7, 9])\\nIn[6]: x1[0]\\nOut[6]: 5\\nIn[7]: x1[4]\\nOut[7]: 7\\nTo index from the end of the array, you can use negative indices:\\nIn[8]: x1[-1]\\nOut[8]: 9\\nIn[9]: x1[-2]\\nOut[9]: 7\\nIn a multidimensional array, you access items using a comma-separated tuple of\\nindices:\\nIn[10]: x2\\nOut[10]: array([[3, 5, 2, 4],\\n                [7, 6, 8, 8],\\n                [1, 6, 7, 7]])\\nIn[11]: x2[0, 0]\\nOut[11]: 3\\nThe Basics of NumPy Arrays \\n| \\n43'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 61}, page_content='In[12]: x2[2, 0]\\nOut[12]: 1\\nIn[13]: x2[2, -1]\\nOut[13]: 7\\nYou can also modify values using any of the above index notation:\\nIn[14]: x2[0, 0] = 12\\n        x2\\nOut[14]: array([[12,  5,  2,  4],\\n                [ 7,  6,  8,  8],\\n                [ 1,  6,  7,  7]])\\nKeep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means,\\nfor example, that if you attempt to insert a floating-point value to an integer array, the\\nvalue will be silently truncated. Don’t be caught unaware by this behavior!\\nIn[15]: x1[0] = 3.14159  # this will be truncated!\\n        x1\\nOut[15]: array([3, 0, 3, 3, 7, 9])\\nArray Slicing: Accessing Subarrays\\nJust as we can use square brackets to access individual array elements, we can also use\\nthem to access subarrays with the slice notation, marked by the colon (:) character.\\nThe NumPy slicing syntax follows that of the standard Python list; to access a slice of\\nan array x, use this:\\nx[start:stop:step]\\nIf any of these are unspecified, they default to the values start=0, stop=size of\\ndimension, step=1. We’ll take a look at accessing subarrays in one dimension and in\\nmultiple dimensions.\\nOne-dimensional subarrays\\nIn[16]: x = np.arange(10)\\n        x\\nOut[16]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nIn[17]: x[:5]  # first five elements\\nOut[17]: array([0, 1, 2, 3, 4])\\nIn[18]: x[5:]  # elements after index 5\\nOut[18]: array([5, 6, 7, 8, 9])\\nIn[19]: x[4:7]  # middle subarray\\nOut[19]: array([4, 5, 6])\\n44 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 62}, page_content='In[20]: x[::2]  # every other element\\nOut[20]: array([0, 2, 4, 6, 8])\\nIn[21]: x[1::2]  # every other element, starting at index 1\\nOut[21]: array([1, 3, 5, 7, 9])\\nA potentially confusing case is when the step value is negative. In this case, the\\ndefaults for start and stop are swapped. This becomes a convenient way to reverse\\nan array:\\nIn[22]: x[::-1]  # all elements, reversed\\nOut[22]: array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\\nIn[23]: x[5::-2]  # reversed every other from index 5\\nOut[23]: array([5, 3, 1])\\nMultidimensional subarrays\\nMultidimensional slices work in the same way, with multiple slices separated by com‐\\nmas. For example:\\nIn[24]: x2\\nOut[24]: array([[12,  5,  2,  4],\\n                [ 7,  6,  8,  8],\\n                [ 1,  6,  7,  7]])\\nIn[25]: x2[:2, :3]  # two rows, three columns\\nOut[25]: array([[12,  5,  2],\\n                [ 7,  6,  8]])\\nIn[26]: x2[:3, ::2]  # all rows, every other column\\nOut[26]: array([[12,  2],\\n                [ 7,  8],\\n                [ 1,  7]])\\nFinally, subarray dimensions can even be reversed together:\\nIn[27]: x2[::-1, ::-1]\\nOut[27]: array([[ 7,  7,  6,  1],\\n                [ 8,  8,  6,  7],\\n                [ 4,  2,  5, 12]])\\nAccessing array rows and columns.    One commonly needed routine is accessing single\\nrows or columns of an array. You can do this by combining indexing and slicing,\\nusing an empty slice marked by a single colon (:):\\nIn[28]: print(x2[:, 0])  # first column of x2\\n[12  7  1]\\nThe Basics of NumPy Arrays \\n| \\n45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 63}, page_content='In[29]: print(x2[0, :])  # first row of x2\\n[12  5  2  4]\\nIn the case of row access, the empty slice can be omitted for a more compact syntax:\\nIn[30]: print(x2[0])  # equivalent to x2[0, :]\\n[12  5  2  4]\\nSubarrays as no-copy views\\nOne important—and extremely useful—thing to know about array slices is that they\\nreturn views rather than copies of the array data. This is one area in which NumPy\\narray slicing differs from Python list slicing: in lists, slices will be copies. Consider our\\ntwo-dimensional array from before:\\nIn[31]: print(x2)\\n[[12  5  2  4]\\n [ 7  6  8  8]\\n [ 1  6  7  7]]\\nLet’s extract a 2×2 subarray from this:\\nIn[32]: x2_sub = x2[:2, :2]\\n        print(x2_sub)\\n[[12  5]\\n [ 7  6]]\\nNow if we modify this subarray, we’ll see that the original array is changed! Observe:\\nIn[33]: x2_sub[0, 0] = 99\\n        print(x2_sub)\\n[[99  5]\\n [ 7  6]]\\nIn[34]: print(x2)\\n[[99  5  2  4]\\n [ 7  6  8  8]\\n [ 1  6  7  7]]\\nThis default behavior is actually quite useful: it means that when we work with large\\ndatasets, we can access and process pieces of these datasets without the need to copy\\nthe underlying data buffer.\\nCreating copies of arrays\\nDespite the nice features of array views, it is sometimes useful to instead explicitly\\ncopy the data within an array or a subarray. This can be most easily done with the\\ncopy() method:\\n46 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 64}, page_content='In[35]: x2_sub_copy = x2[:2, :2].copy()\\n        print(x2_sub_copy)\\n[[99  5]\\n [ 7  6]]\\nIf we now modify this subarray, the original array is not touched:\\nIn[36]: x2_sub_copy[0, 0] = 42\\n        print(x2_sub_copy)\\n[[42  5]\\n [ 7  6]]\\nIn[37]: print(x2)\\n[[99  5  2  4]\\n [ 7  6  8  8]\\n [ 1  6  7  7]]\\nReshaping of Arrays\\nAnother useful type of operation is reshaping of arrays. The most flexible way of\\ndoing this is with the reshape() method. For example, if you want to put the num‐\\nbers 1 through 9 in a 3×3 grid, you can do the following:\\nIn[38]: grid = np.arange(1, 10).reshape((3, 3))\\n        print(grid)\\n[[1 2 3]\\n [4 5 6]\\n [7 8 9]]\\nNote that for this to work, the size of the initial array must match the size of the\\nreshaped array. Where possible, the reshape method will use a no-copy view of the\\ninitial array, but with noncontiguous memory buffers this is not always the case.\\nAnother common reshaping pattern is the conversion of a one-dimensional array\\ninto a two-dimensional row or column matrix. You can do this with the reshape\\nmethod, or more easily by making use of the newaxis keyword within a slice opera‐\\ntion:\\nIn[39]: x = np.array([1, 2, 3])\\n        # row vector via reshape\\n        x.reshape((1, 3))\\nOut[39]: array([[1, 2, 3]])\\nIn[40]: # row vector via newaxis\\n        x[np.newaxis, :]\\nOut[40]: array([[1, 2, 3]])\\nThe Basics of NumPy Arrays \\n| \\n47'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 65}, page_content='In[41]: # column vector via reshape\\n        x.reshape((3, 1))\\nOut[41]: array([[1],\\n                [2],\\n                [3]])\\nIn[42]: # column vector via newaxis\\n        x[:, np.newaxis]\\nOut[42]: array([[1],\\n                [2],\\n                [3]])\\nWe will see this type of transformation often throughout the remainder of the book.\\nArray Concatenation and Splitting\\nAll of the preceding routines worked on single arrays. It’s also possible to combine\\nmultiple arrays into one, and to conversely split a single array into multiple arrays.\\nWe’ll take a look at those operations here.\\nConcatenation of arrays\\nConcatenation, or joining of two arrays in NumPy, is primarily accomplished\\nthrough the routines np.concatenate, np.vstack, and np.hstack. np.concatenate\\ntakes a tuple or list of arrays as its first argument, as we can see here:\\nIn[43]: x = np.array([1, 2, 3])\\n        y = np.array([3, 2, 1])\\n        np.concatenate([x, y])\\nOut[43]: array([1, 2, 3, 3, 2, 1])\\nYou can also concatenate more than two arrays at once:\\nIn[44]: z = [99, 99, 99]\\n        print(np.concatenate([x, y, z]))\\n[ 1  2  3  3  2  1 99 99 99]\\nnp.concatenate can also be used for two-dimensional arrays:\\nIn[45]: grid = np.array([[1, 2, 3],\\n                         [4, 5, 6]])\\nIn[46]: # concatenate along the first axis\\n        np.concatenate([grid, grid])\\nOut[46]: array([[1, 2, 3],\\n                [4, 5, 6],\\n                [1, 2, 3],\\n                [4, 5, 6]])\\nIn[47]: # concatenate along the second axis (zero-indexed)\\n        np.concatenate([grid, grid], axis=1)\\n48 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 66}, page_content='Out[47]: array([[1, 2, 3, 1, 2, 3],\\n                [4, 5, 6, 4, 5, 6]])\\nFor working with arrays of mixed dimensions, it can be clearer to use the np.vstack\\n(vertical stack) and np.hstack (horizontal stack) functions:\\nIn[48]: x = np.array([1, 2, 3])\\n        grid = np.array([[9, 8, 7],\\n                         [6, 5, 4]])\\n        # vertically stack the arrays\\n        np.vstack([x, grid])\\nOut[48]: array([[1, 2, 3],\\n                [9, 8, 7],\\n                [6, 5, 4]])\\nIn[49]: # horizontally stack the arrays\\n        y = np.array([[99],\\n                      [99]])\\n        np.hstack([grid, y])\\nOut[49]: array([[ 9,  8,  7, 99],\\n                [ 6,  5,  4, 99]])\\nSimilarly, np.dstack will stack arrays along the third axis.\\nSplitting of arrays\\nThe opposite of concatenation is splitting, which is implemented by the functions\\nnp.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices\\ngiving the split points:\\nIn[50]: x = [1, 2, 3, 99, 99, 3, 2, 1]\\n        x1, x2, x3 = np.split(x, [3, 5])\\n        print(x1, x2, x3)\\n[1 2 3] [99 99] [3 2 1]\\nNotice that N split points lead to N + 1 subarrays. The related functions np.hsplit\\nand np.vsplit are similar:\\nIn[51]: grid = np.arange(16).reshape((4, 4))\\n        grid\\nOut[51]: array([[ 0,  1,  2,  3],\\n                [ 4,  5,  6,  7],\\n                [ 8,  9, 10, 11],\\n                [12, 13, 14, 15]])\\nIn[52]: upper, lower = np.vsplit(grid, [2])\\n        print(upper)\\n        print(lower)\\n[[0 1 2 3]\\n [4 5 6 7]]\\nThe Basics of NumPy Arrays \\n| \\n49'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 67}, page_content='[[ 8  9 10 11]\\n [12 13 14 15]]\\nIn[53]: left, right = np.hsplit(grid, [2])\\n        print(left)\\n        print(right)\\n[[ 0  1]\\n [ 4  5]\\n [ 8  9]\\n [12 13]]\\n[[ 2  3]\\n [ 6  7]\\n [10 11]\\n [14 15]]\\nSimilarly, np.dsplit will split arrays along the third axis.\\nComputation on NumPy Arrays: Universal Functions\\nUp until now, we have been discussing some of the basic nuts and bolts of NumPy; in\\nthe next few sections, we will dive into the reasons that NumPy is so important in the\\nPython data science world. Namely, it provides an easy and flexible interface to opti‐\\nmized computation with arrays of data.\\nComputation on NumPy arrays can be very fast, or it can be very slow. The key to\\nmaking it fast is to use vectorized operations, generally implemented through Num‐\\nPy’s universal functions (ufuncs). This section motivates the need for NumPy’s ufuncs,\\nwhich can be used to make repeated calculations on array elements much more effi‐\\ncient. It then introduces many of the most common and useful arithmetic ufuncs\\navailable in the NumPy package.\\nThe Slowness of Loops\\nPython’s default implementation (known as CPython) does some operations very\\nslowly. This is in part due to the dynamic, interpreted nature of the language: the fact\\nthat types are flexible, so that sequences of operations cannot be compiled down to\\nefficient machine code as in languages like C and Fortran. Recently there have been\\nvarious attempts to address this weakness: well-known examples are the PyPy project,\\na just-in-time compiled implementation of Python; the Cython project, which con‐\\nverts Python code to compilable C code; and the Numba project, which converts\\nsnippets of Python code to fast LLVM bytecode. Each of these has its strengths and\\nweaknesses, but it is safe to say that none of the three approaches has yet surpassed\\nthe reach and popularity of the standard CPython engine.\\nThe relative sluggishness of Python generally manifests itself in situations where\\nmany small operations are being repeated—for instance, looping over arrays to oper‐\\n50 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 68}, page_content='ate on each element. For example, imagine we have an array of values and we’d like to\\ncompute the reciprocal of each. A straightforward approach might look like this:\\nIn[1]: import numpy as np\\n       np.random.seed(0)\\n       def compute_reciprocals(values):\\n           output = np.empty(len(values))\\n           for i in range(len(values)):\\n               output[i] = 1.0 / values[i]\\n           return output\\n       values = np.random.randint(1, 10, size=5)\\n       compute_reciprocals(values)\\nOut[1]: array([ 0.16666667,  1.        ,  0.25      ,  0.25      ,  0.125     ])\\nThis implementation probably feels fairly natural to someone from, say, a C or Java\\nbackground. But if we measure the execution time of this code for a large input, we\\nsee that this operation is very slow, perhaps surprisingly so! We’ll benchmark this\\nwith IPython’s %timeit magic (discussed in “Profiling and Timing Code” on page 25):\\nIn[2]: big_array = np.random.randint(1, 100, size=1000000)\\n       %timeit compute_reciprocals(big_array)\\n1 loop, best of 3: 2.91 s per loop\\nIt takes several seconds to compute these million operations and to store the result!\\nWhen even cell phones have processing speeds measured in Giga-FLOPS (i.e., bil‐\\nlions of numerical operations per second), this seems almost absurdly slow. It turns\\nout that the bottleneck here is not the operations themselves, but the type-checking\\nand function dispatches that CPython must do at each cycle of the loop. Each time\\nthe reciprocal is computed, Python first examines the object’s type and does a\\ndynamic lookup of the correct function to use for that type. If we were working in\\ncompiled code instead, this type specification would be known before the code exe‐\\ncutes and the result could be computed much more efficiently.\\nIntroducing UFuncs\\nFor many types of operations, NumPy provides a convenient interface into just this\\nkind of statically typed, compiled routine. This is known as a vectorized operation.\\nYou can accomplish this by simply performing an operation on the array, which will\\nthen be applied to each element. This vectorized approach is designed to push the\\nloop into the compiled layer that underlies NumPy, leading to much faster execution.\\nCompare the results of the following two:\\nIn[3]: print(compute_reciprocals(values))\\n       print(1.0 / values)\\nComputation on NumPy Arrays: Universal Functions \\n| \\n51'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 69}, page_content='[ 0.16666667  1.          0.25        0.25        0.125     ]\\n[ 0.16666667  1.          0.25        0.25        0.125     ]\\nLooking at the execution time for our big array, we see that it completes orders of\\nmagnitude faster than the Python loop:\\nIn[4]: %timeit (1.0 / big_array)\\n100 loops, best of 3: 4.6 ms per loop\\nVectorized operations in NumPy are implemented via ufuncs, whose main purpose is\\nto quickly execute repeated operations on values in NumPy arrays. Ufuncs are\\nextremely flexible—before we saw an operation between a scalar and an array, but we\\ncan also operate between two arrays:\\nIn[5]: np.arange(5) / np.arange(1, 6)\\nOut[5]: array([ 0.        ,  0.5       ,  0.66666667,  0.75      ,  0.8       ])\\nAnd ufunc operations are not limited to one-dimensional arrays—they can act on\\nmultidimensional arrays as well:\\nIn[6]: x = np.arange(9).reshape((3, 3))\\n       2 ** x\\nOut[6]: array([[  1,   2,   4],\\n               [  8,  16,  32],\\n               [ 64, 128, 256]])\\nComputations using vectorization through ufuncs are nearly always more efficient\\nthan their counterpart implemented through Python loops, especially as the arrays\\ngrow in size. Any time you see such a loop in a Python script, you should consider\\nwhether it can be replaced with a vectorized expression.\\nExploring NumPy’s UFuncs\\nUfuncs exist in two flavors: unary ufuncs, which operate on a single input, and binary\\nufuncs, which operate on two inputs. We’ll see examples of both these types of func‐\\ntions here.\\nArray arithmetic\\nNumPy’s ufuncs feel very natural to use because they make use of Python’s native\\narithmetic operators. The standard addition, subtraction, multiplication, and division\\ncan all be used:\\nIn[7]: x = np.arange(4)\\n       print(\"x     =\", x)\\n       print(\"x + 5 =\", x + 5)\\n       print(\"x - 5 =\", x - 5)\\n       print(\"x * 2 =\", x * 2)\\n52 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 70}, page_content='print(\"x / 2 =\", x / 2)\\n       print(\"x // 2 =\", x // 2)  # floor division\\nx     = [0 1 2 3]\\nx + 5 = [5 6 7 8]\\nx - 5 = [-5 -4 -3 -2]\\nx * 2 = [0 2 4 6]\\nx / 2 = [ 0.   0.5  1.   1.5]\\nx // 2 = [0 0 1 1]\\nThere is also a unary ufunc for negation, a ** operator for exponentiation, and a %\\noperator for modulus:\\nIn[8]: print(\"-x     = \", -x)\\n       print(\"x ** 2 = \", x ** 2)\\n       print(\"x % 2  = \", x % 2)\\n-x     =  [ 0 -1 -2 -3]\\nx ** 2 =  [0 1 4 9]\\nx % 2  =  [0 1 0 1]\\nIn addition, these can be strung together however you wish, and the standard order\\nof operations is respected:\\nIn[9]: -(0.5*x + 1) ** 2\\nOut[9]: array([-1.  , -2.25, -4.  , -6.25])\\nAll of these arithmetic operations are simply convenient wrappers around specific\\nfunctions built into NumPy; for example, the + operator is a wrapper for the add\\nfunction:\\nIn[10]: np.add(x, 2)\\nOut[10]: array([2, 3, 4, 5])\\nTable 2-2 lists the arithmetic operators implemented in NumPy.\\nTable 2-2. Arithmetic operators implemented in NumPy\\nOperator\\nEquivalent ufunc\\nDescription\\n+\\nnp.add\\nAddition (e.g., 1 + 1 = 2)\\n-\\nnp.subtract\\nSubtraction (e.g., 3 - 2 = 1)\\n-\\nnp.negative\\nUnary negation (e.g., -2)\\n*\\nnp.multiply\\nMultiplication (e.g., 2 * 3 = 6)\\n/\\nnp.divide\\nDivision (e.g., 3 / 2 = 1.5)\\n//\\nnp.floor_divide\\nFloor division (e.g., 3 // 2 = 1)\\n**\\nnp.power\\nExponentiation (e.g., 2 ** 3 = 8)\\n%\\nnp.mod\\nModulus/remainder (e.g., 9 % 4 = 1)\\nComputation on NumPy Arrays: Universal Functions \\n| \\n53'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 71}, page_content='Additionally there are Boolean/bitwise operators; we will explore these in “Compari‐\\nsons, Masks, and Boolean Logic” on page 70.\\nAbsolute value\\nJust as NumPy understands Python’s built-in arithmetic operators, it also understands\\nPython’s built-in absolute value function:\\nIn[11]: x = np.array([-2, -1, 0, 1, 2])\\n        abs(x)\\nOut[11]: array([2, 1, 0, 1, 2])\\nThe corresponding NumPy ufunc is np.absolute, which is also available under the\\nalias np.abs:\\nIn[12]: np.absolute(x)\\nOut[12]: array([2, 1, 0, 1, 2])\\nIn[13]: np.abs(x)\\nOut[13]: array([2, 1, 0, 1, 2])\\nThis ufunc can also handle complex data, in which the absolute value returns the\\nmagnitude:\\nIn[14]: x = np.array([3 - 4j, 4 - 3j, 2 + 0j, 0 + 1j])\\n        np.abs(x)\\nOut[14]: array([ 5.,  5.,  2.,  1.])\\nTrigonometric functions\\nNumPy provides a large number of useful ufuncs, and some of the most useful for the\\ndata scientist are the trigonometric functions. We’ll start by defining an array of\\nangles:\\nIn[15]: theta = np.linspace(0, np.pi, 3)\\nNow we can compute some trigonometric functions on these values:\\nIn[16]: print(\"theta      = \", theta)\\n        print(\"sin(theta) = \", np.sin(theta))\\n        print(\"cos(theta) = \", np.cos(theta))\\n        print(\"tan(theta) = \", np.tan(theta))\\ntheta      =  [ 0.          1.57079633  3.14159265]\\nsin(theta) =  [  0.00000000e+00   1.00000000e+00   1.22464680e-16]\\ncos(theta) =  [  1.00000000e+00   6.12323400e-17  -1.00000000e+00]\\ntan(theta) =  [  0.00000000e+00   1.63312394e+16  -1.22464680e-16]\\nThe values are computed to within machine precision, which is why values that\\nshould be zero do not always hit exactly zero. Inverse trigonometric functions are also\\navailable:\\n54 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 72}, page_content='In[17]: x = [-1, 0, 1]\\n        print(\"x         = \", x)\\n        print(\"arcsin(x) = \", np.arcsin(x))\\n        print(\"arccos(x) = \", np.arccos(x))\\n        print(\"arctan(x) = \", np.arctan(x))\\nx         =  [-1, 0, 1]\\narcsin(x) =  [-1.57079633  0.          1.57079633]\\narccos(x) =  [ 3.14159265  1.57079633  0.        ]\\narctan(x) =  [-0.78539816  0.          0.78539816]\\nExponents and logarithms\\nAnother common type of operation available in a NumPy ufunc are the exponentials:\\nIn[18]: x = [1, 2, 3]\\n        print(\"x     =\", x)\\n        print(\"e^x   =\", np.exp(x))\\n        print(\"2^x   =\", np.exp2(x))\\n        print(\"3^x   =\", np.power(3, x))\\nx     = [1, 2, 3]\\ne^x   = [  2.71828183   7.3890561   20.08553692]\\n2^x   = [ 2.  4.  8.]\\n3^x   = [ 3  9 27]\\nThe inverse of the exponentials, the logarithms, are also available. The basic np.log\\ngives the natural logarithm; if you prefer to compute the base-2 logarithm or the\\nbase-10 logarithm, these are available as well:\\nIn[19]: x = [1, 2, 4, 10]\\n        print(\"x        =\", x)\\n        print(\"ln(x)    =\", np.log(x))\\n        print(\"log2(x)  =\", np.log2(x))\\n        print(\"log10(x) =\", np.log10(x))\\nx        = [1, 2, 4, 10]\\nln(x)    = [ 0.          0.69314718  1.38629436  2.30258509]\\nlog2(x)  = [ 0.          1.          2.          3.32192809]\\nlog10(x) = [ 0.          0.30103     0.60205999  1.        ]\\nThere are also some specialized versions that are useful for maintaining precision\\nwith very small input:\\nIn[20]: x = [0, 0.001, 0.01, 0.1]\\n        print(\"exp(x) - 1 =\", np.expm1(x))\\n        print(\"log(1 + x) =\", np.log1p(x))\\nexp(x) - 1 = [ 0.          0.0010005   0.01005017  0.10517092]\\nlog(1 + x) = [ 0.          0.0009995   0.00995033  0.09531018]\\nWhen x is very small, these functions give more precise values than if the raw np.log\\nor np.exp were used.\\nComputation on NumPy Arrays: Universal Functions \\n| \\n55'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 73}, page_content='Specialized ufuncs\\nNumPy has many more ufuncs available, including hyperbolic trig functions, bitwise\\narithmetic, comparison operators, conversions from radians to degrees, rounding and\\nremainders, and much more. A look through the NumPy documentation reveals a lot\\nof interesting functionality.\\nAnother excellent source for more specialized and obscure ufuncs is the submodule \\nscipy.special. If you want to compute some obscure mathematical function on\\nyour data, chances are it is implemented in scipy.special. There are far too many\\nfunctions to list them all, but the following snippet shows a couple that might come\\nup in a statistics context:\\nIn[21]: from scipy import special\\nIn[22]: # Gamma functions (generalized factorials) and related functions\\n        x = [1, 5, 10]\\n        print(\"gamma(x)     =\", special.gamma(x))\\n        print(\"ln|gamma(x)| =\", special.gammaln(x))\\n        print(\"beta(x, 2)   =\", special.beta(x, 2))\\ngamma(x)     = [  1.00000000e+00   2.40000000e+01   3.62880000e+05]\\nln|gamma(x)| = [  0.           3.17805383  12.80182748]\\nbeta(x, 2)   = [ 0.5         0.03333333  0.00909091]\\nIn[23]: # Error function (integral of Gaussian)\\n        # its complement, and its inverse\\n        x = np.array([0, 0.3, 0.7, 1.0])\\n        print(\"erf(x)  =\", special.erf(x))\\n        print(\"erfc(x) =\", special.erfc(x))\\n        print(\"erfinv(x) =\", special.erfinv(x))\\nerf(x)  = [ 0.          0.32862676  0.67780119  0.84270079]\\nerfc(x) = [ 1.          0.67137324  0.32219881  0.15729921]\\nerfinv(x) = [ 0.          0.27246271  0.73286908         inf]\\nThere are many, many more ufuncs available in both NumPy and scipy.special.\\nBecause the documentation of these packages is available online, a web search along\\nthe lines of “gamma function python” will generally find the relevant information.\\nAdvanced Ufunc Features\\nMany NumPy users make use of ufuncs without ever learning their full set of features.\\nWe’ll outline a few specialized features of ufuncs here.\\nSpecifying output\\nFor large calculations, it is sometimes useful to be able to specify the array where the\\nresult of the calculation will be stored. Rather than creating a temporary array, you\\ncan use this to write computation results directly to the memory location where you’d\\n56 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 74}, page_content='like them to be. For all ufuncs, you can do this using the out argument of the\\nfunction:\\nIn[24]: x = np.arange(5)\\n        y = np.empty(5)\\n        np.multiply(x, 10, out=y)\\n        print(y)\\n[  0.  10.  20.  30.  40.]\\nThis can even be used with array views. For example, we can write the results of a\\ncomputation to every other element of a specified array:\\nIn[25]: y = np.zeros(10)\\n        np.power(2, x, out=y[::2])\\n        print(y)\\n[  1.   0.   2.   0.   4.   0.   8.   0.  16.   0.]\\nIf we had instead written y[::2] = 2 ** x, this would have resulted in the creation\\nof a temporary array to hold the results of 2 ** x, followed by a second operation\\ncopying those values into the y array. This doesn’t make much of a difference for such\\na small computation, but for very large arrays the memory savings from careful use of\\nthe out argument can be significant.\\nAggregates\\nFor binary ufuncs, there are some interesting aggregates that can be computed\\ndirectly from the object. For example, if we’d like to reduce an array with a particular\\noperation, we can use the reduce method of any ufunc. A reduce repeatedly applies a\\ngiven operation to the elements of an array until only a single result remains.\\nFor example, calling reduce on the add ufunc returns the sum of all elements in the\\narray:\\nIn[26]: x = np.arange(1, 6)\\n        np.add.reduce(x)\\nOut[26]: 15\\nSimilarly, calling reduce on the multiply ufunc results in the product of all array\\nelements:\\nIn[27]: np.multiply.reduce(x)\\nOut[27]: 120\\nIf we’d like to store all the intermediate results of the computation, we can instead use\\naccumulate:\\nIn[28]: np.add.accumulate(x)\\nOut[28]: array([ 1,  3,  6, 10, 15])\\nComputation on NumPy Arrays: Universal Functions \\n| \\n57'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 75}, page_content='In[29]: np.multiply.accumulate(x)\\nOut[29]: array([  1,   2,   6,  24, 120])\\nNote that for these particular cases, there are dedicated NumPy functions to compute\\nthe results (np.sum, np.prod, np.cumsum, np.cumprod), which we’ll explore in “Aggre‐\\ngations: Min, Max, and Everything in Between” on page 58.\\nOuter products\\nFinally, any ufunc can compute the output of all pairs of two different inputs using\\nthe outer method. This allows you, in one line, to do things like create a multiplica‐\\ntion table:\\nIn[30]: x = np.arange(1, 6)\\n        np.multiply.outer(x, x)\\nOut[30]: array([[ 1,  2,  3,  4,  5],\\n                [ 2,  4,  6,  8, 10],\\n                [ 3,  6,  9, 12, 15],\\n                [ 4,  8, 12, 16, 20],\\n                [ 5, 10, 15, 20, 25]])\\nThe ufunc.at and ufunc.reduceat methods, which we’ll explore in “Fancy Index‐\\ning” on page 78, are very helpful as well.\\nAnother extremely useful feature of ufuncs is the ability to operate between arrays of\\ndifferent sizes and shapes, a set of operations known as broadcasting. This subject is\\nimportant enough that we will devote a whole section to it (see “Computation on\\nArrays: Broadcasting” on page 63).\\nUfuncs: Learning More\\nMore information on universal functions (including the full list of available func‐\\ntions) can be found on the NumPy and SciPy documentation websites.\\nRecall that you can also access information directly from within IPython by import‐\\ning the packages and using IPython’s tab-completion and help (?) functionality, as\\ndescribed in “Help and Documentation in IPython” on page 3.\\nAggregations: Min, Max, and Everything in Between\\nOften when you are faced with a large amount of data, a first step is to compute sum‐\\nmary statistics for the data in question. Perhaps the most common summary statistics\\nare the mean and standard deviation, which allow you to summarize the “typical” val‐\\nues in a dataset, but other aggregates are useful as well (the sum, product, median,\\nminimum and maximum, quantiles, etc.).\\n58 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 76}, page_content='NumPy has fast built-in aggregation functions for working on arrays; we’ll discuss\\nand demonstrate some of them here.\\nSumming the Values in an Array\\nAs a quick example, consider computing the sum of all values in an array. Python\\nitself can do this using the built-in sum function:\\nIn[1]: import numpy as np\\nIn[2]: L = np.random.random(100)\\n       sum(L)\\nOut[2]: 55.61209116604941\\nThe syntax is quite similar to that of NumPy’s sum function, and the result is the same\\nin the simplest case:\\nIn[3]: np.sum(L)\\nOut[3]: 55.612091166049424\\nHowever, because it executes the operation in compiled code, NumPy’s version of the\\noperation is computed much more quickly:\\nIn[4]: big_array = np.random.rand(1000000)\\n       %timeit sum(big_array)\\n       %timeit np.sum(big_array)\\n10 loops, best of 3: 104 ms per loop\\n1000 loops, best of 3: 442 µs per loop\\nBe careful, though: the sum function and the np.sum function are not identical, which\\ncan sometimes lead to confusion! In particular, their optional arguments have differ‐\\nent meanings, and np.sum is aware of multiple array dimensions, as we will see in the\\nfollowing section.\\nMinimum and Maximum\\nSimilarly, Python has built-in min and max functions, used to find the minimum value\\nand maximum value of any given array:\\nIn[5]: min(big_array), max(big_array)\\nOut[5]: (1.1717128136634614e-06, 0.9999976784968716)\\nNumPy’s corresponding functions have similar syntax, and again operate much more\\nquickly:\\nIn[6]: np.min(big_array), np.max(big_array)\\nOut[6]: (1.1717128136634614e-06, 0.9999976784968716)\\nAggregations: Min, Max, and Everything in Between \\n| \\n59'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 77}, page_content='In[7]: %timeit min(big_array)\\n       %timeit np.min(big_array)\\n10 loops, best of 3: 82.3 ms per loop\\n1000 loops, best of 3: 497 µs per loop\\nFor min, max, sum, and several other NumPy aggregates, a shorter syntax is to use\\nmethods of the array object itself:\\nIn[8]: print(big_array.min(), big_array.max(), big_array.sum())\\n1.17171281366e-06 0.999997678497 499911.628197\\nWhenever possible, make sure that you are using the NumPy version of these aggre‐\\ngates when operating on NumPy arrays!\\nMultidimensional aggregates\\nOne common type of aggregation operation is an aggregate along a row or column.\\nSay you have some data stored in a two-dimensional array:\\nIn[9]: M = np.random.random((3, 4))\\n       print(M)\\n[[ 0.8967576   0.03783739  0.75952519  0.06682827]\\n [ 0.8354065   0.99196818  0.19544769  0.43447084]\\n [ 0.66859307  0.15038721  0.37911423  0.6687194 ]]\\nBy default, each NumPy aggregation function will return the aggregate over the entire\\narray:\\nIn[10]: M.sum()\\nOut[10]: 6.0850555667307118\\nAggregation functions take an additional argument specifying the axis along which\\nthe aggregate is computed. For example, we can find the minimum value within each\\ncolumn by specifying axis=0:\\nIn[11]: M.min(axis=0)\\nOut[11]: array([ 0.66859307,  0.03783739,  0.19544769,  0.06682827])\\nThe function returns four values, corresponding to the four columns of numbers.\\nSimilarly, we can find the maximum value within each row:\\nIn[12]: M.max(axis=1)\\nOut[12]: array([ 0.8967576 ,  0.99196818,  0.6687194 ])\\nThe way the axis is specified here can be confusing to users coming from other lan‐\\nguages. The axis keyword specifies the dimension of the array that will be collapsed,\\nrather than the dimension that will be returned. So specifying axis=0 means that the\\n60 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 78}, page_content='first axis will be collapsed: for two-dimensional arrays, this means that values within\\neach column will be aggregated.\\nOther aggregation functions\\nNumPy provides many other aggregation functions, but we won’t discuss them in\\ndetail here. Additionally, most aggregates have a NaN-safe counterpart that computes\\nthe result while ignoring missing values, which are marked by the special IEEE\\nfloating-point NaN value (for a fuller discussion of missing data, see “Handling Miss‐\\ning Data” on page 119). Some of these NaN-safe functions were not added until\\nNumPy 1.8, so they will not be available in older NumPy versions.\\nTable 2-3 provides a list of useful aggregation functions available in NumPy.\\nTable 2-3. Aggregation functions available in NumPy\\nFunction Name\\nNaN-safe Version\\nDescription\\nnp.sum\\nnp.nansum\\nCompute sum of elements\\nnp.prod\\nnp.nanprod\\nCompute product of elements\\nnp.mean\\nnp.nanmean\\nCompute median of elements\\nnp.std\\nnp.nanstd\\nCompute standard deviation\\nnp.var\\nnp.nanvar\\nCompute variance\\nnp.min\\nnp.nanmin\\nFind minimum value\\nnp.max\\nnp.nanmax\\nFind maximum value\\nnp.argmin\\nnp.nanargmin\\nFind index of minimum value\\nnp.argmax\\nnp.nanargmax\\nFind index of maximum value\\nnp.median\\nnp.nanmedian\\nCompute median of elements\\nnp.percentile\\nnp.nanpercentile Compute rank-based statistics of elements\\nnp.any\\nN/A\\nEvaluate whether any elements are true\\nnp.all\\nN/A\\nEvaluate whether all elements are true\\nWe will see these aggregates often throughout the rest of the book.\\nExample: What Is the Average Height of US Presidents?\\nAggregates available in NumPy can be extremely useful for summarizing a set of val‐\\nues. As a simple example, let’s consider the heights of all US presidents. This data is\\navailable in the file president_heights.csv, which is a simple comma-separated list of\\nlabels and values:\\nIn[13]: !head -4 data/president_heights.csv\\norder,name,height(cm)\\n1,George Washington,189\\nAggregations: Min, Max, and Everything in Between \\n| \\n61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 79}, page_content='2,John Adams,170\\n3,Thomas Jefferson,189\\nWe’ll use the Pandas package, which we’ll explore more fully in Chapter 3, to read the\\nfile and extract this information (note that the heights are measured in centimeters):\\nIn[14]: import pandas as pd\\n        data = pd.read_csv(\\'data/president_heights.csv\\')\\n        heights = np.array(data[\\'height(cm)\\'])\\n        print(heights)\\n[189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173\\n 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183\\n 177 185 188 188 182 185]\\nNow that we have this data array, we can compute a variety of summary statistics:\\nIn[15]: print(\"Mean height:       \", heights.mean())\\n        print(\"Standard deviation:\", heights.std())\\n        print(\"Minimum height:    \", heights.min())\\n        print(\"Maximum height:    \", heights.max())\\nMean height:        179.738095238\\nStandard deviation: 6.93184344275\\nMinimum height:     163\\nMaximum height:     193\\nNote that in each case, the aggregation operation reduced the entire array to a single\\nsummarizing value, which gives us information about the distribution of values. We\\nmay also wish to compute quantiles:\\nIn[16]: print(\"25th percentile:   \", np.percentile(heights, 25))\\n        print(\"Median:            \", np.median(heights))\\n        print(\"75th percentile:   \", np.percentile(heights, 75))\\n25th percentile:    174.25\\nMedian:             182.0\\n75th percentile:    183.0\\nWe see that the median height of US presidents is 182 cm, or just shy of six feet.\\nOf course, sometimes it’s more useful to see a visual representation of this data, which\\nwe can accomplish using tools in Matplotlib (we’ll discuss Matplotlib more fully in\\nChapter 4). For example, this code generates the chart shown in Figure 2-3:\\nIn[17]: %matplotlib inline\\n        import matplotlib.pyplot as plt\\n        import seaborn; seaborn.set()  # set plot style\\nIn[18]: plt.hist(heights)\\n        plt.title(\\'Height Distribution of US Presidents\\')\\n        plt.xlabel(\\'height (cm)\\')\\n        plt.ylabel(\\'number\\');\\n62 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 80}, page_content='Figure 2-3. Histogram of presidential heights\\nThese aggregates are some of the fundamental pieces of exploratory data analysis that\\nwe’ll explore in more depth in later chapters of the book.\\nComputation on Arrays: Broadcasting\\nWe saw in the previous section how NumPy’s universal functions can be used to vec‐\\ntorize operations and thereby remove slow Python loops. Another means of vectoriz‐\\ning operations is to use NumPy’s broadcasting functionality. Broadcasting is simply a\\nset of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on\\narrays of different sizes.\\nIntroducing Broadcasting\\nRecall that for arrays of the same size, binary operations are performed on an\\nelement-by-element basis:\\nIn[1]: import numpy as np\\nIn[2]: a = np.array([0, 1, 2])\\n       b = np.array([5, 5, 5])\\n       a + b\\nOut[2]: array([5, 6, 7])\\nBroadcasting allows these types of binary operations to be performed on arrays of dif‐\\nferent sizes—for example, we can just as easily add a scalar (think of it as a zero-\\ndimensional array) to an array:\\nComputation on Arrays: Broadcasting \\n| \\n63'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 81}, page_content='In[3]: a + 5\\nOut[3]: array([5, 6, 7])\\nWe can think of this as an operation that stretches or duplicates the value 5 into the\\narray [5, 5, 5], and adds the results. The advantage of NumPy’s broadcasting is that\\nthis duplication of values does not actually take place, but it is a useful mental model\\nas we think about broadcasting.\\nWe can similarly extend this to arrays of higher dimension. Observe the result when\\nwe add a one-dimensional array to a two-dimensional array:\\nIn[4]: M = np.ones((3, 3))\\n       M\\nOut[4]: array([[ 1.,  1.,  1.],\\n               [ 1.,  1.,  1.],\\n               [ 1.,  1.,  1.]])\\nIn[5]: M + a\\nOut[5]: array([[ 1.,  2.,  3.],\\n               [ 1.,  2.,  3.],\\n               [ 1.,  2.,  3.]])\\nHere the one-dimensional array a is stretched, or broadcast, across the second\\ndimension in order to match the shape of M.\\nWhile these examples are relatively easy to understand, more complicated cases can\\ninvolve broadcasting of both arrays. Consider the following example:\\nIn[6]: a = np.arange(3)\\n       b = np.arange(3)[:, np.newaxis]\\n       print(a)\\n       print(b)\\n[0 1 2]\\n[[0]\\n [1]\\n [2]]\\nIn[7]: a + b\\nOut[7]: array([[0, 1, 2],\\n               [1, 2, 3],\\n               [2, 3, 4]])\\n64 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 82}, page_content='1 Code to produce this plot can be found in the online appendix, and is adapted from source published in the\\nastroML documentation. Used with permission.\\nJust as before we stretched or broadcasted one value to match the shape of the other,\\nhere we’ve stretched both a and b to match a common shape, and the result is a two-\\ndimensional array! The geometry of these examples is visualized in Figure 2-4.1\\nFigure 2-4. Visualization of NumPy broadcasting\\nThe light boxes represent the broadcasted values: again, this extra memory is not\\nactually allocated in the course of the operation, but it can be useful conceptually to\\nimagine that it is.\\nRules of Broadcasting\\nBroadcasting in NumPy follows a strict set of rules to determine the interaction\\nbetween the two arrays:\\n• Rule 1: If the two arrays differ in their number of dimensions, the shape of the\\none with fewer dimensions is padded with ones on its leading (left) side.\\n• Rule 2: If the shape of the two arrays does not match in any dimension, the array\\nwith shape equal to 1 in that dimension is stretched to match the other shape.\\n• Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is\\nraised.\\nComputation on Arrays: Broadcasting \\n| \\n65'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 83}, page_content='To make these rules clear, let’s consider a few examples in detail.\\nBroadcasting example 1\\nLet’s look at adding a two-dimensional array to a one-dimensional array:\\nIn[8]: M = np.ones((2, 3))\\n       a = np.arange(3)\\nLet’s consider an operation on these two arrays. The shapes of the arrays are:\\nM.shape = (2, 3)\\na.shape = (3,)\\nWe see by rule 1 that the array a has fewer dimensions, so we pad it on the left with\\nones:\\nM.shape -> (2, 3)\\na.shape -> (1, 3)\\nBy rule 2, we now see that the first dimension disagrees, so we stretch this dimension\\nto match:\\nM.shape -> (2, 3)\\na.shape -> (2, 3)\\nThe shapes match, and we see that the final shape will be (2, 3):\\nIn[9]: M + a\\nOut[9]: array([[ 1.,  2.,  3.],\\n               [ 1.,  2.,  3.]])\\nBroadcasting example 2\\nLet’s take a look at an example where both arrays need to be broadcast:\\nIn[10]: a = np.arange(3).reshape((3, 1))\\n        b = np.arange(3)\\nAgain, we’ll start by writing out the shape of the arrays:\\na.shape = (3, 1)\\nb.shape = (3,)\\n66 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 84}, page_content='Rule 1 says we must pad the shape of b with ones:\\na.shape -> (3, 1)\\nb.shape -> (1, 3)\\nAnd rule 2 tells us that we upgrade each of these ones to match the corresponding\\nsize of the other array:\\na.shape -> (3, 3)\\nb.shape -> (3, 3)\\nBecause the result matches, these shapes are compatible. We can see this here:\\nIn[11]: a + b\\nOut[11]: array([[0, 1, 2],\\n                [1, 2, 3],\\n                [2, 3, 4]])\\nBroadcasting example 3\\nNow let’s take a look at an example in which the two arrays are not compatible:\\nIn[12]: M = np.ones((3, 2))\\n        a = np.arange(3)\\nThis is just a slightly different situation than in the first example: the matrix M is\\ntransposed. How does this affect the calculation? The shapes of the arrays are:\\nM.shape = (3, 2)\\na.shape = (3,)\\nAgain, rule 1 tells us that we must pad the shape of a with ones:\\nM.shape -> (3, 2)\\na.shape -> (1, 3)\\nBy rule 2, the first dimension of a is stretched to match that of M:\\nM.shape -> (3, 2)\\na.shape -> (3, 3)\\nNow we hit rule 3—the final shapes do not match, so these two arrays are incompati‐\\nble, as we can observe by attempting this operation:\\nComputation on Arrays: Broadcasting \\n| \\n67'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 85}, page_content='In[13]: M + a\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-13-9e16e9f98da6> in <module>()\\n----> 1 M + a\\nValueError: operands could not be broadcast together with shapes (3,2) (3,)\\nNote the potential confusion here: you could imagine making a and M compatible by,\\nsay, padding a’s shape with ones on the right rather than the left. But this is not how\\nthe broadcasting rules work! That sort of flexibility might be useful in some cases, but\\nit would lead to potential areas of ambiguity. If right-side padding is what you’d like,\\nyou can do this explicitly by reshaping the array (we’ll use the np.newaxis keyword\\nintroduced in “The Basics of NumPy Arrays” on page 42):\\nIn[14]: a[:, np.newaxis].shape\\nOut[14]: (3, 1)\\nIn[15]: M + a[:, np.newaxis]\\nOut[15]: array([[ 1.,  1.],\\n                [ 2.,  2.],\\n                [ 3.,  3.]])\\nAlso note that while we’ve been focusing on the + operator here, these broadcasting\\nrules apply to any binary ufunc. For example, here is the logaddexp(a, b) function,\\nwhich computes log(exp(a) + exp(b)) with more precision than the naive\\napproach:\\nIn[16]: np.logaddexp(M, a[:, np.newaxis])\\nOut[16]: array([[ 1.31326169,  1.31326169],\\n                [ 1.69314718,  1.69314718],\\n                [ 2.31326169,  2.31326169]])\\nFor more information on the many available universal functions, refer to “Computa‐\\ntion on NumPy Arrays: Universal Functions” on page 50.\\nBroadcasting in Practice\\nBroadcasting operations form the core of many examples we’ll see throughout this\\nbook. We’ll now take a look at a couple simple examples of where they can be useful.\\nCentering an array\\nIn the previous section, we saw that ufuncs allow a NumPy user to remove the need\\nto explicitly write slow Python loops. Broadcasting extends this ability. One com‐\\n68 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 86}, page_content=\"monly seen example is centering an array of data. Imagine you have an array of 10\\nobservations, each of which consists of 3 values. Using the standard convention (see\\n“Data Representation in Scikit-Learn” on page 343), we’ll store this in a 10×3 array:\\nIn[17]: X = np.random.random((10, 3))\\nWe can compute the mean of each feature using the mean aggregate across the first\\ndimension:\\nIn[18]: Xmean = X.mean(0)\\n        Xmean\\nOut[18]: array([ 0.53514715,  0.66567217,  0.44385899])\\nAnd now we can center the X array by subtracting the mean (this is a broadcasting\\noperation):\\nIn[19]: X_centered = X - Xmean\\nTo double-check that we’ve done this correctly, we can check that the centered array\\nhas near zero mean:\\nIn[20]: X_centered.mean(0)\\nOut[20]: array([  2.22044605e-17,  -7.77156117e-17,  -1.66533454e-17])\\nTo within-machine precision, the mean is now zero.\\nPlotting a two-dimensional function\\nOne place that broadcasting is very useful is in displaying images based on two-\\ndimensional functions. If we want to define a function z = f(x, y), broadcasting can be\\nused to compute the function across the grid:\\nIn[21]: # x and y have 50 steps from 0 to 5\\n        x = np.linspace(0, 5, 50)\\n        y = np.linspace(0, 5, 50)[:, np.newaxis]\\n        z = np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\\nWe’ll use Matplotlib to plot this two-dimensional array (these tools will be discussed\\nin full in “Density and Contour Plots” on page 241):\\nIn[22]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\nIn[23]: plt.imshow(z, origin='lower', extent=[0, 5, 0, 5],\\n                   cmap='viridis')\\n        plt.colorbar();\\nThe result, shown in Figure 2-5, is a compelling visualization of the two-dimensional\\nfunction.\\nComputation on Arrays: Broadcasting \\n| \\n69\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 87}, page_content=\"Figure 2-5. Visualization of a 2D array\\nComparisons, Masks, and Boolean Logic\\nThis section covers the use of Boolean masks to examine and manipulate values\\nwithin NumPy arrays. Masking comes up when you want to extract, modify, count, or\\notherwise manipulate values in an array based on some criterion: for example, you\\nmight wish to count all values greater than a certain value, or perhaps remove all out‐\\nliers that are above some threshold. In NumPy, Boolean masking is often the most\\nefficient way to accomplish these types of tasks.\\nExample: Counting Rainy Days\\nImagine you have a series of data that represents the amount of precipitation each day\\nfor a year in a given city. For example, here we’ll load the daily rainfall statistics for\\nthe city of Seattle in 2014, using Pandas (which is covered in more detail in Chap‐\\nter 3):\\nIn[1]: import numpy as np\\n       import pandas as pd\\n       # use Pandas to extract rainfall inches as a NumPy array\\n       rainfall = pd.read_csv('data/Seattle2014.csv')['PRCP'].values\\n       inches = rainfall / 254  # 1/10mm -> inches\\n       inches.shape\\nOut[1]: (365,)\\nThe array contains 365 values, giving daily rainfall in inches from January 1 to\\nDecember 31, 2014.\\nAs a first quick visualization, let’s look at the histogram of rainy days shown in\\nFigure 2-6, which was generated using Matplotlib (we will explore this tool more fully\\nin Chapter 4):\\n70 \\n| \\nChapter 2: Introduction to NumPy\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 88}, page_content='In[2]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn; seaborn.set()  # set plot styles\\nIn[3]: plt.hist(inches, 40);\\nFigure 2-6. Histogram of 2014 rainfall in Seattle\\nThis histogram gives us a general idea of what the data looks like: despite its reputa‐\\ntion, the vast majority of days in Seattle saw near zero measured rainfall in 2014. But\\nthis doesn’t do a good job of conveying some information we’d like to see: for exam‐\\nple, how many rainy days were there in the year? What is the average precipitation on\\nthose rainy days? How many days were there with more than half an inch of rain?\\nDigging into the data\\nOne approach to this would be to answer these questions by hand: loop through the\\ndata, incrementing a counter each time we see values in some desired range. For rea‐\\nsons discussed throughout this chapter, such an approach is very inefficient, both\\nfrom the standpoint of time writing code and time computing the result. We saw in\\n“Computation on NumPy Arrays: Universal Functions” on page 50 that NumPy’s\\nufuncs can be used in place of loops to do fast element-wise arithmetic operations on\\narrays; in the same way, we can use other ufuncs to do element-wise comparisons over\\narrays, and we can then manipulate the results to answer the questions we have. We’ll\\nleave the data aside for right now, and discuss some general tools in NumPy to use\\nmasking to quickly answer these types of questions.\\nComparison Operators as ufuncs\\nIn “Computation on NumPy Arrays: Universal Functions” on page 50 we introduced\\nufuncs, and focused in particular on arithmetic operators. We saw that using +, -, *, /,\\nComparisons, Masks, and Boolean Logic \\n| \\n71'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 89}, page_content='and others on arrays leads to element-wise operations. NumPy also implements com‐\\nparison operators such as < (less than) and > (greater than) as element-wise ufuncs.\\nThe result of these comparison operators is always an array with a Boolean data type.\\nAll six of the standard comparison operations are available:\\nIn[4]: x = np.array([1, 2, 3, 4, 5])\\nIn[5]: x < 3  # less than\\nOut[5]: array([ True,  True, False, False, False], dtype=bool)\\nIn[6]: x > 3  # greater than\\nOut[6]: array([False, False, False,  True,  True], dtype=bool)\\nIn[7]: x <= 3  # less than or equal\\nOut[7]: array([ True,  True,  True, False, False], dtype=bool)\\nIn[8]: x >= 3  # greater than or equal\\nOut[8]: array([False, False,  True,  True,  True], dtype=bool)\\nIn[9]: x != 3  # not equal\\nOut[9]: array([ True,  True, False,  True,  True], dtype=bool)\\nIn[10]: x == 3  # equal\\nOut[10]: array([False, False,  True, False, False], dtype=bool)\\nIt is also possible to do an element-by-element comparison of two arrays, and to\\ninclude compound expressions:\\nIn[11]: (2 * x) == (x ** 2)\\nOut[11]: array([False,  True, False, False, False], dtype=bool)\\nAs in the case of arithmetic operators, the comparison operators are implemented as\\nufuncs in NumPy; for example, when you write x < 3, internally NumPy uses\\nnp.less(x, 3). A summary of the comparison operators and their equivalent ufunc\\nis shown here:\\nOperator\\nEquivalent ufunc\\n==\\nnp.equal\\n!=\\nnp.not_equal\\n<\\nnp.less\\n<=\\nnp.less_equal\\n>\\nnp.greater\\n>=\\nnp.greater_equal\\nJust as in the case of arithmetic ufuncs, these will work on arrays of any size and\\nshape. Here is a two-dimensional example:\\n72 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 90}, page_content='In[12]: rng = np.random.RandomState(0)\\n        x = rng.randint(10, size=(3, 4))\\n        x\\nOut[12]: array([[5, 0, 3, 3],\\n                [7, 9, 3, 5],\\n                [2, 4, 7, 6]])\\nIn[13]: x < 6\\nOut[13]: array([[ True,  True,  True,  True],\\n                [False, False,  True,  True],\\n                [ True,  True, False, False]], dtype=bool)\\nIn each case, the result is a Boolean array, and NumPy provides a number of straight‐\\nforward patterns for working with these Boolean results.\\nWorking with Boolean Arrays\\nGiven a Boolean array, there are a host of useful operations you can do. We’ll work\\nwith x, the two-dimensional array we created earlier:\\nIn[14]: print(x)\\n[[5 0 3 3]\\n [7 9 3 5]\\n [2 4 7 6]]\\nCounting entries\\nTo count the number of True entries in a Boolean array, np.count_nonzero is useful:\\nIn[15]: # how many values less than 6?\\n        np.count_nonzero(x < 6)\\nOut[15]: 8\\nWe see that there are eight array entries that are less than 6. Another way to get at this\\ninformation is to use np.sum; in this case, False is interpreted as 0, and True is inter‐\\npreted as 1:\\nIn[16]: np.sum(x < 6)\\nOut[16]: 8\\nThe benefit of sum() is that like with other NumPy aggregation functions, this sum‐\\nmation can be done along rows or columns as well:\\nIn[17]: # how many values less than 6 in each row?\\n        np.sum(x < 6, axis=1)\\nOut[17]: array([4, 2, 2])\\nThis counts the number of values less than 6 in each row of the matrix.\\nComparisons, Masks, and Boolean Logic \\n| \\n73'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 91}, page_content='If we’re interested in quickly checking whether any or all the values are true, we can\\nuse (you guessed it) np.any() or np.all():\\nIn[18]: # are there any values greater than 8?\\n        np.any(x > 8)\\nOut[18]: True\\nIn[19]: # are there any values less than zero?\\n        np.any(x < 0)\\nOut[19]: False\\nIn[20]: # are all values less than 10?\\n        np.all(x < 10)\\nOut[20]: True\\nIn[21]: # are all values equal to 6?\\n        np.all(x == 6)\\nOut[21]: False\\nnp.all() and np.any() can be used along particular axes as well. For example:\\nIn[22]: # are all values in each row less than 8?\\n        np.all(x < 8, axis=1)\\nOut[22]: array([ True, False,  True], dtype=bool)\\nHere all the elements in the first and third rows are less than 8, while this is not the\\ncase for the second row.\\nFinally, a quick warning: as mentioned in “Aggregations: Min, Max, and Everything\\nin Between” on page 58, Python has built-in sum(), any(), and all() functions.\\nThese have a different syntax than the NumPy versions, and in particular will fail or\\nproduce unintended results when used on multidimensional arrays. Be sure that you\\nare using np.sum(), np.any(), and np.all() for these examples!\\nBoolean operators\\nWe’ve already seen how we might count, say, all days with rain less than four inches,\\nor all days with rain greater than two inches. But what if we want to know about all\\ndays with rain less than four inches and greater than one inch? This is accomplished\\nthrough Python’s bitwise logic operators, &, |, ^, and ~. Like with the standard arith‐\\nmetic operators, NumPy overloads these as ufuncs that work element-wise on (usu‐\\nally Boolean) arrays.\\nFor example, we can address this sort of compound question as follows:\\nIn[23]: np.sum((inches > 0.5) & (inches < 1))\\nOut[23]: 29\\nSo we see that there are 29 days with rainfall between 0.5 and 1.0 inches.\\n74 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 92}, page_content='Note that the parentheses here are important—because of operator precedence rules,\\nwith parentheses removed this expression would be evaluated as follows, which\\nresults in an error:\\ninches > (0.5 & inches) < 1\\nUsing the equivalence of A AND B and NOT (A OR B) (which you may remember if\\nyou’ve taken an introductory logic course), we can compute the same result in a dif‐\\nferent manner:\\nIn[24]: np.sum(~( (inches <= 0.5) | (inches >= 1) ))\\nOut[24]: 29\\nCombining comparison operators and Boolean operators on arrays can lead to a wide\\nrange of efficient logical operations.\\nThe following table summarizes the bitwise Boolean operators and their equivalent\\nufuncs:\\nOperator\\nEquivalent ufunc\\n&\\nnp.bitwise_and\\n|\\nnp.bitwise_or\\n^\\nnp.bitwise_xor\\n~\\nnp.bitwise_not\\nUsing these tools, we might start to answer the types of questions we have about our\\nweather data. Here are some examples of results we can compute when combining\\nmasking with aggregations:\\nIn[25]: print(\"Number days without rain:      \", np.sum(inches == 0))\\n        print(\"Number days with rain:         \", np.sum(inches != 0))\\n        print(\"Days with more than 0.5 inches:\", np.sum(inches > 0.5))\\n        print(\"Rainy days with < 0.1 inches  :\", np.sum((inches > 0) &\\n                                                        (inches < 0.2)))\\nNumber days without rain:       215\\nNumber days with rain:          150\\nDays with more than 0.5 inches: 37\\nRainy days with < 0.1 inches  : 75\\nBoolean Arrays as Masks\\nIn the preceding section, we looked at aggregates computed directly on Boolean\\narrays. A more powerful pattern is to use Boolean arrays as masks, to select particular\\nsubsets of the data themselves. Returning to our x array from before, suppose we\\nwant an array of all values in the array that are less than, say, 5:\\nComparisons, Masks, and Boolean Logic \\n| \\n75'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 93}, page_content='In[26]: x\\nOut[26]: array([[5, 0, 3, 3],\\n                [7, 9, 3, 5],\\n                [2, 4, 7, 6]])\\nWe can obtain a Boolean array for this condition easily, as we’ve already seen:\\nIn[27]: x < 5\\nOut[27]: array([[False,  True,  True,  True],\\n                [False, False,  True, False],\\n                [ True,  True, False, False]], dtype=bool)\\nNow to select these values from the array, we can simply index on this Boolean array;\\nthis is known as a masking operation:\\nIn[28]: x[x < 5]\\nOut[28]: array([0, 3, 3, 3, 2, 4])\\nWhat is returned is a one-dimensional array filled with all the values that meet this\\ncondition; in other words, all the values in positions at which the mask array is True.\\nWe are then free to operate on these values as we wish. For example, we can compute\\nsome relevant statistics on our Seattle rain data:\\nIn[29]:\\n# construct a mask of all rainy days\\nrainy = (inches > 0)\\n# construct a mask of all summer days (June 21st is the 172nd day)\\nsummer = (np.arange(365) - 172 < 90) & (np.arange(365) - 172 > 0)\\nprint(\"Median precip on rainy days in 2014 (inches):   \",\\n      np.median(inches[rainy]))\\nprint(\"Median precip on summer days in 2014 (inches):  \",\\n      np.median(inches[summer]))\\nprint(\"Maximum precip on summer days in 2014 (inches): \",\\n      np.max(inches[summer]))\\nprint(\"Median precip on non-summer rainy days (inches):\",\\n      np.median(inches[rainy & ~summer]))\\nMedian precip on rainy days in 2014 (inches):    0.194881889764\\nMedian precip on summer days in 2014 (inches):   0.0\\nMaximum precip on summer days in 2014 (inches):  0.850393700787\\nMedian precip on non-summer rainy days (inches): 0.200787401575\\nBy combining Boolean operations, masking operations, and aggregates, we can very\\nquickly answer these sorts of questions for our dataset.\\n76 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 94}, page_content=\"Using the Keywords and/or Versus the Operators &/|\\nOne common point of confusion is the difference between the keywords and and or\\non one hand, and the operators & and | on the other hand. When would you use one\\nversus the other?\\nThe difference is this: and and or gauge the truth or falsehood of entire object, while &\\nand | refer to bits within each object.\\nWhen you use and or or, it’s equivalent to asking Python to treat the object as a single\\nBoolean entity. In Python, all nonzero integers will evaluate as True. Thus:\\nIn[30]: bool(42), bool(0)\\nOut[30]: (True, False)\\nIn[31]: bool(42 and 0)\\nOut[31]: False\\nIn[32]: bool(42 or 0)\\nOut[32]: True\\nWhen you use & and | on integers, the expression operates on the bits of the element,\\napplying the and or the or to the individual bits making up the number:\\nIn[33]: bin(42)\\nOut[33]: '0b101010'\\nIn[34]: bin(59)\\nOut[34]: '0b111011'\\nIn[35]: bin(42 & 59)\\nOut[35]: '0b101010'\\nIn[36]: bin(42 | 59)\\nOut[36]: '0b111011'\\nNotice that the corresponding bits of the binary representation are compared in order\\nto yield the result.\\nWhen you have an array of Boolean values in NumPy, this can be thought of as a\\nstring of bits where 1 = True and 0 = False, and the result of & and | operates in a\\nsimilar manner as before:\\nIn[37]: A = np.array([1, 0, 1, 0, 1, 0], dtype=bool)\\n        B = np.array([1, 1, 1, 0, 1, 1], dtype=bool)\\n        A | B\\nOut[37]: array([ True,  True,  True, False,  True,  True], dtype=bool)\\nComparisons, Masks, and Boolean Logic \\n| \\n77\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 95}, page_content='Using or on these arrays will try to evaluate the truth or falsehood of the entire array\\nobject, which is not a well-defined value:\\nIn[38]: A or B\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-38-5d8e4f2e21c0> in <module>()\\n----> 1 A or B\\nValueError: The truth value of an array with more than one element is...\\nSimilarly, when doing a Boolean expression on a given array, you should use | or &\\nrather than or or and:\\nIn[39]: x = np.arange(10)\\n        (x > 4) & (x < 8)\\nOut[39]: array([False, False, ...,  True,  True, False, False], dtype=bool)\\nTrying to evaluate the truth or falsehood of the entire array will give the same\\nValueError we saw previously:\\nIn[40]: (x > 4) and (x < 8)\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-40-3d24f1ffd63d> in <module>()\\n----> 1 (x > 4) and (x < 8)\\nValueError: The truth value of an array with more than one element is...\\nSo remember this: and and or perform a single Boolean evaluation on an entire\\nobject, while & and | perform multiple Boolean evaluations on the content (the indi‐\\nvidual bits or bytes) of an object. For Boolean NumPy arrays, the latter is nearly\\nalways the desired operation.\\nFancy Indexing\\nIn the previous sections, we saw how to access and modify portions of arrays using\\nsimple indices (e.g., arr[0]), slices (e.g., arr[:5]), and Boolean masks (e.g., arr[arr\\n> 0]). In this section, we’ll look at another style of array indexing, known as fancy\\nindexing. Fancy indexing is like the simple indexing we’ve already seen, but we pass\\narrays of indices in place of single scalars. This allows us to very quickly access and\\nmodify complicated subsets of an array’s values.\\n78 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 96}, page_content='Exploring Fancy Indexing\\nFancy indexing is conceptually simple: it means passing an array of indices to access\\nmultiple array elements at once. For example, consider the following array:\\nIn[1]: import numpy as np\\n       rand = np.random.RandomState(42)\\n       x = rand.randint(100, size=10)\\n       print(x)\\n[51 92 14 71 60 20 82 86 74 74]\\nSuppose we want to access three different elements. We could do it like this:\\nIn[2]: [x[3], x[7], x[2]]\\nOut[2]: [71, 86, 14]\\nAlternatively, we can pass a single list or array of indices to obtain the same result:\\nIn[3]: ind = [3, 7, 4]\\n       x[ind]\\nOut[3]: array([71, 86, 60])\\nWith fancy indexing, the shape of the result reflects the shape of the index arrays\\nrather than the shape of the array being indexed:\\nIn[4]: ind = np.array([[3, 7],\\n                       [4, 5]])\\n       x[ind]\\nOut[4]: array([[71, 86],\\n               [60, 20]])\\nFancy indexing also works in multiple dimensions. Consider the following array:\\nIn[5]: X = np.arange(12).reshape((3, 4))\\n       X\\nOut[5]: array([[ 0,  1,  2,  3],\\n               [ 4,  5,  6,  7],\\n               [ 8,  9, 10, 11]])\\nLike with standard indexing, the first index refers to the row, and the second to the\\ncolumn:\\nIn[6]: row = np.array([0, 1, 2])\\n       col = np.array([2, 1, 3])\\n       X[row, col]\\nOut[6]: array([ 2,  5, 11])\\nNotice that the first value in the result is X[0, 2], the second is X[1, 1], and the\\nthird is X[2, 3]. The pairing of indices in fancy indexing follows all the broadcasting\\nrules that were mentioned in “Computation on Arrays: Broadcasting” on page 63. So,\\nFancy Indexing \\n| \\n79'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 97}, page_content='for example, if we combine a column vector and a row vector within the indices, we\\nget a two-dimensional result:\\nIn[7]: X[row[:, np.newaxis], col]\\nOut[7]: array([[ 2,  1,  3],\\n               [ 6,  5,  7],\\n               [10,  9, 11]])\\nHere, each row value is matched with each column vector, exactly as we saw in broad‐\\ncasting of arithmetic operations. For example:\\nIn[8]: row[:, np.newaxis] * col\\nOut[8]: array([[0, 0, 0],\\n               [2, 1, 3],\\n               [4, 2, 6]])\\nIt is always important to remember with fancy indexing that the return value reflects\\nthe broadcasted shape of the indices, rather than the shape of the array being indexed.\\nCombined Indexing\\nFor even more powerful operations, fancy indexing can be combined with the other\\nindexing schemes we’ve seen:\\nIn[9]: print(X)\\n[[ 0  1  2  3]\\n [ 4  5  6  7]\\n [ 8  9 10 11]]\\nWe can combine fancy and simple indices:\\nIn[10]: X[2, [2, 0, 1]]\\nOut[10]: array([10,  8,  9])\\nWe can also combine fancy indexing with slicing:\\nIn[11]: X[1:, [2, 0, 1]]\\nOut[11]: array([[ 6,  4,  5],\\n                [10,  8,  9]])\\nAnd we can combine fancy indexing with masking:\\nIn[12]: mask = np.array([1, 0, 1, 0], dtype=bool)\\n        X[row[:, np.newaxis], mask]\\nOut[12]: array([[ 0,  2],\\n                [ 4,  6],\\n                [ 8, 10]])\\nAll of these indexing options combined lead to a very flexible set of operations for\\naccessing and modifying array values.\\n80 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 98}, page_content='Example: Selecting Random Points\\nOne common use of fancy indexing is the selection of subsets of rows from a matrix.\\nFor example, we might have an N by D matrix representing N points in D dimen‐\\nsions, such as the following points drawn from a two-dimensional normal distribu‐\\ntion:\\nIn[13]: mean = [0, 0]\\n        cov = [[1, 2],\\n               [2, 5]]\\n        X = rand.multivariate_normal(mean, cov, 100)\\n        X.shape\\nOut[13]: (100, 2)\\nUsing the plotting tools we will discuss in Chapter 4, we can visualize these points as\\na scatter plot (Figure 2-7):\\nIn[14]: %matplotlib inline\\n        import matplotlib.pyplot as plt\\n        import seaborn; seaborn.set()  # for plot styling\\n        plt.scatter(X[:, 0], X[:, 1]);\\nFigure 2-7. Normally distributed points\\nLet’s use fancy indexing to select 20 random points. We’ll do this by first choosing 20\\nrandom indices with no repeats, and use these indices to select a portion of the origi‐\\nnal array:\\nIn[15]: indices = np.random.choice(X.shape[0], 20, replace=False)\\n        indices\\nOut[15]: array([93, 45, 73, 81, 50, 10, 98, 94,  4, 64, 65, 89, 47, 84, 82,\\n                80, 25, 90, 63, 20])\\nFancy Indexing \\n| \\n81'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 99}, page_content=\"In[16]: selection = X[indices]  # fancy indexing here\\n        selection.shape\\nOut[16]: (20, 2)\\nNow to see which points were selected, let’s over-plot large circles at the locations of\\nthe selected points (Figure 2-8):\\nIn[17]: plt.scatter(X[:, 0], X[:, 1], alpha=0.3)\\n        plt.scatter(selection[:, 0], selection[:, 1],\\n                    facecolor='none', s=200);\\nFigure 2-8. Random selection among points\\nThis sort of strategy is often used to quickly partition datasets, as is often needed in\\ntrain/test splitting for validation of statistical models (see “Hyperparameters and\\nModel Validation” on page 359), and in sampling approaches to answering statistical\\nquestions.\\nModifying Values with Fancy Indexing\\nJust as fancy indexing can be used to access parts of an array, it can also be used to\\nmodify parts of an array. For example, imagine we have an array of indices and we’d\\nlike to set the corresponding items in an array to some value:\\nIn[18]: x = np.arange(10)\\n        i = np.array([2, 1, 8, 4])\\n        x[i] = 99\\n        print(x)\\n[ 0 99 99  3 99  5  6  7 99  9]\\nWe can use any assignment-type operator for this. For example:\\n82 \\n| \\nChapter 2: Introduction to NumPy\\nwww.allitebooks.com\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 100}, page_content='In[19]: x[i] -= 10\\n        print(x)\\n[ 0 89 89  3 89  5  6  7 89  9]\\nNotice, though, that repeated indices with these operations can cause some poten‐\\ntially unexpected results. Consider the following:\\nIn[20]: x = np.zeros(10)\\n        x[[0, 0]] = [4, 6]\\n        print(x)\\n[ 6.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\\nWhere did the 4 go? The result of this operation is to first assign x[0] = 4, followed\\nby x[0] = 6. The result, of course, is that x[0] contains the value 6.\\nFair enough, but consider this operation:\\nIn[21]: i = [2, 3, 3, 4, 4, 4]\\n        x[i] += 1\\n        x\\nOut[21]: array([ 6.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])\\nYou might expect that x[3] would contain the value 2, and x[4] would contain the\\nvalue 3, as this is how many times each index is repeated. Why is this not the case?\\nConceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1.\\nx[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in\\nmind, it is not the augmentation that happens multiple times, but the assignment,\\nwhich leads to the rather nonintuitive results.\\nSo what if you want the other behavior where the operation is repeated? For this, you\\ncan use the at() method of ufuncs (available since NumPy 1.8), and do the following:\\nIn[22]: x = np.zeros(10)\\n        np.add.at(x, i, 1)\\n        print(x)\\n[ 0.  0.  1.  2.  3.  0.  0.  0.  0.  0.]\\nThe at() method does an in-place application of the given operator at the specified\\nindices (here, i) with the specified value (here, 1). Another method that is similar in\\nspirit is the reduceat() method of ufuncs, which you can read about in the NumPy\\ndocumentation.\\nExample: Binning Data\\nYou can use these ideas to efficiently bin data to create a histogram by hand. For\\nexample, imagine we have 1,000 values and would like to quickly find where they fall\\nwithin an array of bins. We could compute it using ufunc.at like this:\\nFancy Indexing \\n| \\n83'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 101}, page_content='In[23]: np.random.seed(42)\\n        x = np.random.randn(100)\\n        # compute a histogram by hand\\n        bins = np.linspace(-5, 5, 20)\\n        counts = np.zeros_like(bins)\\n        # find the appropriate bin for each x\\n        i = np.searchsorted(bins, x)\\n        # add 1 to each of these bins\\n        np.add.at(counts, i, 1)\\nThe counts now reflect the number of points within each bin—in other words, a his‐\\ntogram (Figure 2-9):\\nIn[24]: # plot the results\\n        plt.plot(bins, counts, linestyle=\\'steps\\');\\nFigure 2-9. A histogram computed by hand\\nOf course, it would be silly to have to do this each time you want to plot a histogram.\\nThis is why Matplotlib provides the plt.hist() routine, which does the same in a\\nsingle line:\\nplt.hist(x, bins, histtype=\\'step\\');\\nThis function will create a nearly identical plot to the one seen here. To compute the\\nbinning, Matplotlib uses the np.histogram function, which does a very similar com‐\\nputation to what we did before. Let’s compare the two here:\\nIn[25]: print(\"NumPy routine:\")\\n        %timeit counts, edges = np.histogram(x, bins)\\n84 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 102}, page_content='print(\"Custom routine:\")\\n        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)\\nNumPy routine:\\n10000 loops, best of 3: 97.6 µs per loop\\nCustom routine:\\n10000 loops, best of 3: 19.5 µs per loop\\nOur own one-line algorithm is several times faster than the optimized algorithm in\\nNumPy! How can this be? If you dig into the np.histogram source code (you can do\\nthis in IPython by typing np.histogram??), you’ll see that it’s quite a bit more\\ninvolved than the simple search-and-count that we’ve done; this is because NumPy’s\\nalgorithm is more flexible, and particularly is designed for better performance when\\nthe number of data points becomes large:\\nIn[26]: x = np.random.randn(1000000)\\n        print(\"NumPy routine:\")\\n        %timeit counts, edges = np.histogram(x, bins)\\n        print(\"Custom routine:\")\\n        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)\\nNumPy routine:\\n10 loops, best of 3: 68.7 ms per loop\\nCustom routine:\\n10 loops, best of 3: 135 ms per loop\\nWhat this comparison shows is that algorithmic efficiency is almost never a simple\\nquestion. An algorithm efficient for large datasets will not always be the best choice\\nfor small datasets, and vice versa (see “Big-O Notation” on page 92). But the advan‐\\ntage of coding this algorithm yourself is that with an understanding of these basic\\nmethods, you could use these building blocks to extend this to do some very interest‐\\ning custom behaviors. The key to efficiently using Python in data-intensive applica‐\\ntions is knowing about general convenience routines like np.histogram and when\\nthey’re appropriate, but also knowing how to make use of lower-level functionality\\nwhen you need more pointed behavior.\\nSorting Arrays\\nUp to this point we have been concerned mainly with tools to access and operate on\\narray data with NumPy. This section covers algorithms related to sorting values in\\nNumPy arrays. These algorithms are a favorite topic in introductory computer sci‐\\nence courses: if you’ve ever taken one, you probably have had dreams (or, depending\\non your temperament, nightmares) about insertion sorts, selection sorts, merge sorts,\\nquick sorts, bubble sorts, and many, many more. All are means of accomplishing a\\nsimilar task: sorting the values in a list or array.\\nSorting Arrays \\n| \\n85'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 103}, page_content='For example, a simple selection sort repeatedly finds the minimum value from a list,\\nand makes swaps until the list is sorted. We can code this in just a few lines of Python:\\nIn[1]: import numpy as np\\n       def selection_sort(x):\\n           for i in range(len(x)):\\n               swap = i + np.argmin(x[i:])\\n               (x[i], x[swap]) = (x[swap], x[i])\\n           return x\\nIn[2]: x = np.array([2, 1, 4, 3, 5])\\n       selection_sort(x)\\nOut[2]: array([1, 2, 3, 4, 5])\\nAs any first-year computer science major will tell you, the selection sort is useful for\\nits simplicity, but is much too slow to be useful for larger arrays. For a list of N values,\\nit requires N loops, each of which does on the order of ~ N comparisons to find the\\nswap value. In terms of the “big-O” notation often used to characterize these algo‐\\nrithms (see “Big-O Notation” on page 92), selection sort averages �N2 : if you dou‐\\nble the number of items in the list, the execution time will go up by about a factor of\\nfour.\\nEven selection sort, though, is much better than my all-time favorite sorting algo‐\\nrithms, the bogosort:\\nIn[3]: def bogosort(x):\\n           while np.any(x[:-1] > x[1:]):\\n               np.random.shuffle(x)\\n           return x\\nIn[4]: x = np.array([2, 1, 4, 3, 5])\\n       bogosort(x)\\nOut[4]: array([1, 2, 3, 4, 5])\\nThis silly sorting method relies on pure chance: it repeatedly applies a random shuf‐\\nfling of the array until the result happens to be sorted. With an average scaling of\\n�N × N!  (that’s N times N factorial), this should—quite obviously—never be used\\nfor any real computation.\\nFortunately, Python contains built-in sorting algorithms that are much more efficient\\nthan either of the simplistic algorithms just shown. We’ll start by looking at the\\nPython built-ins, and then take a look at the routines included in NumPy and opti‐\\nmized for NumPy arrays.\\nFast Sorting in NumPy: np.sort and np.argsort\\nAlthough Python has built-in sort and sorted functions to work with lists, we won’t\\ndiscuss them here because NumPy’s np.sort function turns out to be much more\\n86 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 104}, page_content='efficient and useful for our purposes. By default np.sort uses an �N log N , quick‐\\nsort algorithm, though mergesort and heapsort are also available. For most applica‐\\ntions, the default quicksort is more than sufficient.\\nTo return a sorted version of the array without modifying the input, you can use\\nnp.sort:\\nIn[5]: x = np.array([2, 1, 4, 3, 5])\\n       np.sort(x)\\nOut[5]: array([1, 2, 3, 4, 5])\\nIf you prefer to sort the array in-place, you can instead use the sort method of arrays:\\nIn[6]: x.sort()\\n       print(x)\\n[1 2 3 4 5]\\nA related function is argsort, which instead returns the indices of the sorted\\nelements:\\nIn[7]: x = np.array([2, 1, 4, 3, 5])\\n       i = np.argsort(x)\\n       print(i)\\n[1 0 3 2 4]\\nThe first element of this result gives the index of the smallest element, the second\\nvalue gives the index of the second smallest, and so on. These indices can then be\\nused (via fancy indexing) to construct the sorted array if desired:\\nIn[8]: x[i]\\nOut[8]: array([1, 2, 3, 4, 5])\\nSorting along rows or columns\\nA useful feature of NumPy’s sorting algorithms is the ability to sort along specific\\nrows or columns of a multidimensional array using the axis argument. For example:\\nIn[9]: rand = np.random.RandomState(42)\\n       X = rand.randint(0, 10, (4, 6))\\n       print(X)\\n[[6 3 7 4 6 9]\\n [2 6 7 4 3 7]\\n [7 2 5 4 1 7]\\n [5 1 4 0 9 5]]\\nIn[10]: # sort each column of X\\n        np.sort(X, axis=0)\\nOut[10]: array([[2, 1, 4, 0, 1, 5],\\n                [5, 2, 5, 4, 3, 7],\\nSorting Arrays \\n| \\n87'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 105}, page_content='[6, 3, 7, 4, 6, 7],\\n                [7, 6, 7, 4, 9, 9]])\\nIn[11]: # sort each row of X\\n        np.sort(X, axis=1)\\nOut[11]: array([[3, 4, 6, 6, 7, 9],\\n                [2, 3, 4, 6, 7, 7],\\n                [1, 2, 4, 5, 7, 7],\\n                [0, 1, 4, 5, 5, 9]])\\nKeep in mind that this treats each row or column as an independent array, and any\\nrelationships between the row or column values will be lost!\\nPartial Sorts: Partitioning\\nSometimes we’re not interested in sorting the entire array, but simply want to find the\\nK smallest values in the array. NumPy provides this in the np.partition function.\\nnp.partition takes an array and a number K; the result is a new array with the small‐\\nest K values to the left of the partition, and the remaining values to the right, in arbi‐\\ntrary order:\\nIn[12]: x = np.array([7, 2, 3, 1, 6, 5, 4])\\n        np.partition(x, 3)\\nOut[12]: array([2, 1, 3, 4, 6, 5, 7])\\nNote that the first three values in the resulting array are the three smallest in the\\narray, and the remaining array positions contain the remaining values. Within the\\ntwo partitions, the elements have arbitrary order.\\nSimilarly to sorting, we can partition along an arbitrary axis of a multidimensional\\narray:\\nIn[13]: np.partition(X, 2, axis=1)\\nOut[13]: array([[3, 4, 6, 7, 6, 9],\\n                [2, 3, 4, 7, 6, 7],\\n                [1, 2, 4, 5, 7, 7],\\n                [0, 1, 4, 5, 9, 5]])\\nThe result is an array where the first two slots in each row contain the smallest values\\nfrom that row, with the remaining values filling the remaining slots.\\nFinally, just as there is a np.argsort that computes indices of the sort, there is a\\nnp.argpartition that computes indices of the partition. We’ll see this in action in the\\nfollowing section.\\nExample: k-Nearest Neighbors\\nLet’s quickly see how we might use this argsort function along multiple axes to find\\nthe nearest neighbors of each point in a set. We’ll start by creating a random set of 10\\n88 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 106}, page_content='points on a two-dimensional plane. Using the standard convention, we’ll arrange\\nthese in a 10×2 array:\\nIn[14]: X = rand.rand(10, 2)\\nTo get an idea of how these points look, let’s quickly scatter plot them (Figure 2-10):\\nIn[15]: %matplotlib inline\\n        import matplotlib.pyplot as plt\\n        import seaborn; seaborn.set() # Plot styling\\n        plt.scatter(X[:, 0], X[:, 1], s=100);\\nFigure 2-10. Visualization of points in the k-neighbors example\\nNow we’ll compute the distance between each pair of points. Recall that the squared-\\ndistance between two points is the sum of the squared differences in each dimension;\\nusing the efficient broadcasting (“Computation on Arrays: Broadcasting” on page 63)\\nand aggregation (“Aggregations: Min, Max, and Everything in Between” on page 58)\\nroutines provided by NumPy, we can compute the matrix of square distances in a sin‐\\ngle line of code:\\nIn[16]: dist_sq = np.sum((X[:,np.newaxis,:] - X[np.newaxis,:,:]) ** 2, axis=-1)\\nThis operation has a lot packed into it, and it might be a bit confusing if you’re unfa‐\\nmiliar with NumPy’s broadcasting rules. When you come across code like this, it can\\nbe useful to break it down into its component steps:\\nIn[17]: # for each pair of points, compute differences in their coordinates\\n        differences = X[:, np.newaxis, :] - X[np.newaxis, :, :]\\n        differences.shape\\nOut[17]: (10, 10, 2)\\nSorting Arrays \\n| \\n89'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 107}, page_content='In[18]: # square the coordinate differences\\n        sq_differences = differences ** 2\\n        sq_differences.shape\\nOut[18]: (10, 10, 2)\\nIn[19]: # sum the coordinate differences to get the squared distance\\n        dist_sq = sq_differences.sum(-1)\\n        dist_sq.shape\\nOut[19]: (10, 10)\\nJust to double-check what we are doing, we should see that the diagonal of this matrix\\n(i.e., the set of distances between each point and itself) is all zero:\\nIn[20]: dist_sq.diagonal()\\nOut[20]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\\nIt checks out! With the pairwise square-distances converted, we can now use np.arg\\nsort to sort along each row. The leftmost columns will then give the indices of the\\nnearest neighbors:\\nIn[21]: nearest = np.argsort(dist_sq, axis=1)\\n        print(nearest)\\n[[0 3 9 7 1 4 2 5 6 8]\\n [1 4 7 9 3 6 8 5 0 2]\\n [2 1 4 6 3 0 8 9 7 5]\\n [3 9 7 0 1 4 5 8 6 2]\\n [4 1 8 5 6 7 9 3 0 2]\\n [5 8 6 4 1 7 9 3 2 0]\\n [6 8 5 4 1 7 9 3 2 0]\\n [7 9 3 1 4 0 5 8 6 2]\\n [8 5 6 4 1 7 9 3 2 0]\\n [9 7 3 0 1 4 5 8 6 2]]\\nNotice that the first column gives the numbers 0 through 9 in order: this is due to the\\nfact that each point’s closest neighbor is itself, as we would expect.\\nBy using a full sort here, we’ve actually done more work than we need to in this case.\\nIf we’re simply interested in the nearest k neighbors, all we need is to partition each\\nrow so that the smallest k + 1 squared distances come first, with larger distances fill‐\\ning the remaining positions of the array. We can do this with the np.argpartition\\nfunction:\\nIn[22]: K = 2\\n        nearest_partition = np.argpartition(dist_sq, K + 1, axis=1)\\nIn order to visualize this network of neighbors, let’s quickly plot the points along with\\nlines representing the connections from each point to its two nearest neighbors\\n(Figure 2-11):\\n90 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 108}, page_content=\"In[23]: plt.scatter(X[:, 0], X[:, 1], s=100)\\n        # draw lines from each point to its two nearest neighbors\\n        K = 2\\n        for i in range(X.shape[0]):\\n            for j in nearest_partition[i, :K+1]:\\n                # plot a line from X[i] to X[j]\\n                # use some zip magic to make it happen:\\n                plt.plot(*zip(X[j], X[i]), color='black')\\nFigure 2-11. Visualization of the neighbors of each point\\nEach point in the plot has lines drawn to its two nearest neighbors. At first glance, it\\nmight seem strange that some of the points have more than two lines coming out of\\nthem: this is due to the fact that if point A is one of the two nearest neighbors of point\\nB, this does not necessarily imply that point B is one of the two nearest neighbors of\\npoint A.\\nAlthough the broadcasting and row-wise sorting of this approach might seem less\\nstraightforward than writing a loop, it turns out to be a very efficient way of operating\\non this data in Python. You might be tempted to do the same type of operation by\\nmanually looping through the data and sorting each set of neighbors individually, but\\nthis would almost certainly lead to a slower algorithm than the vectorized version we\\nused. The beauty of this approach is that it’s written in a way that’s agnostic to the size\\nof the input data: we could just as easily compute the neighbors among 100 or\\n1,000,000 points in any number of dimensions, and the code would look the same.\\nFinally, I’ll note that when doing very large nearest-neighbor searches, there are tree-\\nbased and/or approximate algorithms that can scale as �N log N  or better rather\\nSorting Arrays \\n| \\n91\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 109}, page_content='than the �N2  of the brute-force algorithm. One example of this is the KD-Tree,\\nimplemented in Scikit-Learn.\\nBig-O Notation\\nBig-O notation is a means of describing how the number of operations required for\\nan algorithm scales as the input grows in size. To use it correctly is to dive deeply into\\nthe realm of computer science theory, and to carefully distinguish it from the related\\nsmall-o notation, big-θ notation, big-Ω notation, and probably many mutant hybrids\\nthereof. While these distinctions add precision to statements about algorithmic scal‐\\ning, outside computer science theory exams and the remarks of pedantic blog com‐\\nmenters, you’ll rarely see such distinctions made in practice. Far more common in the\\ndata science world is a less rigid use of big-O notation: as a general (if imprecise)\\ndescription of the scaling of an algorithm. With apologies to theorists and pedants,\\nthis is the interpretation we’ll use throughout this book.\\nBig-O notation, in this loose sense, tells you how much time your algorithm will take\\nas you increase the amount of data. If you have an �N  (read “order N”) algorithm\\nthat takes 1 second to operate on a list of length N=1,000, then you should expect it to\\ntake roughly 5 seconds for a list of length N=5,000. If you have an �N2  (read “order\\nN squared”) algorithm that takes 1 second for N=1,000, then you should expect it to\\ntake about 25 seconds for N=5,000.\\nFor our purposes, the N will usually indicate some aspect of the size of the dataset (the\\nnumber of points, the number of dimensions, etc.). When trying to analyze billions or\\ntrillions of samples, the difference between �N  and �N2  can be far from trivial!\\nNotice that the big-O notation by itself tells you nothing about the actual wall-clock\\ntime of a computation, but only about its scaling as you change N. Generally, for\\nexample, an �N  algorithm is considered to have better scaling than an �N2  algo‐\\nrithm, and for good reason. But for small datasets in particular, the algorithm with\\nbetter scaling might not be faster. For example, in a given problem an �N2  algo‐\\nrithm might take 0.01 seconds, while a “better” �N  algorithm might take 1 second.\\nScale up N by a factor of 1,000, though, and the �N  algorithm will win out.\\nEven this loose version of Big-O notation can be very useful for comparing the per‐\\nformance of algorithms, and we’ll use this notation throughout the book when talking\\nabout how algorithms scale.\\nStructured Data: NumPy’s Structured Arrays\\nWhile often our data can be well represented by a homogeneous array of values,\\nsometimes this is not the case. This section demonstrates the use of NumPy’s struc‐\\ntured arrays and record arrays, which provide efficient storage for compound, hetero‐\\n92 \\n| \\nChapter 2: Introduction to NumPy'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 110}, page_content=\"geneous data. While the patterns shown here are useful for simple operations,\\nscenarios like this often lend themselves to the use of Pandas DataFrames, which we’ll\\nexplore in Chapter 3.\\nImagine that we have several categories of data on a number of people (say, name,\\nage, and weight), and we’d like to store these values for use in a Python program. It\\nwould be possible to store these in three separate arrays:\\nIn[2]: name = ['Alice', 'Bob', 'Cathy', 'Doug']\\n       age = [25, 45, 37, 19]\\n       weight = [55.0, 85.5, 68.0, 61.5]\\nBut this is a bit clumsy. There’s nothing here that tells us that the three arrays are\\nrelated; it would be more natural if we could use a single structure to store all of this\\ndata. NumPy can handle this through structured arrays, which are arrays with com‐\\npound data types.\\nRecall that previously we created a simple array using an expression like this:\\nIn[3]: x = np.zeros(4, dtype=int)\\nWe can similarly create a structured array using a compound data type specification:\\nIn[4]: # Use a compound data type for structured arrays\\n       data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),\\n                                 'formats':('U10', 'i4', 'f8')})\\n       print(data.dtype)\\n[('name', '<U10'), ('age', '<i4'), ('weight', '<f8')]\\nHere 'U10' translates to “Unicode string of maximum length 10,” 'i4' translates to\\n“4-byte (i.e., 32 bit) integer,” and 'f8' translates to “8-byte (i.e., 64 bit) float.” We’ll\\ndiscuss other options for these type codes in the following section.\\nNow that we’ve created an empty container array, we can fill the array with our lists of\\nvalues:\\nIn[5]: data['name'] = name\\n       data['age'] = age\\n       data['weight'] = weight\\n       print(data)\\n[('Alice', 25, 55.0) ('Bob', 45, 85.5) ('Cathy', 37, 68.0)\\n ('Doug', 19, 61.5)]\\nAs we had hoped, the data is now arranged together in one convenient block of\\nmemory.\\nThe handy thing with structured arrays is that you can now refer to values either by\\nindex or by name:\\nIn[6]: # Get all names\\n       data['name']\\nStructured Data: NumPy’s Structured Arrays \\n| \\n93\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 111}, page_content=\"Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'],\\n              dtype='<U10')\\nIn[7]: # Get first row of data\\n       data[0]\\nOut[7]: ('Alice', 25, 55.0)\\nIn[8]: # Get the name from the last row\\n       data[-1]['name']\\nOut[8]: 'Doug'\\nUsing Boolean masking, this even allows you to do some more sophisticated opera‐\\ntions such as filtering on age:\\nIn[9]: # Get names where age is under 30\\n       data[data['age'] < 30]['name']\\nOut[9]: array(['Alice', 'Doug'],\\n              dtype='<U10')\\nNote that if you’d like to do any operations that are any more complicated than these,\\nyou should probably consider the Pandas package, covered in the next chapter. As\\nwe’ll see, Pandas provides a DataFrame object, which is a structure built on NumPy\\narrays that offers a variety of useful data manipulation functionality similar to what\\nwe’ve shown here, as well as much, much more.\\nCreating Structured Arrays\\nStructured array data types can be specified in a number of ways. Earlier, we saw the\\ndictionary method:\\nIn[10]: np.dtype({'names':('name', 'age', 'weight'),\\n                  'formats':('U10', 'i4', 'f8')})\\nOut[10]: dtype([('name', '<U10'), ('age', '<i4'), ('weight', '<f8')])\\nFor clarity, numerical types can be specified with Python types or NumPy dtypes\\ninstead:\\nIn[11]: np.dtype({'names':('name', 'age', 'weight'),\\n                  'formats':((np.str_, 10), int, np.float32)})\\nOut[11]: dtype([('name', '<U10'), ('age', '<i8'), ('weight', '<f4')])\\nA compound type can also be specified as a list of tuples:\\nIn[12]: np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])\\nOut[12]: dtype([('name', 'S10'), ('age', '<i4'), ('weight', '<f8')])\\nIf the names of the types do not matter to you, you can specify the types alone in a\\ncomma-separated string:\\nIn[13]: np.dtype('S10,i4,f8')\\n94 \\n| \\nChapter 2: Introduction to NumPy\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 112}, page_content=\"Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')])\\nThe shortened string format codes may seem confusing, but they are built on simple\\nprinciples. The first (optional) character is < or >, which means “little endian” or “big\\nendian,” respectively, and specifies the ordering convention for significant bits. The\\nnext character specifies the type of data: characters, bytes, ints, floating points, and so\\non (see Table 2-4). The last character or characters represents the size of the object in\\nbytes.\\nTable 2-4. NumPy data types\\nCharacter\\nDescription\\nExample\\n'b'\\nByte\\nnp.dtype('b')\\n'i'\\nSigned integer\\nnp.dtype('i4') == np.int32\\n'u'\\nUnsigned integer\\nnp.dtype('u1') == np.uint8\\n'f'\\nFloating point\\nnp.dtype('f8') == np.int64\\n'c'\\nComplex floating point\\nnp.dtype('c16') == np.complex128\\n'S', 'a' string\\nnp.dtype('S5')\\n'U'\\nUnicode string\\nnp.dtype('U') == np.str_\\n'V'\\nRaw data (void)\\nnp.dtype('V') == np.void\\nMore Advanced Compound Types\\nIt is possible to define even more advanced compound types. For example, you can\\ncreate a type where each element contains an array or matrix of values. Here, we’ll\\ncreate a data type with a mat component consisting of a 3×3 floating-point matrix:\\nIn[14]: tp = np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))])\\n        X = np.zeros(1, dtype=tp)\\n        print(X[0])\\n        print(X['mat'][0])\\n(0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\\n[[ 0.  0.  0.]\\n [ 0.  0.  0.]\\n [ 0.  0.  0.]]\\nNow each element in the X array consists of an id and a 3×3 matrix. Why would you\\nuse this rather than a simple multidimensional array, or perhaps a Python dictionary?\\nThe reason is that this NumPy dtype directly maps onto a C structure definition, so\\nthe buffer containing the array content can be accessed directly within an appropri‐\\nately written C program. If you find yourself writing a Python interface to a legacy C\\nor Fortran library that manipulates structured data, you’ll probably find structured\\narrays quite useful!\\nStructured Data: NumPy’s Structured Arrays \\n| \\n95\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 113}, page_content=\"RecordArrays: Structured Arrays with a Twist\\nNumPy also provides the np.recarray class, which is almost identical to the struc‐\\ntured arrays just described, but with one additional feature: fields can be accessed as\\nattributes rather than as dictionary keys. Recall that we previously accessed the ages\\nby writing:\\nIn[15]: data['age']\\nOut[15]: array([25, 45, 37, 19], dtype=int32)\\nIf we view our data as a record array instead, we can access this with slightly fewer\\nkeystrokes:\\nIn[16]: data_rec = data.view(np.recarray)\\n        data_rec.age\\nOut[16]: array([25, 45, 37, 19], dtype=int32)\\nThe downside is that for record arrays, there is some extra overhead involved in\\naccessing the fields, even when using the same syntax. We can see this here:\\nIn[17]: %timeit data['age']\\n        %timeit data_rec['age']\\n        %timeit data_rec.age\\n1000000 loops, best of 3: 241 ns per loop\\n100000 loops, best of 3: 4.61 µs per loop\\n100000 loops, best of 3: 7.27 µs per loop\\nWhether the more convenient notation is worth the additional overhead will depend\\non your own application.\\nOn to Pandas\\nThis section on structured and record arrays is purposely at the end of this chapter,\\nbecause it leads so well into the next package we will cover: Pandas. Structured arrays\\nlike the ones discussed here are good to know about for certain situations, especially\\nin case you’re using NumPy arrays to map onto binary data formats in C, Fortran, or\\nanother language. For day-to-day use of structured data, the Pandas package is a\\nmuch better choice, and we’ll dive into a full discussion of it in the next chapter.\\n96 \\n| \\nChapter 2: Introduction to NumPy\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 114}, page_content='CHAPTER 3\\nData Manipulation with Pandas\\nIn the previous chapter, we dove into detail on NumPy and its ndarray object, which\\nprovides efficient storage and manipulation of dense typed arrays in Python. Here\\nwe’ll build on this knowledge by looking in detail at the data structures provided by\\nthe Pandas library. Pandas is a newer package built on top of NumPy, and provides an\\nefficient implementation of a DataFrame. DataFrames are essentially multidimen‐\\nsional arrays with attached row and column labels, and often with heterogeneous\\ntypes and/or missing data. As well as offering a convenient storage interface for\\nlabeled data, Pandas implements a number of powerful data operations familiar to\\nusers of both database frameworks and spreadsheet programs.\\nAs we saw, NumPy’s ndarray data structure provides essential features for the type of\\nclean, well-organized data typically seen in numerical computing tasks. While it\\nserves this purpose very well, its limitations become clear when we need more flexi‐\\nbility (attaching labels to data, working with missing data, etc.) and when attempting\\noperations that do not map well to element-wise broadcasting (groupings, pivots,\\netc.), each of which is an important piece of analyzing the less structured data avail‐\\nable in many forms in the world around us. Pandas, and in particular its Series and\\nDataFrame objects, builds on the NumPy array structure and provides efficient access\\nto these sorts of “data munging” tasks that occupy much of a data scientist’s time.\\nIn this chapter, we will focus on the mechanics of using Series, DataFrame, and\\nrelated structures effectively. We will use examples drawn from real datasets where\\nappropriate, but these examples are not necessarily the focus.\\nInstalling and Using Pandas\\nInstalling Pandas on your system requires NumPy to be installed, and if you’re build‐\\ning the library from source, requires the appropriate tools to compile the C and\\n97'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 115}, page_content=\"Cython sources on which Pandas is built. Details on this installation can be found in\\nthe Pandas documentation. If you followed the advice outlined in the preface and\\nused the Anaconda stack, you already have Pandas installed.\\nOnce Pandas is installed, you can import it and check the version:\\nIn[1]: import pandas\\n       pandas.__version__\\nOut[1]: '0.18.1'\\nJust as we generally import NumPy under the alias np, we will import Pandas under\\nthe alias pd:\\nIn[2]: import pandas as pd\\nThis import convention will be used throughout the remainder of this book.\\nReminder About Built-In Documentation\\nAs you read through this chapter, don’t forget that IPython gives you the ability to\\nquickly explore the contents of a package (by using the tab-completion feature) as\\nwell as the documentation of various functions (using the ? character). (Refer back to\\n“Help and Documentation in IPython” on page 3 if you need a refresher on this.)\\nFor example, to display all the contents of the pandas namespace, you can type this:\\nIn [3]: pd.<TAB>\\nAnd to display the built-in Pandas documentation, you can use this:\\nIn [4]: pd?\\nMore detailed documentation, along with tutorials and other resources, can be found\\nat http://pandas.pydata.org/.\\nIntroducing Pandas Objects\\nAt the very basic level, Pandas objects can be thought of as enhanced versions of\\nNumPy structured arrays in which the rows and columns are identified with labels\\nrather than simple integer indices. As we will see during the course of this chapter,\\nPandas provides a host of useful tools, methods, and functionality on top of the basic\\ndata structures, but nearly everything that follows will require an understanding of\\nwhat these structures are. Thus, before we go any further, let’s introduce these three\\nfundamental Pandas data structures: the Series, DataFrame, and Index.\\nWe will start our code sessions with the standard NumPy and Pandas imports:\\nIn[1]: import numpy as np\\n       import pandas as pd\\n98 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 116}, page_content='The Pandas Series Object\\nA Pandas Series is a one-dimensional array of indexed data. It can be created from a\\nlist or array as follows:\\nIn[2]: data = pd.Series([0.25, 0.5, 0.75, 1.0])\\n       data\\nOut[2]: 0    0.25\\n        1    0.50\\n        2    0.75\\n        3    1.00\\n        dtype: float64\\nAs we see in the preceding output, the Series wraps both a sequence of values and a\\nsequence of indices, which we can access with the values and index attributes. The\\nvalues are simply a familiar NumPy array:\\nIn[3]: data.values\\nOut[3]: array([ 0.25,  0.5 ,  0.75,  1.  ])\\nThe index is an array-like object of type pd.Index, which we’ll discuss in more detail\\nmomentarily:\\nIn[4]: data.index\\nOut[4]: RangeIndex(start=0, stop=4, step=1)\\nLike with a NumPy array, data can be accessed by the associated index via the familiar\\nPython square-bracket notation:\\nIn[5]: data[1]\\nOut[5]: 0.5\\nIn[6]: data[1:3]\\nOut[6]: 1    0.50\\n        2    0.75\\n        dtype: float64\\nAs we will see, though, the Pandas Series is much more general and flexible than the\\none-dimensional NumPy array that it emulates.\\nSeries as generalized NumPy array\\nFrom what we’ve seen so far, it may look like the Series object is basically inter‐\\nchangeable with a one-dimensional NumPy array. The essential difference is the pres‐\\nence of the index: while the NumPy array has an implicitly defined integer index used\\nto access the values, the Pandas Series has an explicitly defined index associated with\\nthe values.\\nIntroducing Pandas Objects \\n| \\n99'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 117}, page_content=\"This explicit index definition gives the Series object additional capabilities. For\\nexample, the index need not be an integer, but can consist of values of any desired\\ntype. For example, if we wish, we can use strings as an index:\\nIn[7]: data = pd.Series([0.25, 0.5, 0.75, 1.0],\\n                        index=['a', 'b', 'c', 'd'])\\n       data\\nOut[7]: a    0.25\\n        b    0.50\\n        c    0.75\\n        d    1.00\\n        dtype: float64\\nAnd the item access works as expected:\\nIn[8]: data['b']\\nOut[8]: 0.5\\nWe can even use noncontiguous or nonsequential indices:\\nIn[9]: data = pd.Series([0.25, 0.5, 0.75, 1.0],\\n                        index=[2, 5, 3, 7])\\n       data\\nOut[9]: 2    0.25\\n        5    0.50\\n        3    0.75\\n        7    1.00\\n        dtype: float64\\nIn[10]: data[5]\\nOut[10]: 0.5\\nSeries as specialized dictionary\\nIn this way, you can think of a Pandas Series a bit like a specialization of a Python\\ndictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary\\nvalues, and a Series is a structure that maps typed keys to a set of typed values. This\\ntyping is important: just as the type-specific compiled code behind a NumPy array\\nmakes it more efficient than a Python list for certain operations, the type information\\nof a Pandas Series makes it much more efficient than Python dictionaries for certain\\noperations.\\nWe can make the Series-as-dictionary analogy even more clear by constructing a\\nSeries object directly from a Python dictionary:\\n100 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 118}, page_content=\"In[11]: population_dict = {'California': 38332521,\\n                           'Texas': 26448193,\\n                           'New York': 19651127,\\n                           'Florida': 19552860,\\n                           'Illinois': 12882135}\\n        population = pd.Series(population_dict)\\n        population\\nOut[11]: California    38332521\\n         Florida       19552860\\n         Illinois      12882135\\n         New York      19651127\\n         Texas         26448193\\n         dtype: int64\\nBy default, a Series will be created where the index is drawn from the sorted keys.\\nFrom here, typical dictionary-style item access can be performed:\\nIn[12]: population['California']\\nOut[12]: 38332521\\nUnlike a dictionary, though, the Series also supports array-style operations such as\\nslicing:\\nIn[13]: population['California':'Illinois']\\nOut[13]: California    38332521\\n         Florida       19552860\\n         Illinois      12882135\\n         dtype: int64\\nWe’ll discuss some of the quirks of Pandas indexing and slicing in “Data Indexing and\\nSelection” on page 107.\\nConstructing Series objects\\nWe’ve already seen a few ways of constructing a Pandas Series from scratch; all of\\nthem are some version of the following:\\n>>> pd.Series(data, index=index)\\nwhere index is an optional argument, and data can be one of many entities.\\nFor example, data can be a list or NumPy array, in which case index defaults to an\\ninteger sequence:\\nIn[14]: pd.Series([2, 4, 6])\\nOut[14]: 0    2\\n         1    4\\n         2    6\\n         dtype: int64\\nIntroducing Pandas Objects \\n| \\n101\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 119}, page_content=\"data can be a scalar, which is repeated to fill the specified index:\\nIn[15]: pd.Series(5, index=[100, 200, 300])\\nOut[15]: 100    5\\n         200    5\\n         300    5\\n         dtype: int64\\ndata can be a dictionary, in which index defaults to the sorted dictionary keys:\\nIn[16]: pd.Series({2:'a', 1:'b', 3:'c'})\\nOut[16]: 1    b\\n         2    a\\n         3    c\\n         dtype: object\\nIn each case, the index can be explicitly set if a different result is preferred:\\nIn[17]: pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])\\nOut[17]: 3    c\\n         2    a\\n         dtype: object\\nNotice that in this case, the Series is populated only with the explicitly identified\\nkeys.\\nThe Pandas DataFrame Object\\nThe next fundamental structure in Pandas is the DataFrame. Like the Series object\\ndiscussed in the previous section, the DataFrame can be thought of either as a gener‐\\nalization of a NumPy array, or as a specialization of a Python dictionary. We’ll now\\ntake a look at each of these perspectives.\\nDataFrame as a generalized NumPy array\\nIf a Series is an analog of a one-dimensional array with flexible indices, a DataFrame\\nis an analog of a two-dimensional array with both flexible row indices and flexible\\ncolumn names. Just as you might think of a two-dimensional array as an ordered\\nsequence of aligned one-dimensional columns, you can think of a DataFrame as a\\nsequence of aligned Series objects. Here, by “aligned” we mean that they share the\\nsame index.\\nTo demonstrate this, let’s first construct a new Series listing the area of each of the\\nfive states discussed in the previous section:\\nIn[18]:\\narea_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\\n             'Florida': 170312, 'Illinois': 149995}\\n102 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 120}, page_content=\"area = pd.Series(area_dict)\\narea\\nOut[18]: California    423967\\n         Florida       170312\\n         Illinois      149995\\n         New York      141297\\n         Texas         695662\\n         dtype: int64\\nNow that we have this along with the population Series from before, we can use a\\ndictionary to construct a single two-dimensional object containing this information:\\nIn[19]: states = pd.DataFrame({'population': population,\\n                               'area': area})\\n        states\\nOut[19]:             area      population\\n         California  423967    38332521\\n         Florida     170312    19552860\\n         Illinois    149995    12882135\\n         New York    141297    19651127\\n         Texas       695662    26448193\\nLike the Series object, the DataFrame has an index attribute that gives access to the\\nindex labels:\\nIn[20]: states.index\\nOut[20]:\\nIndex(['California', 'Florida', 'Illinois', 'New York', 'Texas'], dtype='object')\\nAdditionally, the DataFrame has a columns attribute, which is an Index object holding\\nthe column labels:\\nIn[21]: states.columns\\nOut[21]: Index(['area', 'population'], dtype='object')\\nThus the DataFrame can be thought of as a generalization of a two-dimensional\\nNumPy array, where both the rows and columns have a generalized index for access‐\\ning the data.\\nDataFrame as specialized dictionary\\nSimilarly, we can also think of a DataFrame as a specialization of a dictionary. Where\\na dictionary maps a key to a value, a DataFrame maps a column name to a Series of\\ncolumn data. For example, asking for the 'area' attribute returns the Series object\\ncontaining the areas we saw earlier:\\nIn[22]: states['area']\\nOut[22]: California    423967\\n         Florida       170312\\nIntroducing Pandas Objects \\n| \\n103\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 121}, page_content=\"Illinois      149995\\n         New York      141297\\n         Texas         695662\\n         Name: area, dtype: int64\\nNotice the potential point of confusion here: in a two-dimensional NumPy array,\\ndata[0] will return the first row. For a DataFrame, data['col0'] will return the first\\ncolumn. Because of this, it is probably better to think about DataFrames as generalized\\ndictionaries rather than generalized arrays, though both ways of looking at the situa‐\\ntion can be useful. We’ll explore more flexible means of indexing DataFrames in “Data\\nIndexing and Selection” on page 107.\\nConstructing DataFrame objects\\nA Pandas DataFrame can be constructed in a variety of ways. Here we’ll give several\\nexamples.\\nFrom a single Series object.    A DataFrame is a collection of Series objects, and a single-\\ncolumn DataFrame can be constructed from a single Series:\\nIn[23]: pd.DataFrame(population, columns=['population'])\\nOut[23]:               population\\n         California    38332521\\n         Florida       19552860\\n         Illinois      12882135\\n         New York      19651127\\n         Texas         26448193\\nFrom a list of dicts.    Any list of dictionaries can be made into a DataFrame. We’ll use a\\nsimple list comprehension to create some data:\\nIn[24]: data = [{'a': i, 'b': 2 * i}\\n                for i in range(3)]\\n        pd.DataFrame(data)\\nOut[24]:    a  b\\n         0  0  0\\n         1  1  2\\n         2  2  4\\nEven if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e.,\\n“not a number”) values:\\nIn[25]: pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\\nOut[25]:    a    b  c\\n         0  1.0  2  NaN\\n         1  NaN  3  4.0\\n104 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 122}, page_content=\"From a dictionary of Series objects.    As we saw before, a DataFrame can be constructed\\nfrom a dictionary of Series objects as well:\\nIn[26]: pd.DataFrame({'population': population,\\n                      'area': area})\\nOut[26]:             area      population\\n         California  423967    38332521\\n         Florida     170312    19552860\\n         Illinois    149995    12882135\\n         New York    141297    19651127\\n         Texas       695662    26448193\\nFrom a two-dimensional NumPy array.    Given a two-dimensional array of data, we can\\ncreate a DataFrame with any specified column and index names. If omitted, an integer\\nindex will be used for each:\\nIn[27]: pd.DataFrame(np.random.rand(3, 2),\\n                     columns=['foo', 'bar'],\\n                     index=['a', 'b', 'c'])\\nOut[27]:    foo       bar\\n         a  0.865257  0.213169\\n         b  0.442759  0.108267\\n         c  0.047110  0.905718\\nFrom a NumPy structured array.    We covered structured arrays in “Structured Data:\\nNumPy’s Structured Arrays” on page 92. A Pandas DataFrame operates much like a\\nstructured array, and can be created directly from one:\\nIn[28]: A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\\n        A\\nOut[28]: array([(0, 0.0), (0, 0.0), (0, 0.0)],\\n               dtype=[('A', '<i8'), ('B', '<f8')])\\nIn[29]: pd.DataFrame(A)\\nOut[29]:    A  B\\n         0  0  0.0\\n         1  0  0.0\\n         2  0  0.0\\nThe Pandas Index Object\\nWe have seen here that both the Series and DataFrame objects contain an explicit\\nindex that lets you reference and modify data. This Index object is an interesting\\nstructure in itself, and it can be thought of either as an immutable array or as an\\nordered set (technically a multiset, as Index objects may contain repeated values).\\nThose views have some interesting consequences in the operations available on Index\\nobjects. As a simple example, let’s construct an Index from a list of integers:\\nIntroducing Pandas Objects \\n| \\n105\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 123}, page_content='In[30]: ind = pd.Index([2, 3, 5, 7, 11])\\n        ind\\nOut[30]: Int64Index([2, 3, 5, 7, 11], dtype=\\'int64\\')\\nIndex as immutable array\\nThe Index object in many ways operates like an array. For example, we can use stan‐\\ndard Python indexing notation to retrieve values or slices:\\nIn[31]: ind[1]\\nOut[31]: 3\\nIn[32]: ind[::2]\\nOut[32]: Int64Index([2, 5, 11], dtype=\\'int64\\')\\nIndex objects also have many of the attributes familiar from NumPy arrays:\\nIn[33]: print(ind.size, ind.shape, ind.ndim, ind.dtype)\\n5 (5,) 1 int64\\nOne difference between Index objects and NumPy arrays is that indices are immuta‐\\nble—that is, they cannot be modified via the normal means:\\nIn[34]: ind[1] = 0\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-34-40e631c82e8a> in <module>()\\n----> 1 ind[1] = 0\\n/Users/jakevdp/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py ...\\n   1243\\n   1244     def __setitem__(self, key, value):\\n-> 1245         raise TypeError(\"Index does not support mutable operations\")\\n   1246\\n   1247     def __getitem__(self, key):\\nTypeError: Index does not support mutable operations\\nThis immutability makes it safer to share indices between multiple DataFrames and\\narrays, without the potential for side effects from inadvertent index modification.\\nIndex as ordered set\\nPandas objects are designed to facilitate operations such as joins across datasets,\\nwhich depend on many aspects of set arithmetic. The Index object follows many of\\n106 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 124}, page_content=\"the conventions used by Python’s built-in set data structure, so that unions, intersec‐\\ntions, differences, and other combinations can be computed in a familiar way:\\nIn[35]: indA = pd.Index([1, 3, 5, 7, 9])\\n        indB = pd.Index([2, 3, 5, 7, 11])\\nIn[36]: indA & indB  # intersection\\nOut[36]: Int64Index([3, 5, 7], dtype='int64')\\nIn[37]: indA | indB  # union\\nOut[37]: Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64')\\nIn[38]: indA ^ indB  # symmetric difference\\nOut[38]: Int64Index([1, 2, 9, 11], dtype='int64')\\nThese operations may also be accessed via object methods—for example, indA.inter\\nsection(indB).\\nData Indexing and Selection\\nIn Chapter 2, we looked in detail at methods and tools to access, set, and modify val‐\\nues in NumPy arrays. These included indexing (e.g., arr[2, 1]), slicing (e.g., arr[:,\\n1:5]), masking (e.g., arr[arr > 0]), fancy indexing (e.g., arr[0, [1, 5]]), and\\ncombinations thereof (e.g., arr[:, [1, 5]]). Here we’ll look at similar means of\\naccessing and modifying values in Pandas Series and DataFrame objects. If you have\\nused the NumPy patterns, the corresponding patterns in Pandas will feel very famil‐\\niar, though there are a few quirks to be aware of.\\nWe’ll start with the simple case of the one-dimensional Series object, and then move\\non to the more complicated two-dimensional DataFrame object.\\nData Selection in Series\\nAs we saw in the previous section, a Series object acts in many ways like a one-\\ndimensional NumPy array, and in many ways like a standard Python dictionary. If we\\nkeep these two overlapping analogies in mind, it will help us to understand the pat‐\\nterns of data indexing and selection in these arrays.\\nSeries as dictionary\\nLike a dictionary, the Series object provides a mapping from a collection of keys to a\\ncollection of values:\\nIn[1]: import pandas as pd\\n       data = pd.Series([0.25, 0.5, 0.75, 1.0],\\n                        index=['a', 'b', 'c', 'd'])\\n       data\\nData Indexing and Selection \\n| \\n107\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 125}, page_content=\"Out[1]: a    0.25\\n        b    0.50\\n        c    0.75\\n        d    1.00\\n        dtype: float64\\nIn[2]: data['b']\\nOut[2]: 0.5\\nWe can also use dictionary-like Python expressions and methods to examine the\\nkeys/indices and values:\\nIn[3]: 'a' in data\\nOut[3]: True\\nIn[4]: data.keys()\\nOut[4]: Index(['a', 'b', 'c', 'd'], dtype='object')\\nIn[5]: list(data.items())\\nOut[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]\\nSeries objects can even be modified with a dictionary-like syntax. Just as you can\\nextend a dictionary by assigning to a new key, you can extend a Series by assigning\\nto a new index value:\\nIn[6]: data['e'] = 1.25\\n       data\\nOut[6]: a    0.25\\n        b    0.50\\n        c    0.75\\n        d    1.00\\n        e    1.25\\n        dtype: float64\\nThis easy mutability of the objects is a convenient feature: under the hood, Pandas is\\nmaking decisions about memory layout and data copying that might need to take\\nplace; the user generally does not need to worry about these issues.\\nSeries as one-dimensional array\\nA Series builds on this dictionary-like interface and provides array-style item selec‐\\ntion via the same basic mechanisms as NumPy arrays—that is, slices, masking, and\\nfancy indexing. Examples of these are as follows:\\nIn[7]: # slicing by explicit index\\n       data['a':'c']\\nOut[7]: a    0.25\\n        b    0.50\\n        c    0.75\\n        dtype: float64\\n108 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 126}, page_content=\"In[8]: # slicing by implicit integer index\\n       data[0:2]\\nOut[8]: a    0.25\\n        b    0.50\\n        dtype: float64\\nIn[9]: # masking\\n       data[(data > 0.3) & (data < 0.8)]\\nOut[9]: b    0.50\\n        c    0.75\\n        dtype: float64\\nIn[10]: # fancy indexing\\n        data[['a', 'e']]\\nOut[10]: a    0.25\\n         e    1.25\\n         dtype: float64\\nAmong these, slicing may be the source of the most confusion. Notice that when you\\nare slicing with an explicit index (i.e., data['a':'c']), the final index is included in\\nthe slice, while when you’re slicing with an implicit index (i.e., data[0:2]), the final\\nindex is excluded from the slice.\\nIndexers: loc, iloc, and ix\\nThese slicing and indexing conventions can be a source of confusion. For example, if\\nyour Series has an explicit integer index, an indexing operation such as data[1] will\\nuse the explicit indices, while a slicing operation like data[1:3] will use the implicit\\nPython-style index.\\nIn[11]: data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\\n        data\\nOut[11]: 1    a\\n         3    b\\n         5    c\\n         dtype: object\\nIn[12]: # explicit index when indexing\\n        data[1]\\nOut[12]: 'a'\\nIn[13]: # implicit index when slicing\\n        data[1:3]\\nOut[13]: 3    b\\n         5    c\\n         dtype: object\\nBecause of this potential confusion in the case of integer indexes, Pandas provides\\nsome special indexer attributes that explicitly expose certain indexing schemes. These\\nData Indexing and Selection \\n| \\n109\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 127}, page_content=\"are not functional methods, but attributes that expose a particular slicing interface to\\nthe data in the Series.\\nFirst, the loc attribute allows indexing and slicing that always references the explicit\\nindex:\\nIn[14]: data.loc[1]\\nOut[14]: 'a'\\nIn[15]: data.loc[1:3]\\nOut[15]: 1    a\\n         3    b\\n         dtype: object\\nThe iloc attribute allows indexing and slicing that always references the implicit\\nPython-style index:\\nIn[16]: data.iloc[1]\\nOut[16]: 'b'\\nIn[17]: data.iloc[1:3]\\nOut[17]: 3    b\\n         5    c\\n         dtype: object\\nA third indexing attribute, ix, is a hybrid of the two, and for Series objects is equiva‐\\nlent to standard []-based indexing. The purpose of the ix indexer will become more\\napparent in the context of DataFrame objects, which we will discuss in a moment.\\nOne guiding principle of Python code is that “explicit is better than implicit.” The\\nexplicit nature of loc and iloc make them very useful in maintaining clean and read‐\\nable code; especially in the case of integer indexes, I recommend using these both to\\nmake code easier to read and understand, and to prevent subtle bugs due to the\\nmixed indexing/slicing convention.\\nData Selection in DataFrame\\nRecall that a DataFrame acts in many ways like a two-dimensional or structured array,\\nand in other ways like a dictionary of Series structures sharing the same index.\\nThese analogies can be helpful to keep in mind as we explore data selection within\\nthis structure.\\nDataFrame as a dictionary\\nThe first analogy we will consider is the DataFrame as a dictionary of related Series\\nobjects. Let’s return to our example of areas and populations of states:\\n110 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 128}, page_content='In[18]: area = pd.Series({\\'California\\': 423967, \\'Texas\\': 695662,\\n                          \\'New York\\': 141297, \\'Florida\\': 170312,\\n                          \\'Illinois\\': 149995})\\n        pop = pd.Series({\\'California\\': 38332521, \\'Texas\\': 26448193,\\n                         \\'New York\\': 19651127, \\'Florida\\': 19552860,\\n                         \\'Illinois\\': 12882135})\\n        data = pd.DataFrame({\\'area\\':area, \\'pop\\':pop})\\n        data\\nOut[18]:             area    pop\\n         California  423967  38332521\\n         Florida     170312  19552860\\n         Illinois    149995  12882135\\n         New York    141297  19651127\\n         Texas       695662  26448193\\nThe individual Series that make up the columns of the DataFrame can be accessed\\nvia dictionary-style indexing of the column name:\\nIn[19]: data[\\'area\\']\\nOut[19]: California    423967\\n         Florida       170312\\n         Illinois      149995\\n         New York      141297\\n         Texas         695662\\n         Name: area, dtype: int64\\nEquivalently, we can use attribute-style access with column names that are strings:\\nIn[20]: data.area\\nOut[20]: California    423967\\n         Florida       170312\\n         Illinois      149995\\n         New York      141297\\n         Texas         695662\\n         Name: area, dtype: int64\\nThis attribute-style column access actually accesses the exact same object as the\\ndictionary-style access:\\nIn[21]: data.area is data[\\'area\\']\\nOut[21]: True\\nThough this is a useful shorthand, keep in mind that it does not work for all cases!\\nFor example, if the column names are not strings, or if the column names conflict\\nwith methods of the DataFrame, this attribute-style access is not possible. For exam‐\\nple, the DataFrame has a pop() method, so data.pop will point to this rather than the\\n\"pop\" column:\\nIn[22]: data.pop is data[\\'pop\\']\\nOut[22]: False\\nData Indexing and Selection \\n| \\n111'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 129}, page_content=\"In particular, you should avoid the temptation to try column assignment via attribute\\n(i.e., use data['pop'] = z rather than data.pop = z).\\nLike with the Series objects discussed earlier, this dictionary-style syntax can also be\\nused to modify the object, in this case to add a new column:\\nIn[23]: data['density'] = data['pop'] / data['area']\\n        data\\nOut[23]:             area    pop       density\\n         California  423967  38332521   90.413926\\n         Florida     170312  19552860  114.806121\\n         Illinois    149995  12882135   85.883763\\n         New York    141297  19651127  139.076746\\n         Texas       695662  26448193   38.018740\\nThis shows a preview of the straightforward syntax of element-by-element arithmetic\\nbetween Series objects; we’ll dig into this further in “Operating on Data in Pandas”\\non page 115.\\nDataFrame as two-dimensional array\\nAs mentioned previously, we can also view the DataFrame as an enhanced two-\\ndimensional array. We can examine the raw underlying data array using the values\\nattribute:\\nIn[24]: data.values\\nOut[24]: array([[  4.23967000e+05,   3.83325210e+07,   9.04139261e+01],\\n                [  1.70312000e+05,   1.95528600e+07,   1.14806121e+02],\\n                [  1.49995000e+05,   1.28821350e+07,   8.58837628e+01],\\n                [  1.41297000e+05,   1.96511270e+07,   1.39076746e+02],\\n                [  6.95662000e+05,   2.64481930e+07,   3.80187404e+01]])\\nWith this picture in mind, we can do many familiar array-like observations on the\\nDataFrame itself. For example, we can transpose the full DataFrame to swap rows and\\ncolumns:\\nIn[25]: data.T\\nOut[25]:\\n         California    Florida       Illinois      New York      Texas\\narea     4.239670e+05  1.703120e+05  1.499950e+05  1.412970e+05  6.956620e+05\\npop      3.833252e+07  1.955286e+07  1.288214e+07  1.965113e+07  2.644819e+07\\ndensity  9.041393e+01  1.148061e+02  8.588376e+01  1.390767e+02  3.801874e+01\\nWhen it comes to indexing of DataFrame objects, however, it is clear that the\\ndictionary-style indexing of columns precludes our ability to simply treat it as a\\nNumPy array. In particular, passing a single index to an array accesses a row:\\nIn[26]: data.values[0]\\nOut[26]: array([  4.23967000e+05,   3.83325210e+07,   9.04139261e+01])\\n112 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 130}, page_content=\"and passing a single “index” to a DataFrame accesses a column:\\nIn[27]: data['area']\\nOut[27]: California    423967\\n         Florida       170312\\n         Illinois      149995\\n         New York      141297\\n         Texas         695662\\n         Name: area, dtype: int64\\nThus for array-style indexing, we need another convention. Here Pandas again uses\\nthe loc, iloc, and ix indexers mentioned earlier. Using the iloc indexer, we can\\nindex the underlying array as if it is a simple NumPy array (using the implicit\\nPython-style index), but the DataFrame index and column labels are maintained in\\nthe result:\\nIn[28]: data.iloc[:3, :2]\\nOut[28]:             area    pop\\n         California  423967  38332521\\n         Florida     170312  19552860\\n         Illinois    149995  12882135\\nIn[29]: data.loc[:'Illinois', :'pop']\\nOut[29]:             area    pop\\n         California  423967  38332521\\n         Florida     170312  19552860\\n         Illinois    149995  12882135\\nThe ix indexer allows a hybrid of these two approaches:\\nIn[30]: data.ix[:3, :'pop']\\nOut[30]:             area    pop\\n         California  423967  38332521\\n         Florida     170312  19552860\\n         Illinois    149995  12882135\\nKeep in mind that for integer indices, the ix indexer is subject to the same potential\\nsources of confusion as discussed for integer-indexed Series objects.\\nAny of the familiar NumPy-style data access patterns can be used within these index‐\\ners. For example, in the loc indexer we can combine masking and fancy indexing as\\nin the following:\\nIn[31]: data.loc[data.density > 100, ['pop', 'density']]\\nOut[31]:           pop       density\\n         Florida   19552860  114.806121\\n         New York  19651127  139.076746\\nData Indexing and Selection \\n| \\n113\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 131}, page_content=\"Any of these indexing conventions may also be used to set or modify values; this is\\ndone in the standard way that you might be accustomed to from working with\\nNumPy:\\nIn[32]: data.iloc[0, 2] = 90\\n        data\\nOut[32]:             area    pop       density\\n         California  423967  38332521   90.000000\\n         Florida     170312  19552860  114.806121\\n         Illinois    149995  12882135   85.883763\\n         New York    141297  19651127  139.076746\\n         Texas       695662  26448193   38.018740\\nTo build up your fluency in Pandas data manipulation, I suggest spending some time\\nwith a simple DataFrame and exploring the types of indexing, slicing, masking, and\\nfancy indexing that are allowed by these various indexing approaches.\\nAdditional indexing conventions\\nThere are a couple extra indexing conventions that might seem at odds with the pre‐\\nceding discussion, but nevertheless can be very useful in practice. First, while index‐\\ning refers to columns, slicing refers to rows:\\nIn[33]: data['Florida':'Illinois']\\nOut[33]:           area    pop       density\\n         Florida   170312  19552860  114.806121\\n         Illinois  149995  12882135   85.883763\\nSuch slices can also refer to rows by number rather than by index:\\nIn[34]: data[1:3]\\nOut[34]:           area    pop       density\\n         Florida   170312  19552860  114.806121\\n         Illinois  149995  12882135   85.883763\\nSimilarly, direct masking operations are also interpreted row-wise rather than\\ncolumn-wise:\\nIn[35]: data[data.density > 100]\\nOut[35]:           area    pop       density\\n         Florida   170312  19552860  114.806121\\n         New York  141297  19651127  139.076746\\nThese two conventions are syntactically similar to those on a NumPy array, and while\\nthese may not precisely fit the mold of the Pandas conventions, they are nevertheless\\nquite useful in practice.\\n114 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 132}, page_content=\"Operating on Data in Pandas\\nOne of the essential pieces of NumPy is the ability to perform quick element-wise\\noperations, both with basic arithmetic (addition, subtraction, multiplication, etc.) and\\nwith more sophisticated operations (trigonometric functions, exponential and loga‐\\nrithmic functions, etc.). Pandas inherits much of this functionality from NumPy, and\\nthe ufuncs that we introduced in “Computation on NumPy Arrays: Universal Func‐\\ntions” on page 50 are key to this.\\nPandas includes a couple useful twists, however: for unary operations like negation\\nand trigonometric functions, these ufuncs will preserve index and column labels in the\\noutput, and for binary operations such as addition and multiplication, Pandas will\\nautomatically align indices when passing the objects to the ufunc. This means that\\nkeeping the context of data and combining data from different sources—both poten‐\\ntially error-prone tasks with raw NumPy arrays—become essentially foolproof ones\\nwith Pandas. We will additionally see that there are well-defined operations between\\none-dimensional Series structures and two-dimensional DataFrame structures.\\nUfuncs: Index Preservation\\nBecause Pandas is designed to work with NumPy, any NumPy ufunc will work on\\nPandas Series and DataFrame objects. Let’s start by defining a simple Series and\\nDataFrame on which to demonstrate this:\\nIn[1]: import pandas as pd\\n       import numpy as np\\nIn[2]: rng = np.random.RandomState(42)\\n       ser = pd.Series(rng.randint(0, 10, 4))\\n       ser\\nOut[2]: 0    6\\n        1    3\\n        2    7\\n        3    4\\n        dtype: int64\\nIn[3]: df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\\n                         columns=['A', 'B', 'C', 'D'])\\n       df\\nOut[3]:    A  B  C  D\\n        0  6  9  2  6\\n        1  7  4  3  7\\n        2  7  2  5  4\\nIf we apply a NumPy ufunc on either of these objects, the result will be another Pan‐\\ndas object with the indices preserved:\\nIn[4]: np.exp(ser)\\nOperating on Data in Pandas \\n| \\n115\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 133}, page_content=\"Out[4]: 0     403.428793\\n        1      20.085537\\n        2    1096.633158\\n        3      54.598150\\n        dtype: float64\\nOr, for a slightly more complex calculation:\\nIn[5]: np.sin(df * np.pi / 4)\\nOut[5]:           A             B         C             D\\n        0 -1.000000  7.071068e-01  1.000000 -1.000000e+00\\n        1 -0.707107  1.224647e-16  0.707107 -7.071068e-01\\n        2 -0.707107  1.000000e+00 -0.707107  1.224647e-16\\nAny of the ufuncs discussed in “Computation on NumPy Arrays: Universal Func‐\\ntions” on page 50 can be used in a similar manner.\\nUFuncs: Index Alignment\\nFor binary operations on two Series or DataFrame objects, Pandas will align indices\\nin the process of performing the operation. This is very convenient when you are\\nworking with incomplete data, as we’ll see in some of the examples that follow.\\nIndex alignment in Series\\nAs an example, suppose we are combining two different data sources, and find only\\nthe top three US states by area and the top three US states by population:\\nIn[6]: area = pd.Series({'Alaska': 1723337, 'Texas': 695662,\\n                         'California': 423967}, name='area')\\n       population = pd.Series({'California': 38332521, 'Texas': 26448193,\\n                               'New York': 19651127}, name='population')\\nLet’s see what happens when we divide these to compute the population density:\\nIn[7]: population / area\\nOut[7]: Alaska              NaN\\n        California    90.413926\\n        New York            NaN\\n        Texas         38.018740\\n        dtype: float64\\nThe resulting array contains the union of indices of the two input arrays, which we\\ncould determine using standard Python set arithmetic on these indices:\\nIn[8]: area.index | population.index\\nOut[8]: Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')\\nAny item for which one or the other does not have an entry is marked with NaN, or\\n“Not a Number,” which is how Pandas marks missing data (see further discussion of\\nmissing data in “Handling Missing Data” on page 119). This index matching is imple‐\\n116 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 134}, page_content=\"mented this way for any of Python’s built-in arithmetic expressions; any missing val‐\\nues are filled in with NaN by default:\\nIn[9]: A = pd.Series([2, 4, 6], index=[0, 1, 2])\\n       B = pd.Series([1, 3, 5], index=[1, 2, 3])\\n       A + B\\nOut[9]: 0    NaN\\n        1    5.0\\n        2    9.0\\n        3    NaN\\n        dtype: float64\\nIf using NaN values is not the desired behavior, we can modify the fill value using\\nappropriate object methods in place of the operators. For example, calling A.add(B)\\nis equivalent to calling A + B, but allows optional explicit specification of the fill value\\nfor any elements in A or B that might be missing:\\nIn[10]: A.add(B, fill_value=0)\\nOut[10]: 0    2.0\\n         1    5.0\\n         2    9.0\\n         3    5.0\\n         dtype: float64\\nIndex alignment in DataFrame\\nA similar type of alignment takes place for both columns and indices when you are\\nperforming operations on DataFrames:\\nIn[11]: A = pd.DataFrame(rng.randint(0, 20, (2, 2)),\\n                         columns=list('AB'))\\n        A\\nOut[11]:    A   B\\n         0  1  11\\n         1  5   1\\nIn[12]: B = pd.DataFrame(rng.randint(0, 10, (3, 3)),\\n                         columns=list('BAC'))\\n        B\\nOut[12]:    B  A  C\\n         0  4  0  9\\n         1  5  8  0\\n         2  9  2  6\\nIn[13]: A + B\\nOut[13]:       A     B   C\\n         0   1.0  15.0 NaN\\n         1  13.0   6.0 NaN\\n         2   NaN   NaN NaN\\nOperating on Data in Pandas \\n| \\n117\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 135}, page_content='Notice that indices are aligned correctly irrespective of their order in the two objects,\\nand indices in the result are sorted. As was the case with Series, we can use the asso‐\\nciated object’s arithmetic method and pass any desired fill_value to be used in place\\nof missing entries. Here we’ll fill with the mean of all values in A (which we compute\\nby first stacking the rows of A):\\nIn[14]: fill = A.stack().mean()\\n        A.add(B, fill_value=fill)\\nOut[14]:       A     B     C\\n         0   1.0  15.0  13.5\\n         1  13.0   6.0   4.5\\n         2   6.5  13.5  10.5\\nTable 3-1 lists Python operators and their equivalent Pandas object methods.\\nTable 3-1. Mapping between Python operators and Pandas methods\\nPython operator\\nPandas method(s)\\n+\\nadd()\\n-\\nsub(), subtract()\\n*\\nmul(), multiply()\\n/\\ntruediv(), div(), divide()\\n//\\nfloordiv()\\n%\\nmod()\\n**\\npow()\\nUfuncs: Operations Between DataFrame and Series\\nWhen you are performing operations between a DataFrame and a Series, the index\\nand column alignment is similarly maintained. Operations between a DataFrame and\\na Series are similar to operations between a two-dimensional and one-dimensional\\nNumPy array. Consider one common operation, where we find the difference of a\\ntwo-dimensional array and one of its rows:\\nIn[15]: A = rng.randint(10, size=(3, 4))\\n        A\\nOut[15]: array([[3, 8, 2, 4],\\n                [2, 6, 4, 8],\\n                [6, 1, 3, 8]])\\nIn[16]: A - A[0]\\nOut[16]: array([[ 0,  0,  0,  0],\\n                [-1, -2,  2,  4],\\n                [ 3, -7,  1,  4]])\\n118 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 136}, page_content=\"According to NumPy’s broadcasting rules (see “Computation on Arrays: Broadcast‐\\ning” on page 63), subtraction between a two-dimensional array and one of its rows is\\napplied row-wise.\\nIn Pandas, the convention similarly operates row-wise by default:\\nIn[17]: df = pd.DataFrame(A, columns=list('QRST'))\\n        df - df.iloc[0]\\nOut[17]:    Q  R  S  T\\n         0  0  0  0  0\\n         1 -1 -2  2  4\\n         2  3 -7  1  4\\nIf you would instead like to operate column-wise, you can use the object methods\\nmentioned earlier, while specifying the axis keyword:\\nIn[18]: df.subtract(df['R'], axis=0)\\nOut[18]:    Q  R  S  T\\n         0 -5  0 -6 -4\\n         1 -4  0 -2  2\\n         2  5  0  2  7\\nNote that these DataFrame/Series operations, like the operations discussed before,\\nwill automatically align indices between the two elements:\\nIn[19]: halfrow = df.iloc[0, ::2]\\n        halfrow\\nOut[19]: Q    3\\n         S    2\\n         Name: 0, dtype: int64\\nIn[20]: df - halfrow\\nOut[20]:      Q   R    S   T\\n         0  0.0 NaN  0.0 NaN\\n         1 -1.0 NaN  2.0 NaN\\n         2  3.0 NaN  1.0 NaN\\nThis preservation and alignment of indices and columns means that operations on\\ndata in Pandas will always maintain the data context, which prevents the types of silly\\nerrors that might come up when you are working with heterogeneous and/or mis‐\\naligned data in raw NumPy arrays.\\nHandling Missing Data\\nThe difference between data found in many tutorials and data in the real world is that\\nreal-world data is rarely clean and homogeneous. In particular, many interesting\\ndatasets will have some amount of data missing. To make matters even more compli‐\\ncated, different data sources may indicate missing data in different ways.\\nHandling Missing Data \\n| \\n119\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 137}, page_content='In this section, we will discuss some general considerations for missing data, discuss\\nhow Pandas chooses to represent it, and demonstrate some built-in Pandas tools for\\nhandling missing data in Python. Here and throughout the book, we’ll refer to miss‐\\ning data in general as null, NaN, or NA values.\\nTrade-Offs in Missing Data Conventions\\nA number of schemes have been developed to indicate the presence of missing data in\\na table or DataFrame. Generally, they revolve around one of two strategies: using a\\nmask that globally indicates missing values, or choosing a sentinel value that indicates\\na missing entry.\\nIn the masking approach, the mask might be an entirely separate Boolean array, or it\\nmay involve appropriation of one bit in the data representation to locally indicate the\\nnull status of a value.\\nIn the sentinel approach, the sentinel value could be some data-specific convention,\\nsuch as indicating a missing integer value with –9999 or some rare bit pattern, or it\\ncould be a more global convention, such as indicating a missing floating-point value\\nwith NaN (Not a Number), a special value which is part of the IEEE floating-point\\nspecification.\\nNone of these approaches is without trade-offs: use of a separate mask array requires\\nallocation of an additional Boolean array, which adds overhead in both storage and\\ncomputation. A sentinel value reduces the range of valid values that can be repre‐\\nsented, and may require extra (often non-optimized) logic in CPU and GPU arith‐\\nmetic. Common special values like NaN are not available for all data types.\\nAs in most cases where no universally optimal choice exists, different languages and\\nsystems use different conventions. For example, the R language uses reserved bit pat‐\\nterns within each data type as sentinel values indicating missing data, while the SciDB\\nsystem uses an extra byte attached to every cell to indicate a NA state.\\nMissing Data in Pandas\\nThe way in which Pandas handles missing values is constrained by its reliance on the\\nNumPy package, which does not have a built-in notion of NA values for non-\\nfloating-point data types.\\nPandas could have followed R’s lead in specifying bit patterns for each individual data\\ntype to indicate nullness, but this approach turns out to be rather unwieldy. While R\\ncontains four basic data types, NumPy supports far more than this: for example,\\nwhile R has a single integer type, NumPy supports fourteen basic integer types once\\nyou account for available precisions, signedness, and endianness of the encoding.\\nReserving a specific bit pattern in all available NumPy types would lead to an\\nunwieldy amount of overhead in special-casing various operations for various types,\\n120 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 138}, page_content='likely even requiring a new fork of the NumPy package. Further, for the smaller data\\ntypes (such as 8-bit integers), sacrificing a bit to use as a mask will significantly\\nreduce the range of values it can represent.\\nNumPy does have support for masked arrays—that is, arrays that have a separate\\nBoolean mask array attached for marking data as “good” or “bad.” Pandas could have\\nderived from this, but the overhead in both storage, computation, and code mainte‐\\nnance makes that an unattractive choice.\\nWith these constraints in mind, Pandas chose to use sentinels for missing data, and\\nfurther chose to use two already-existing Python null values: the special floating-\\npoint NaN value, and the Python None object. This choice has some side effects, as we\\nwill see, but in practice ends up being a good compromise in most cases of interest.\\nNone: Pythonic missing data\\nThe first sentinel value used by Pandas is None, a Python singleton object that is often\\nused for missing data in Python code. Because None is a Python object, it cannot be\\nused in any arbitrary NumPy/Pandas array, but only in arrays with data type\\n\\'object\\' (i.e., arrays of Python objects):\\nIn[1]: import numpy as np\\n       import pandas as pd\\nIn[2]: vals1 = np.array([1, None, 3, 4])\\n       vals1\\nOut[2]: array([1, None, 3, 4], dtype=object)\\nThis dtype=object means that the best common type representation NumPy could\\ninfer for the contents of the array is that they are Python objects. While this kind of\\nobject array is useful for some purposes, any operations on the data will be done at\\nthe Python level, with much more overhead than the typically fast operations seen for\\narrays with native types:\\nIn[3]: for dtype in [\\'object\\', \\'int\\']:\\n           print(\"dtype =\", dtype)\\n           %timeit np.arange(1E6, dtype=dtype).sum()\\n           print()\\ndtype = object\\n10 loops, best of 3: 78.2 ms per loop\\ndtype = int\\n100 loops, best of 3: 3.06 ms per loop\\nThe use of Python objects in an array also means that if you perform aggregations\\nlike sum() or min() across an array with a None value, you will generally get an error:\\nHandling Missing Data \\n| \\n121'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 139}, page_content=\"In[4]: vals1.sum()\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-4-749fd8ae6030> in <module>()\\n----> 1 vals1.sum()\\n/Users/jakevdp/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py ...\\n     30\\n     31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):\\n---> 32     return umr_sum(a, axis, dtype, out, keepdims)\\n     33\\n     34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):\\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\\nThis reflects the fact that addition between an integer and None is undefined.\\nNaN: Missing numerical data\\nThe other missing data representation, NaN (acronym for Not a Number), is different;\\nit is a special floating-point value recognized by all systems that use the standard\\nIEEE floating-point representation:\\nIn[5]: vals2 = np.array([1, np.nan, 3, 4])\\n       vals2.dtype\\nOut[5]: dtype('float64')\\nNotice that NumPy chose a native floating-point type for this array: this means that\\nunlike the object array from before, this array supports fast operations pushed into\\ncompiled code. You should be aware that NaN is a bit like a data virus—it infects any\\nother object it touches. Regardless of the operation, the result of arithmetic with NaN\\nwill be another NaN:\\nIn[6]: 1 + np.nan\\nOut[6]: nan\\nIn[7]: 0 *  np.nan\\nOut[7]: nan\\nNote that this means that aggregates over the values are well defined (i.e., they don’t\\nresult in an error) but not always useful:\\nIn[8]: vals2.sum(), vals2.min(), vals2.max()\\nOut[8]: (nan, nan, nan)\\nNumPy does provide some special aggregations that will ignore these missing values:\\n122 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 140}, page_content='In[9]: np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)\\nOut[9]: (8.0, 1.0, 4.0)\\nKeep in mind that NaN is specifically a floating-point value; there is no equivalent\\nNaN value for integers, strings, or other types.\\nNaN and None in Pandas\\nNaN and None both have their place, and Pandas is built to handle the two of them\\nnearly interchangeably, converting between them where appropriate:\\nIn[10]: pd.Series([1, np.nan, 2, None])\\nOut[10]: 0    1.0\\n         1    NaN\\n         2    2.0\\n         3    NaN\\n         dtype: float64\\nFor types that don’t have an available sentinel value, Pandas automatically type-casts\\nwhen NA values are present. For example, if we set a value in an integer array to\\nnp.nan, it will automatically be upcast to a floating-point type to accommodate the\\nNA:\\nIn[11]: x = pd.Series(range(2), dtype=int)\\n        x\\nOut[11]: 0    0\\n         1    1\\n         dtype: int64\\nIn[12]: x[0] = None\\n        x\\nOut[12]: 0    NaN\\n         1    1.0\\n         dtype: float64\\nNotice that in addition to casting the integer array to floating point, Pandas automati‐\\ncally converts the None to a NaN value. (Be aware that there is a proposal to add a\\nnative integer NA to Pandas in the future; as of this writing, it has not been included.)\\nWhile this type of magic may feel a bit hackish compared to the more unified\\napproach to NA values in domain-specific languages like R, the Pandas sentinel/cast‐\\ning approach works quite well in practice and in my experience only rarely causes\\nissues.\\nTable 3-2 lists the upcasting conventions in Pandas when NA values are introduced.\\nHandling Missing Data \\n| \\n123'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 141}, page_content=\"Table 3-2. Pandas handling of NAs by type\\nTypeclass\\nConversion when storing NAs NA sentinel value\\nfloating\\nNo change\\nnp.nan\\nobject\\nNo change\\nNone or np.nan\\ninteger\\nCast to float64\\nnp.nan\\nboolean\\nCast to object\\nNone or np.nan\\nKeep in mind that in Pandas, string data is always stored with an object dtype.\\nOperating on Null Values\\nAs we have seen, Pandas treats None and NaN as essentially interchangeable for indi‐\\ncating missing or null values. To facilitate this convention, there are several useful\\nmethods for detecting, removing, and replacing null values in Pandas data structures.\\nThey are:\\nisnull()\\nGenerate a Boolean mask indicating missing values\\nnotnull()\\nOpposite of isnull()\\ndropna()\\nReturn a filtered version of the data\\nfillna()\\nReturn a copy of the data with missing values filled or imputed\\nWe will conclude this section with a brief exploration and demonstration of these\\nroutines.\\nDetecting null values\\nPandas data structures have two useful methods for detecting null data: isnull() and\\nnotnull(). Either one will return a Boolean mask over the data. For example:\\nIn[13]: data = pd.Series([1, np.nan, 'hello', None])\\nIn[14]: data.isnull()\\nOut[14]: 0    False\\n         1     True\\n         2    False\\n         3     True\\n         dtype: bool\\nAs mentioned in “Data Indexing and Selection” on page 107, Boolean masks can be\\nused directly as a Series or DataFrame index:\\n124 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 142}, page_content=\"In[15]: data[data.notnull()]\\nOut[15]: 0        1\\n         2    hello\\n         dtype: object\\nThe isnull() and notnull() methods produce similar Boolean results for Data\\nFrames.\\nDropping null values\\nIn addition to the masking used before, there are the convenience methods, dropna()\\n(which removes NA values) and fillna() (which fills in NA values). For a Series,\\nthe result is straightforward:\\nIn[16]: data.dropna()\\nOut[16]: 0        1\\n         2    hello\\n         dtype: object\\nFor a DataFrame, there are more options. Consider the following DataFrame:\\nIn[17]: df = pd.DataFrame([[1,      np.nan, 2],\\n                           [2,      3,      5],\\n                           [np.nan, 4,      6]])\\n        df\\nOut[17]:      0    1  2\\n         0  1.0  NaN  2\\n         1  2.0  3.0  5\\n         2  NaN  4.0  6\\nWe cannot drop single values from a DataFrame; we can only drop full rows or full\\ncolumns. Depending on the application, you might want one or the other, so\\ndropna() gives a number of options for a DataFrame.\\nBy default, dropna() will drop all rows in which any null value is present:\\nIn[18]: df.dropna()\\nOut[18]:      0    1  2\\n         1  2.0  3.0  5\\nAlternatively, you can drop NA values along a different axis; axis=1 drops all col‐\\numns containing a null value:\\nIn[19]: df.dropna(axis='columns')\\nOut[19]:    2\\n         0  2\\n         1  5\\n         2  6\\nHandling Missing Data \\n| \\n125\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 143}, page_content=\"But this drops some good data as well; you might rather be interested in dropping\\nrows or columns with all NA values, or a majority of NA values. This can be specified\\nthrough the how or thresh parameters, which allow fine control of the number of\\nnulls to allow through.\\nThe default is how='any', such that any row or column (depending on the axis key‐\\nword) containing a null value will be dropped. You can also specify how='all', which\\nwill only drop rows/columns that are all null values:\\nIn[20]: df[3] = np.nan\\n        df\\nOut[20]:      0    1  2   3\\n         0  1.0  NaN  2 NaN\\n         1  2.0  3.0  5 NaN\\n         2  NaN  4.0  6 NaN\\nIn[21]: df.dropna(axis='columns', how='all')\\nOut[21]:      0    1  2\\n         0  1.0  NaN  2\\n         1  2.0  3.0  5\\n         2  NaN  4.0  6\\nFor finer-grained control, the thresh parameter lets you specify a minimum number\\nof non-null values for the row/column to be kept:\\nIn[22]: df.dropna(axis='rows', thresh=3)\\nOut[22]:      0    1  2   3\\n         1  2.0  3.0  5 NaN\\nHere the first and last row have been dropped, because they contain only two non-\\nnull values.\\nFilling null values\\nSometimes rather than dropping NA values, you’d rather replace them with a valid\\nvalue. This value might be a single number like zero, or it might be some sort of\\nimputation or interpolation from the good values. You could do this in-place using\\nthe isnull() method as a mask, but because it is such a common operation Pandas\\nprovides the fillna() method, which returns a copy of the array with the null values\\nreplaced.\\nConsider the following Series:\\nIn[23]: data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\\n        data\\nOut[23]: a    1.0\\n         b    NaN\\n         c    2.0\\n         d    NaN\\n126 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 144}, page_content=\"e    3.0\\n         dtype: float64\\nWe can fill NA entries with a single value, such as zero:\\nIn[24]: data.fillna(0)\\nOut[24]: a    1.0\\n         b    0.0\\n         c    2.0\\n         d    0.0\\n         e    3.0\\n         dtype: float64\\nWe can specify a forward-fill to propagate the previous value forward:\\nIn[25]: # forward-fill\\n        data.fillna(method='ffill')\\nOut[25]: a    1.0\\n         b    1.0\\n         c    2.0\\n         d    2.0\\n         e    3.0\\n         dtype: float64\\nOr we can specify a back-fill to propagate the next values backward:\\nIn[26]: # back-fill\\n        data.fillna(method='bfill')\\nOut[26]: a    1.0\\n         b    2.0\\n         c    2.0\\n         d    3.0\\n         e    3.0\\n         dtype: float64\\nFor DataFrames, the options are similar, but we can also specify an axis along which\\nthe fills take place:\\nIn[27]: df\\nOut[27]:      0    1  2   3\\n         0  1.0  NaN  2 NaN\\n         1  2.0  3.0  5 NaN\\n         2  NaN  4.0  6 NaN\\nIn[28]: df.fillna(method='ffill', axis=1)\\nOut[28]:      0    1    2    3\\n         0  1.0  1.0  2.0  2.0\\n         1  2.0  3.0  5.0  5.0\\n         2  NaN  4.0  6.0  6.0\\nNotice that if a previous value is not available during a forward fill, the NA value\\nremains.\\nHandling Missing Data \\n| \\n127\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 145}, page_content=\"Hierarchical Indexing\\nUp to this point we’ve been focused primarily on one-dimensional and two-\\ndimensional data, stored in Pandas Series and DataFrame objects, respectively. Often\\nit is useful to go beyond this and store higher-dimensional data—that is, data indexed\\nby more than one or two keys. While Pandas does provide Panel and Panel4D objects\\nthat natively handle three-dimensional and four-dimensional data (see “Panel Data”\\non page 141), a far more common pattern in practice is to make use of hierarchical\\nindexing (also known as multi-indexing) to incorporate multiple index levels within a\\nsingle index. In this way, higher-dimensional data can be compactly represented\\nwithin the familiar one-dimensional Series and two-dimensional DataFrame objects.\\nIn this section, we’ll explore the direct creation of MultiIndex objects; considerations\\naround indexing, slicing, and computing statistics across multiply indexed data; and\\nuseful routines for converting between simple and hierarchically indexed representa‐\\ntions of your data.\\nWe begin with the standard imports:\\nIn[1]: import pandas as pd\\n       import numpy as np\\nA Multiply Indexed Series\\nLet’s start by considering how we might represent two-dimensional data within a\\none-dimensional Series. For concreteness, we will consider a series of data where\\neach point has a character and numerical key.\\nThe bad way\\nSuppose you would like to track data about states from two different years. Using the\\nPandas tools we’ve already covered, you might be tempted to simply use Python\\ntuples as keys:\\nIn[2]: index = [('California', 2000), ('California', 2010),\\n                ('New York', 2000), ('New York', 2010),\\n                ('Texas', 2000), ('Texas', 2010)]\\n       populations = [33871648, 37253956,\\n                      18976457, 19378102,\\n                      20851820, 25145561]\\n       pop = pd.Series(populations, index=index)\\n       pop\\nOut[2]: (California, 2000)    33871648\\n        (California, 2010)    37253956\\n        (New York, 2000)      18976457\\n        (New York, 2010)      19378102\\n        (Texas, 2000)         20851820\\n128 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 146}, page_content=\"(Texas, 2010)         25145561\\n        dtype: int64\\nWith this indexing scheme, you can straightforwardly index or slice the series based\\non this multiple index:\\nIn[3]: pop[('California', 2010):('Texas', 2000)]\\nOut[3]: (California, 2010)    37253956\\n        (New York, 2000)      18976457\\n        (New York, 2010)      19378102\\n        (Texas, 2000)         20851820\\n        dtype: int64\\nBut the convenience ends there. For example, if you need to select all values from\\n2010, you’ll need to do some messy (and potentially slow) munging to make it\\nhappen:\\nIn[4]: pop[[i for i in pop.index if i[1] == 2010]]\\nOut[4]: (California, 2010)    37253956\\n        (New York, 2010)      19378102\\n        (Texas, 2010)         25145561\\n        dtype: int64\\nThis produces the desired result, but is not as clean (or as efficient for large datasets)\\nas the slicing syntax we’ve grown to love in Pandas.\\nThe better way: Pandas MultiIndex\\nFortunately, Pandas provides a better way. Our tuple-based indexing is essentially a\\nrudimentary multi-index, and the Pandas MultiIndex type gives us the type of opera‐\\ntions we wish to have. We can create a multi-index from the tuples as follows:\\nIn[5]: index = pd.MultiIndex.from_tuples(index)\\n       index\\nOut[5]: MultiIndex(levels=[['California', 'New York', 'Texas'], [2000, 2010]],\\n                   labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\\nNotice that the MultiIndex contains multiple levels of indexing—in this case, the state\\nnames and the years, as well as multiple labels for each data point which encode these\\nlevels.\\nIf we reindex our series with this MultiIndex, we see the hierarchical representation\\nof the data:\\nIn[6]: pop = pop.reindex(index)\\n       pop\\nOut[6]: California  2000    33871648\\n                    2010    37253956\\n        New York    2000    18976457\\n                    2010    19378102\\nHierarchical Indexing \\n| \\n129\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 147}, page_content='Texas       2000    20851820\\n                    2010    25145561\\n        dtype: int64\\nHere the first two columns of the Series representation show the multiple index val‐\\nues, while the third column shows the data. Notice that some entries are missing in\\nthe first column: in this multi-index representation, any blank entry indicates the\\nsame value as the line above it.\\nNow to access all data for which the second index is 2010, we can simply use the Pan‐\\ndas slicing notation:\\nIn[7]: pop[:, 2010]\\nOut[7]: California    37253956\\n        New York      19378102\\n        Texas         25145561\\n        dtype: int64\\nThe result is a singly indexed array with just the keys we’re interested in. This syntax\\nis much more convenient (and the operation is much more efficient!) than the home-\\nspun tuple-based multi-indexing solution that we started with. We’ll now further dis‐\\ncuss this sort of indexing operation on hierarchically indexed data.\\nMultiIndex as extra dimension\\nYou might notice something else here: we could easily have stored the same data\\nusing a simple DataFrame with index and column labels. In fact, Pandas is built with\\nthis equivalence in mind. The unstack() method will quickly convert a multiply-\\nindexed Series into a conventionally indexed DataFrame:\\nIn[8]: pop_df = pop.unstack()\\n       pop_df\\nOut[8]:                 2000      2010\\n        California  33871648  37253956\\n        New York    18976457  19378102\\n        Texas       20851820  25145561\\nNaturally, the stack() method provides the opposite operation:\\nIn[9]: pop_df.stack()\\nOut[9]:  California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nSeeing this, you might wonder why would we would bother with hierarchical index‐\\ning at all. The reason is simple: just as we were able to use multi-indexing to represent\\n130 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 148}, page_content=\"two-dimensional data within a one-dimensional Series, we can also use it to repre‐\\nsent data of three or more dimensions in a Series or DataFrame. Each extra level in a\\nmulti-index represents an extra dimension of data; taking advantage of this property\\ngives us much more flexibility in the types of data we can represent. Concretely, we\\nmight want to add another column of demographic data for each state at each year\\n(say, population under 18); with a MultiIndex this is as easy as adding another col‐\\numn to the DataFrame:\\nIn[10]: pop_df = pd.DataFrame({'total': pop,\\n                               'under18': [9267089, 9284094,\\n                                           4687374, 4318033,\\n                                           5906301, 6879014]})\\n        pop_df\\nOut[10]:                     total  under18\\n         California 2000  33871648  9267089\\n                    2010  37253956  9284094\\n         New York   2000  18976457  4687374\\n                    2010  19378102  4318033\\n         Texas      2000  20851820  5906301\\n                    2010  25145561  6879014\\nIn addition, all the ufuncs and other functionality discussed in “Operating on Data in\\nPandas” on page 115 work with hierarchical indices as well. Here we compute the\\nfraction of people under 18 by year, given the above data:\\nIn[11]: f_u18 = pop_df['under18'] / pop_df['total']\\n        f_u18.unstack()\\nOut[11]:                 2000      2010\\n         California  0.273594  0.249211\\n         New York    0.247010  0.222831\\n         Texas       0.283251  0.273568\\nThis allows us to easily and quickly manipulate and explore even high-dimensional\\ndata.\\nMethods of MultiIndex Creation\\nThe most straightforward way to construct a multiply indexed Series or DataFrame\\nis to simply pass a list of two or more index arrays to the constructor. For example:\\nIn[12]: df = pd.DataFrame(np.random.rand(4, 2),\\n                          index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\\n                          columns=['data1', 'data2'])\\n        df\\nOut[12]:         data1     data2\\n         a 1  0.554233  0.356072\\n           2  0.925244  0.219474\\n         b 1  0.441759  0.610054\\n           2  0.171495  0.886688\\nHierarchical Indexing \\n| \\n131\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 149}, page_content=\"The work of creating the MultiIndex is done in the background.\\nSimilarly, if you pass a dictionary with appropriate tuples as keys, Pandas will auto‐\\nmatically recognize this and use a MultiIndex by default:\\nIn[13]: data = {('California', 2000): 33871648,\\n                ('California', 2010): 37253956,\\n                ('Texas', 2000): 20851820,\\n                ('Texas', 2010): 25145561,\\n                ('New York', 2000): 18976457,\\n                ('New York', 2010): 19378102}\\n        pd.Series(data)\\nOut[13]: California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nNevertheless, it is sometimes useful to explicitly create a MultiIndex; we’ll see a cou‐\\nple of these methods here.\\nExplicit MultiIndex constructors\\nFor more flexibility in how the index is constructed, you can instead use the class\\nmethod constructors available in the pd.MultiIndex. For example, as we did before,\\nyou can construct the MultiIndex from a simple list of arrays, giving the index values\\nwithin each level:\\nIn[14]: pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])\\nOut[14]: MultiIndex(levels=[['a', 'b'], [1, 2]],\\n                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\\nYou can construct it from a list of tuples, giving the multiple index values of each\\npoint:\\nIn[15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])\\nOut[15]: MultiIndex(levels=[['a', 'b'], [1, 2]],\\n                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\\nYou can even construct it from a Cartesian product of single indices:\\nIn[16]: pd.MultiIndex.from_product([['a', 'b'], [1, 2]])\\nOut[16]: MultiIndex(levels=[['a', 'b'], [1, 2]],\\n                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\\nSimilarly, you can construct the MultiIndex directly using its internal encoding by\\npassing levels (a list of lists containing available index values for each level) and\\nlabels (a list of lists that reference these labels):\\n132 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 150}, page_content=\"In[17]: pd.MultiIndex(levels=[['a', 'b'], [1, 2]],\\n                       labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\\nOut[17]: MultiIndex(levels=[['a', 'b'], [1, 2]],\\n                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\\nYou can pass any of these objects as the index argument when creating a Series or\\nDataFrame, or to the reindex method of an existing Series or DataFrame.\\nMultiIndex level names\\nSometimes it is convenient to name the levels of the MultiIndex. You can accomplish\\nthis by passing the names argument to any of the above MultiIndex constructors, or\\nby setting the names attribute of the index after the fact:\\nIn[18]: pop.index.names = ['state', 'year']\\n        pop\\nOut[18]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nWith more involved datasets, this can be a useful way to keep track of the meaning of\\nvarious index values.\\nMultiIndex for columns\\nIn a DataFrame, the rows and columns are completely symmetric, and just as the rows\\ncan have multiple levels of indices, the columns can have multiple levels as well. Con‐\\nsider the following, which is a mock-up of some (somewhat realistic) medical data:\\nIn[19]:\\n# hierarchical indices and columns\\nindex = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],\\n                                   names=['year', 'visit'])\\ncolumns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],\\n                                     names=['subject', 'type'])\\n# mock some data\\ndata = np.round(np.random.randn(4, 6), 1)\\ndata[:, ::2] *= 10\\ndata += 37\\n# create the DataFrame\\nhealth_data = pd.DataFrame(data, index=index, columns=columns)\\nhealth_data\\nHierarchical Indexing \\n| \\n133\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 151}, page_content=\"Out[19]: subject      Bob       Guido         Sue\\n         type          HR  Temp    HR  Temp    HR  Temp\\n         year visit\\n         2013 1      31.0  38.7  32.0  36.7  35.0  37.2\\n              2      44.0  37.7  50.0  35.0  29.0  36.7\\n         2014 1      30.0  37.4  39.0  37.8  61.0  36.9\\n              2      47.0  37.8  48.0  37.3  51.0  36.5\\nHere we see where the multi-indexing for both rows and columns can come in very\\nhandy. This is fundamentally four-dimensional data, where the dimensions are the\\nsubject, the measurement type, the year, and the visit number. With this in place we\\ncan, for example, index the top-level column by the person’s name and get a full Data\\nFrame containing just that person’s information:\\nIn[20]: health_data['Guido']\\nOut[20]: type          HR  Temp\\n         year visit\\n         2013 1      32.0  36.7\\n              2      50.0  35.0\\n         2014 1      39.0  37.8\\n              2      48.0  37.3\\nFor complicated records containing multiple labeled measurements across multiple\\ntimes for many subjects (people, countries, cities, etc.), use of hierarchical rows and\\ncolumns can be extremely convenient!\\nIndexing and Slicing a MultiIndex\\nIndexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you\\nthink about the indices as added dimensions. We’ll first look at indexing multiply\\nindexed Series, and then multiply indexed DataFrames.\\nMultiply indexed Series\\nConsider the multiply indexed Series of state populations we saw earlier:\\nIn[21]: pop\\nOut[21]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nWe can access single elements by indexing with multiple terms:\\nIn[22]: pop['California', 2000]\\nOut[22]: 33871648\\n134 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 152}, page_content=\"The MultiIndex also supports partial indexing, or indexing just one of the levels in\\nthe index. The result is another Series, with the lower-level indices maintained:\\nIn[23]: pop['California']\\nOut[23]: year\\n         2000    33871648\\n         2010    37253956\\n         dtype: int64\\nPartial slicing is available as well, as long as the MultiIndex is sorted (see discussion\\nin “Sorted and unsorted indices” on page 137):\\nIn[24]: pop.loc['California':'New York']\\nOut[24]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         dtype: int64\\nWith sorted indices, we can perform partial indexing on lower levels by passing an\\nempty slice in the first index:\\nIn[25]: pop[:, 2000]\\nOut[25]: state\\n         California    33871648\\n         New York      18976457\\n         Texas         20851820\\n         dtype: int64\\nOther types of indexing and selection (discussed in “Data Indexing and Selection” on\\npage 107) work as well; for example, selection based on Boolean masks:\\nIn[26]: pop[pop > 22000000]\\nOut[26]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         Texas       2010    25145561\\n         dtype: int64\\nSelection based on fancy indexing also works:\\nIn[27]: pop[['California', 'Texas']]\\nOut[27]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nHierarchical Indexing \\n| \\n135\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 153}, page_content=\"Multiply indexed DataFrames\\nA multiply indexed DataFrame behaves in a similar manner. Consider our toy medi‐\\ncal DataFrame from before:\\nIn[28]: health_data\\nOut[28]: subject      Bob       Guido         Sue\\n         type          HR  Temp    HR  Temp    HR  Temp\\n         year visit\\n         2013 1      31.0  38.7  32.0  36.7  35.0  37.2\\n              2      44.0  37.7  50.0  35.0  29.0  36.7\\n         2014 1      30.0  37.4  39.0  37.8  61.0  36.9\\n              2      47.0  37.8  48.0  37.3  51.0  36.5\\nRemember that columns are primary in a DataFrame, and the syntax used for multi‐\\nply indexed Series applies to the columns. For example, we can recover Guido’s heart\\nrate data with a simple operation:\\nIn[29]: health_data['Guido', 'HR']\\nOut[29]: year  visit\\n         2013  1        32.0\\n               2        50.0\\n         2014  1        39.0\\n               2        48.0\\n         Name: (Guido, HR), dtype: float64\\nAlso, as with the single-index case, we can use the loc, iloc, and ix indexers intro‐\\nduced in “Data Indexing and Selection” on page 107. For example:\\nIn[30]: health_data.iloc[:2, :2]\\nOut[30]: subject      Bob\\n         type          HR  Temp\\n         year visit\\n         2013 1      31.0  38.7\\n              2      44.0  37.7\\nThese indexers provide an array-like view of the underlying two-dimensional data,\\nbut each individual index in loc or iloc can be passed a tuple of multiple indices. For\\nexample:\\nIn[31]: health_data.loc[:, ('Bob', 'HR')]\\nOut[31]: year  visit\\n         2013  1        31.0\\n               2        44.0\\n         2014  1        30.0\\n               2        47.0\\n         Name: (Bob, HR), dtype: float64\\nWorking with slices within these index tuples is not especially convenient; trying to\\ncreate a slice within a tuple will lead to a syntax error:\\n136 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 154}, page_content='In[32]: health_data.loc[(:, 1), (:, \\'HR\\')]\\n  File \"<ipython-input-32-8e3cc151e316>\", line 1\\n    health_data.loc[(:, 1), (:, \\'HR\\')]\\n                     ^\\nSyntaxError: invalid syntax\\nYou could get around this by building the desired slice explicitly using Python’s built-\\nin slice() function, but a better way in this context is to use an IndexSlice object,\\nwhich Pandas provides for precisely this situation. For example:\\nIn[33]: idx = pd.IndexSlice\\n        health_data.loc[idx[:, 1], idx[:, \\'HR\\']]\\nOut[33]: subject      Bob Guido   Sue\\n         type          HR    HR    HR\\n         year visit\\n         2013 1      31.0  32.0  35.0\\n         2014 1      30.0  39.0  61.0\\nThere are so many ways to interact with data in multiply indexed Series and Data\\nFrames, and as with many tools in this book the best way to become familiar with\\nthem is to try them out!\\nRearranging Multi-Indices\\nOne of the keys to working with multiply indexed data is knowing how to effectively\\ntransform the data. There are a number of operations that will preserve all the infor‐\\nmation in the dataset, but rearrange it for the purposes of various computations. We\\nsaw a brief example of this in the stack() and unstack() methods, but there are\\nmany more ways to finely control the rearrangement of data between hierarchical\\nindices and columns, and we’ll explore them here.\\nSorted and unsorted indices\\nEarlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of\\nthe MultiIndex slicing operations will fail if the index is not sorted. Let’s take a look at\\nthis here.\\nWe’ll start by creating some simple multiply indexed data where the indices are not\\nlexographically sorted:\\nIn[34]: index = pd.MultiIndex.from_product([[\\'a\\', \\'c\\', \\'b\\'], [1, 2]])\\n        data = pd.Series(np.random.rand(6), index=index)\\n        data.index.names = [\\'char\\', \\'int\\']\\n        data\\nOut[34]: char  int\\n         a     1      0.003001\\n               2      0.164974\\n         c     1      0.741650\\nHierarchical Indexing \\n| \\n137'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 155}, page_content=\"2      0.569264\\n         b     1      0.001693\\n               2      0.526226\\n         dtype: float64\\nIf we try to take a partial slice of this index, it will result in an error:\\nIn[35]: try:\\n            data['a':'b']\\n        except KeyError as e:\\n            print(type(e))\\n            print(e)\\n<class 'KeyError'>\\n'Key length (1) was greater than MultiIndex lexsort depth (0)'\\nAlthough it is not entirely clear from the error message, this is the result of the Multi\\nIndex not being sorted. For various reasons, partial slices and other similar opera‐\\ntions require the levels in the MultiIndex to be in sorted (i.e., lexographical) order.\\nPandas provides a number of convenience routines to perform this type of sorting;\\nexamples are the sort_index() and sortlevel() methods of the DataFrame. We’ll\\nuse the simplest, sort_index(), here:\\nIn[36]: data = data.sort_index()\\n        data\\nOut[36]: char  int\\n         a     1      0.003001\\n               2      0.164974\\n         b     1      0.001693\\n               2      0.526226\\n         c     1      0.741650\\n               2      0.569264\\n         dtype: float64\\nWith the index sorted in this way, partial slicing will work as expected:\\nIn[37]: data['a':'b']\\nOut[37]: char  int\\n         a     1      0.003001\\n               2      0.164974\\n         b     1      0.001693\\n               2      0.526226\\n         dtype: float64\\nStacking and unstacking indices\\nAs we saw briefly before, it is possible to convert a dataset from a stacked multi-index\\nto a simple two-dimensional representation, optionally specifying the level to use:\\n138 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 156}, page_content=\"In[38]: pop.unstack(level=0)\\nOut[38]:  state  California   New York     Texas\\n          year\\n          2000     33871648   18976457  20851820\\n          2010     37253956   19378102  25145561\\nIn[39]: pop.unstack(level=1)\\nOut[39]: year            2000      2010\\n         state\\n         California  33871648  37253956\\n         New York    18976457  19378102\\n         Texas       20851820  25145561\\nThe opposite of unstack() is stack(), which here can be used to recover the original\\nseries:\\nIn[40]: pop.unstack().stack()\\nOut[40]: state       year\\n         California  2000    33871648\\n                     2010    37253956\\n         New York    2000    18976457\\n                     2010    19378102\\n         Texas       2000    20851820\\n                     2010    25145561\\n         dtype: int64\\nIndex setting and resetting\\nAnother way to rearrange hierarchical data is to turn the index labels into columns;\\nthis can be accomplished with the reset_index method. Calling this on the popula‐\\ntion dictionary will result in a DataFrame with a state and year column holding the\\ninformation that was formerly in the index. For clarity, we can optionally specify the\\nname of the data for the column representation:\\nIn[41]: pop_flat = pop.reset_index(name='population')\\n        pop_flat\\nOut[41]:         state  year  population\\n         0  California  2000    33871648\\n         1  California  2010    37253956\\n         2    New York  2000    18976457\\n         3    New York  2010    19378102\\n         4       Texas  2000    20851820\\n         5       Texas  2010    25145561\\nOften when you are working with data in the real world, the raw input data looks like\\nthis and it’s useful to build a MultiIndex from the column values. This can be done\\nwith the set_index method of the DataFrame, which returns a multiply indexed Data\\nFrame:\\nHierarchical Indexing \\n| \\n139\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 157}, page_content=\"In[42]: pop_flat.set_index(['state', 'year'])\\nOut[42]:                  population\\n         state      year\\n         California 2000    33871648\\n                    2010    37253956\\n         New York   2000    18976457\\n                    2010    19378102\\n         Texas      2000    20851820\\n                    2010    25145561\\nIn practice, I find this type of reindexing to be one of the more useful patterns when I\\nencounter real-world datasets.\\nData Aggregations on Multi-Indices\\nWe’ve previously seen that Pandas has built-in data aggregation methods, such as\\nmean(), sum(), and max(). For hierarchically indexed data, these can be passed a\\nlevel parameter that controls which subset of the data the aggregate is computed on.\\nFor example, let’s return to our health data:\\nIn[43]: health_data\\nOut[43]:  subject      Bob       Guido         Sue\\n          type          HR  Temp    HR  Temp    HR  Temp\\n          year visit\\n          2013 1      31.0  38.7  32.0  36.7  35.0  37.2\\n               2      44.0  37.7  50.0  35.0  29.0  36.7\\n          2014 1      30.0  37.4  39.0  37.8  61.0  36.9\\n               2      47.0  37.8  48.0  37.3  51.0  36.5\\nPerhaps we’d like to average out the measurements in the two visits each year. We can\\ndo this by naming the index level we’d like to explore, in this case the year:\\nIn[44]: data_mean = health_data.mean(level='year')\\n        data_mean\\nOut[44]: subject   Bob       Guido          Sue\\n         type       HR  Temp    HR   Temp    HR   Temp\\n         year\\n         2013     37.5  38.2  41.0  35.85  32.0  36.95\\n         2014     38.5  37.6  43.5  37.55  56.0  36.70\\nBy further making use of the axis keyword, we can take the mean among levels on\\nthe columns as well:\\nIn[45]: data_mean.mean(axis=1, level='type')\\nOut[45]: type         HR       Temp\\n         year\\n         2013  36.833333  37.000000\\n         2014  46.000000  37.283333\\n140 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 158}, page_content='Thus in two lines, we’ve been able to find the average heart rate and temperature\\nmeasured among all subjects in all visits each year. This syntax is actually a shortcut\\nto the GroupBy functionality, which we will discuss in “Aggregation and Grouping” on\\npage 158. While this is a toy example, many real-world datasets have similar hierarch‐\\nical structure.\\nPanel Data\\nPandas has a few other fundamental data structures that we have not yet discussed,\\nnamely the pd.Panel and pd.Panel4D objects. These can be thought of, respectively,\\nas three-dimensional and four-dimensional generalizations of the (one-dimensional)\\nSeries and (two-dimensional) DataFrame structures. Once you are familiar with\\nindexing and manipulation of data in a Series and DataFrame, Panel and Panel4D\\nare relatively straightforward to use. In particular, the ix, loc, and iloc indexers dis‐\\ncussed in “Data Indexing and Selection” on page 107 extend readily to these higher-\\ndimensional structures.\\nWe won’t cover these panel structures further in this text, as I’ve found in the majority\\nof cases that multi-indexing is a more useful and conceptually simpler representation\\nfor higher-dimensional data. Additionally, panel data is fundamentally a dense data\\nrepresentation, while multi-indexing is fundamentally a sparse data representation.\\nAs the number of dimensions increases, the dense representation can become very\\ninefficient for the majority of real-world datasets. For the occasional specialized appli‐\\ncation, however, these structures can be useful. If you’d like to read more about the\\nPanel and Panel4D structures, see the references listed in “Further Resources” on\\npage 215.\\nCombining Datasets: Concat and Append\\nSome of the most interesting studies of data come from combining different data\\nsources. These operations can involve anything from very straightforward concatena‐\\ntion of two different datasets, to more complicated database-style joins and merges\\nthat correctly handle any overlaps between the datasets. Series and DataFrames are\\nbuilt with this type of operation in mind, and Pandas includes functions and methods\\nthat make this sort of data wrangling fast and straightforward.\\nHere we’ll take a look at simple concatenation of Series and DataFrames with the\\npd.concat function; later we’ll dive into more sophisticated in-memory merges and\\njoins implemented in Pandas.\\nWe begin with the standard imports:\\nIn[1]: import pandas as pd\\n       import numpy as np\\nCombining Datasets: Concat and Append \\n| \\n141'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 159}, page_content='For convenience, we’ll define this function, which creates a DataFrame of a particular\\nform that will be useful below:\\nIn[2]: def make_df(cols, ind):\\n           \"\"\"Quickly make a DataFrame\"\"\"\\n           data = {c: [str(c) + str(i) for i in ind]\\n                   for c in cols}\\n           return pd.DataFrame(data, ind)\\n       # example DataFrame\\n       make_df(\\'ABC\\', range(3))\\nOut[2]:     A   B   C\\n        0  A0  B0  C0\\n        1  A1  B1  C1\\n        2  A2  B2  C2\\nRecall: Concatenation of NumPy Arrays\\nConcatenation of Series and DataFrame objects is very similar to concatenation of\\nNumPy arrays, which can be done via the np.concatenate function as discussed in\\n“The Basics of NumPy Arrays” on page 42. Recall that with it, you can combine the\\ncontents of two or more arrays into a single array:\\nIn[4]: x = [1, 2, 3]\\n       y = [4, 5, 6]\\n       z = [7, 8, 9]\\n       np.concatenate([x, y, z])\\nOut[4]: array([1, 2, 3, 4, 5, 6, 7, 8, 9])\\nThe first argument is a list or tuple of arrays to concatenate. Additionally, it takes an\\naxis keyword that allows you to specify the axis along which the result will be\\nconcatenated:\\nIn[5]: x = [[1, 2],\\n            [3, 4]]\\n       np.concatenate([x, x], axis=1)\\nOut[5]: array([[1, 2, 1, 2],\\n               [3, 4, 3, 4]])\\nSimple Concatenation with pd.concat\\nPandas has a function, pd.concat(), which has a similar syntax to np.concatenate\\nbut contains a number of options that we’ll discuss momentarily:\\n# Signature in Pandas v0.18\\npd.concat(objs, axis=0, join=\\'outer\\', join_axes=None, ignore_index=False,\\n          keys=None, levels=None, names=None, verify_integrity=False,\\n          copy=True)\\n142 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 160}, page_content=\"pd.concat() can be used for a simple concatenation of Series or DataFrame objects,\\njust as np.concatenate() can be used for simple concatenations of arrays:\\nIn[6]: ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\\n       ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\\n       pd.concat([ser1, ser2])\\nOut[6]: 1    A\\n        2    B\\n        3    C\\n        4    D\\n        5    E\\n        6    F\\n        dtype: object\\nIt also works to concatenate higher-dimensional objects, such as DataFrames:\\nIn[7]: df1 = make_df('AB', [1, 2])\\n       df2 = make_df('AB', [3, 4])\\n       print(df1); print(df2); print(pd.concat([df1, df2]))\\ndf1             df2           pd.concat([df1, df2])\\n     A   B          A   B          A   B\\n 1  A1  B1      3  A3  B3      1  A1  B1\\n 2  A2  B2      4  A4  B4      2  A2  B2\\n                               3  A3  B3\\n                               4  A4  B4\\nBy default, the concatenation takes place row-wise within the DataFrame (i.e.,\\naxis=0). Like np.concatenate, pd.concat allows specification of an axis along which\\nconcatenation will take place. Consider the following example:\\nIn[8]: df3 = make_df('AB', [0, 1])\\n       df4 = make_df('CD', [0, 1])\\n       print(df3); print(df4); print(pd.concat([df3, df4], axis='col'))\\ndf3             df4           pd.concat([df3, df4], axis='col')\\n     A   B          C   D         A   B   C   D\\n 0  A0  B0      0  C0  D0     0  A0  B0  C0  D0\\n 1  A1  B1      1  C1  D1     1  A1  B1  C1  D1\\nWe could have equivalently specified axis=1; here we’ve used the more intuitive\\naxis='col'.\\nDuplicate indices\\nOne important difference between np.concatenate and pd.concat is that Pandas\\nconcatenation preserves indices, even if the result will have duplicate indices! Consider\\nthis simple example:\\nIn[9]: x = make_df('AB', [0, 1])\\n       y = make_df('AB', [2, 3])\\nCombining Datasets: Concat and Append \\n| \\n143\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 161}, page_content='y.index = x.index  # make duplicate indices!\\n       print(x); print(y); print(pd.concat([x, y]))\\nx             y            pd.concat([x, y])\\n     A   B        A   B        A   B\\n 0  A0  B0    0  A2  B2    0  A0  B0\\n 1  A1  B1    1  A3  B3    1  A1  B1\\n                           0  A2  B2\\n                           1  A3  B3\\nNotice the repeated indices in the result. While this is valid within DataFrames, the\\noutcome is often undesirable. pd.concat() gives us a few ways to handle it.\\nCatching the repeats as an error.    If you’d like to simply verify that the indices in the\\nresult of pd.concat() do not overlap, you can specify the verify_integrity flag.\\nWith this set to True, the concatenation will raise an exception if there are duplicate\\nindices. Here is an example, where for clarity we’ll catch and print the error message:\\nIn[10]: try:\\n            pd.concat([x, y], verify_integrity=True)\\n        except ValueError as e:\\n            print(\"ValueError:\", e)\\nValueError: Indexes have overlapping values: [0, 1]\\nIgnoring the index.    Sometimes the index itself does not matter, and you would prefer\\nit to simply be ignored. You can specify this option using the ignore_index flag. With\\nthis set to True, the concatenation will create a new integer index for the resulting\\nSeries:\\nIn[11]: print(x); print(y); print(pd.concat([x, y], ignore_index=True))\\nx             y            pd.concat([x, y], ignore_index=True)\\n     A   B        A   B        A   B\\n 0  A0  B0    0  A2  B2    0  A0  B0\\n 1  A1  B1    1  A3  B3    1  A1  B1\\n                           2  A2  B2\\n                           3  A3  B3\\nAdding MultiIndex keys.    Another alternative is to use the keys option to specify a label\\nfor the data sources; the result will be a hierarchically indexed series containing the\\ndata:\\nIn[12]: print(x); print(y); print(pd.concat([x, y], keys=[\\'x\\', \\'y\\']))\\nx               y              pd.concat([x, y], keys=[\\'x\\', \\'y\\'])\\n     A   B          A   B             A   B\\n 0  A0  B0      0  A2  B2      x  0  A0  B0\\n 1  A1  B1      1  A3  B3         1  A1  B1\\n                               y  0  A2  B2\\n                                  1  A3  B3\\n144 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 162}, page_content=\"The result is a multiply indexed DataFrame, and we can use the tools discussed in\\n“Hierarchical Indexing” on page 128 to transform this data into the representation\\nwe’re interested in.\\nConcatenation with joins\\nIn the simple examples we just looked at, we were mainly concatenating DataFrames\\nwith shared column names. In practice, data from different sources might have differ‐\\nent sets of column names, and pd.concat offers several options in this case. Consider\\nthe concatenation of the following two DataFrames, which have some (but not all!)\\ncolumns in common:\\nIn[13]: df5 = make_df('ABC', [1, 2])\\n        df6 = make_df('BCD', [3, 4])\\n        print(df5); print(df6); print(pd.concat([df5, df6])\\ndf5               df6              pd.concat([df5, df6])\\n     A   B   C        B   C   D         A   B   C    D\\n 1  A1  B1  C1    3  B3  C3  D3    1   A1  B1  C1  NaN\\n 2  A2  B2  C2    4  B4  C4  D4    2   A2  B2  C2  NaN\\n                                   3  NaN  B3  C3   D3\\n                                   4  NaN  B4  C4   D4\\nBy default, the entries for which no data is available are filled with NA values. To\\nchange this, we can specify one of several options for the join and join_axes param‐\\neters of the concatenate function. By default, the join is a union of the input columns\\n(join='outer'), but we can change this to an intersection of the columns using\\njoin='inner':\\nIn[14]: print(df5); print(df6);\\n        print(pd.concat([df5, df6], join='inner'))\\ndf5               df6              pd.concat([df5, df6], join='inner')\\n     A   B   C        B   C   D        B   C\\n 1  A1  B1  C1    3  B3  C3  D3    1  B1  C1\\n 2  A2  B2  C2    4  B4  C4  D4    2  B2  C2\\n                                   3  B3  C3\\n                                   4  B4  C4\\nAnother option is to directly specify the index of the remaining colums using the\\njoin_axes argument, which takes a list of index objects. Here we’ll specify that the\\nreturned columns should be the same as those of the first input:\\nIn[15]: print(df5); print(df6);\\n        print(pd.concat([df5, df6], join_axes=[df5.columns]))\\ndf5               df6              pd.concat([df5, df6], join_axes=[df5.columns])\\n     A   B   C        B   C   D         A   B   C\\n 1  A1  B1  C1    3  B3  C3  D3    1   A1  B1  C1\\n 2  A2  B2  C2    4  B4  C4  D4    2   A2  B2  C2\\nCombining Datasets: Concat and Append \\n| \\n145\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 163}, page_content='3  NaN  B3  C3\\n                                   4  NaN  B4  C4\\nThe combination of options of the pd.concat function allows a wide range of possi‐\\nble behaviors when you are joining two datasets; keep these in mind as you use these\\ntools for your own data.\\nThe append() method\\nBecause direct array concatenation is so common, Series and DataFrame objects\\nhave an append method that can accomplish the same thing in fewer keystrokes. For\\nexample, rather than calling pd.concat([df1, df2]), you can simply call\\ndf1.append(df2):\\nIn[16]: print(df1); print(df2); print(df1.append(df2))\\ndf1             df2           df1.append(df2)\\n     A   B          A   B          A   B\\n 1  A1  B1      3  A3  B3     1   A1  B1\\n 2  A2  B2      4  A4  B4     2   A2  B2\\n                              3   A3  B3\\n                              4   A4  B4\\nKeep in mind that unlike the append() and extend() methods of Python lists, the\\nappend() method in Pandas does not modify the original object—instead, it creates a\\nnew object with the combined data. It also is not a very efficient method, because it\\ninvolves creation of a new index and data buffer. Thus, if you plan to do multiple\\nappend operations, it is generally better to build a list of DataFrames and pass them all\\nat once to the concat() function.\\nIn the next section, we’ll look at another more powerful approach to combining data\\nfrom multiple sources, the database-style merges/joins implemented in pd.merge. For\\nmore information on concat(), append(), and related functionality, see the “Merge,\\nJoin, and Concatenate” section of the Pandas documentation.\\nCombining Datasets: Merge and Join\\nOne essential feature offered by Pandas is its high-performance, in-memory join and\\nmerge operations. If you have ever worked with databases, you should be familiar\\nwith this type of data interaction. The main interface for this is the pd.merge func‐\\ntion, and we’ll see a few examples of how this can work in practice.\\nRelational Algebra\\nThe behavior implemented in pd.merge() is a subset of what is known as relational\\nalgebra, which is a formal set of rules for manipulating relational data, and forms the\\nconceptual foundation of operations available in most databases. The strength of the\\n146 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 164}, page_content=\"relational algebra approach is that it proposes several primitive operations, which\\nbecome the building blocks of more complicated operations on any dataset. With this\\nlexicon of fundamental operations implemented efficiently in a database or other pro‐\\ngram, a wide range of fairly complicated composite operations can be performed.\\nPandas implements several of these fundamental building blocks in the pd.merge()\\nfunction and the related join() method of Series and DataFrames. As we will see,\\nthese let you efficiently link data from different sources.\\nCategories of Joins\\nThe pd.merge() function implements a number of types of joins: the one-to-one,\\nmany-to-one, and many-to-many joins. All three types of joins are accessed via an\\nidentical call to the pd.merge() interface; the type of join performed depends on the\\nform of the input data. Here we will show simple examples of the three types of\\nmerges, and discuss detailed options further below.\\nOne-to-one joins\\nPerhaps the simplest type of merge expression is the one-to-one join, which is in\\nmany ways very similar to the column-wise concatenation seen in “Combining Data‐\\nsets: Concat and Append” on page 141. As a concrete example, consider the following\\ntwo DataFrames, which contain information on several employees in a company:\\nIn[2]:\\ndf1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\\n                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\\ndf2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\\n                    'hire_date': [2004, 2008, 2012, 2014]})\\nprint(df1); print(df2)\\ndf1                        df2\\n  employee        group      employee  hire_date\\n0      Bob   Accounting    0     Lisa       2004\\n1     Jake  Engineering    1      Bob       2008\\n2     Lisa  Engineering    2     Jake       2012\\n3      Sue           HR    3      Sue       2014\\nTo combine this information into a single DataFrame, we can use the pd.merge()\\nfunction:\\nIn[3]: df3 = pd.merge(df1, df2)\\n       df3\\nOut[3]:   employee        group  hire_date\\n        0      Bob   Accounting       2008\\n        1     Jake  Engineering       2012\\n        2     Lisa  Engineering       2004\\n        3      Sue           HR       2014\\nCombining Datasets: Merge and Join \\n| \\n147\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 165}, page_content=\"The pd.merge() function recognizes that each DataFrame has an “employee” column,\\nand automatically joins using this column as a key. The result of the merge is a new\\nDataFrame that combines the information from the two inputs. Notice that the order\\nof entries in each column is not necessarily maintained: in this case, the order of the\\n“employee” column differs between df1 and df2, and the pd.merge() function cor‐\\nrectly accounts for this. Additionally, keep in mind that the merge in general discards\\nthe index, except in the special case of merges by index (see “The left_index and\\nright_index keywords” on page 151).\\nMany-to-one joins\\nMany-to-one joins are joins in which one of the two key columns contains duplicate\\nentries. For the many-to-one case, the resulting DataFrame will preserve those dupli‐\\ncate entries as appropriate. Consider the following example of a many-to-one join:\\nIn[4]: df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\\n                           'supervisor': ['Carly', 'Guido', 'Steve']})\\n       print(df3); print(df4); print(pd.merge(df3, df4))\\ndf3                                   df4\\n  employee        group  hire_date             group supervisor\\n0      Bob   Accounting       2008    0   Accounting      Carly\\n1     Jake  Engineering       2012    1  Engineering      Guido\\n2     Lisa  Engineering       2004    2           HR      Steve\\n3      Sue           HR       2014\\npd.merge(df3, df4)\\n  employee        group  hire_date supervisor\\n0      Bob   Accounting       2008      Carly\\n1     Jake  Engineering       2012      Guido\\n2     Lisa  Engineering       2004      Guido\\n3      Sue           HR       2014      Steve\\nThe resulting DataFrame has an additional column with the “supervisor” information,\\nwhere the information is repeated in one or more locations as required by the inputs.\\nMany-to-many joins\\nMany-to-many joins are a bit confusing conceptually, but are nevertheless well\\ndefined. If the key column in both the left and right array contains duplicates, then\\nthe result is a many-to-many merge. This will be perhaps most clear with a concrete\\nexample. Consider the following, where we have a DataFrame showing one or more\\nskills associated with a particular group.\\nBy performing a many-to-many join, we can recover the skills associated with any\\nindividual person:\\nIn[5]: df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\\n                                     'Engineering', 'Engineering', 'HR', 'HR'],\\n148 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 166}, page_content=\"'skills': ['math', 'spreadsheets', 'coding', 'linux',\\n                                      'spreadsheets', 'organization']})\\nprint(df1); print(df5); print(pd.merge(df1, df5))\\ndf1                          df5\\n  employee        group               group        skills\\n0      Bob   Accounting      0   Accounting          math\\n1     Jake  Engineering      1   Accounting  spreadsheets\\n2     Lisa  Engineering      2  Engineering        coding\\n3      Sue           HR      3  Engineering         linux\\n                             4           HR  spreadsheets\\n                             5           HR  organization\\npd.merge(df1, df5)\\n  employee        group        skills\\n0      Bob   Accounting          math\\n1      Bob   Accounting  spreadsheets\\n2     Jake  Engineering        coding\\n3     Jake  Engineering         linux\\n4     Lisa  Engineering        coding\\n5     Lisa  Engineering         linux\\n6      Sue           HR  spreadsheets\\n7      Sue           HR  organization\\nThese three types of joins can be used with other Pandas tools to implement a wide\\narray of functionality. But in practice, datasets are rarely as clean as the one we’re\\nworking with here. In the following section, we’ll consider some of the options pro‐\\nvided by pd.merge() that enable you to tune how the join operations work.\\nSpecification of the Merge Key\\nWe’ve already seen the default behavior of pd.merge(): it looks for one or more\\nmatching column names between the two inputs, and uses this as the key. However,\\noften the column names will not match so nicely, and pd.merge() provides a variety\\nof options for handling this.\\nThe on keyword\\nMost simply, you can explicitly specify the name of the key column using the on key‐\\nword, which takes a column name or a list of column names:\\nIn[6]: print(df1); print(df2); print(pd.merge(df1, df2, on='employee'))\\ndf1                          df2\\n  employee        group          employee  hire_date\\n0      Bob   Accounting        0     Lisa       2004\\n1     Jake  Engineering        1      Bob       2008\\n2     Lisa  Engineering        2     Jake       2012\\n3      Sue           HR        3      Sue       2014\\nCombining Datasets: Merge and Join \\n| \\n149\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 167}, page_content='pd.merge(df1, df2, on=\\'employee\\')\\n  employee        group  hire_date\\n0      Bob   Accounting       2008\\n1     Jake  Engineering       2012\\n2     Lisa  Engineering       2004\\n3      Sue           HR       2014\\nThis option works only if both the left and right DataFrames have the specified col‐\\numn name.\\nThe left_on and right_on keywords\\nAt times you may wish to merge two datasets with different column names; for exam‐\\nple, we may have a dataset in which the employee name is labeled as “name” rather\\nthan “employee”. In this case, we can use the left_on and right_on keywords to\\nspecify the two column names:\\nIn[7]:\\ndf3 = pd.DataFrame({\\'name\\': [\\'Bob\\', \\'Jake\\', \\'Lisa\\', \\'Sue\\'],\\n                    \\'salary\\': [70000, 80000, 120000, 90000]})\\nprint(df1); print(df3);\\nprint(pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\"))\\ndf1                          df3\\n  employee        group          name  salary\\n0      Bob   Accounting        0   Bob   70000\\n1     Jake  Engineering        1  Jake   80000\\n2     Lisa  Engineering        2  Lisa  120000\\n3      Sue           HR        3   Sue   90000\\npd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")\\n  employee        group  name  salary\\n0      Bob   Accounting   Bob   70000\\n1     Jake  Engineering  Jake   80000\\n2     Lisa  Engineering  Lisa  120000\\n3      Sue           HR   Sue   90000\\nThe result has a redundant column that we can drop if desired—for example, by\\nusing the drop() method of DataFrames:\\nIn[8]:\\npd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop(\\'name\\', axis=1)\\nOut[8]:   employee        group  salary\\n        0      Bob   Accounting   70000\\n        1     Jake  Engineering   80000\\n        2     Lisa  Engineering  120000\\n        3      Sue           HR   90000\\n150 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 168}, page_content=\"The left_index and right_index keywords\\nSometimes, rather than merging on a column, you would instead like to merge on an\\nindex. For example, your data might look like this:\\nIn[9]: df1a = df1.set_index('employee')\\n       df2a = df2.set_index('employee')\\n       print(df1a); print(df2a)\\ndf1a                        df2a\\n                group                  hire_date\\nemployee                     employee\\nBob        Accounting        Lisa           2004\\nJake      Engineering        Bob            2008\\nLisa      Engineering        Jake           2012\\nSue                HR        Sue            2014\\nYou can use the index as the key for merging by specifying the left_index and/or\\nright_index flags in pd.merge():\\nIn[10]:\\nprint(df1a); print(df2a);\\nprint(pd.merge(df1a, df2a, left_index=True, right_index=True))\\ndf1a                        df2a\\n                group                  hire_date\\nemployee                     employee\\nBob        Accounting        Lisa           2004\\nJake      Engineering        Bob            2008\\nLisa      Engineering        Jake           2012\\nSue                HR        Sue            2014\\npd.merge(df1a, df2a, left_index=True, right_index=True)\\n                 group  hire_date\\nemployee\\nLisa      Engineering       2004\\nBob        Accounting       2008\\nJake      Engineering       2012\\nSue                HR       2014\\nFor convenience, DataFrames implement the join() method, which performs a\\nmerge that defaults to joining on indices:\\nIn[11]: print(df1a); print(df2a); print(df1a.join(df2a))\\ndf1a                      df2a\\n                group                hire_date\\nemployee                   employee\\nBob        Accounting      Lisa           2004\\nJake      Engineering      Bob            2008\\nLisa      Engineering      Jake           2012\\nSue                HR      Sue            2014\\nCombining Datasets: Merge and Join \\n| \\n151\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 169}, page_content=\"df1a.join(df2a)\\n                group  hire_date\\nemployee\\nBob        Accounting       2008\\nJake      Engineering       2012\\nLisa      Engineering       2004\\nSue                HR       2014\\nIf you’d like to mix indices and columns, you can combine left_index with right_on\\nor left_on with right_index to get the desired behavior:\\nIn[12]:\\nprint(df1a); print(df3);\\nprint(pd.merge(df1a, df3, left_index=True, right_on='name'))\\ndf1a                        df3\\n                group\\nemployee                    name  salary\\nBob        Accounting    0   Bob   70000\\nJake      Engineering    1  Jake   80000\\nLisa      Engineering    2  Lisa  120000\\nSue                HR    3   Sue   90000\\npd.merge(df1a, df3, left_index=True, right_on='name')\\n          group  name  salary\\n0   Accounting   Bob   70000\\n1  Engineering  Jake   80000\\n2  Engineering  Lisa  120000\\n3           HR   Sue   90000\\nAll of these options also work with multiple indices and/or multiple columns; the\\ninterface for this behavior is very intuitive. For more information on this, see the\\n“Merge, Join, and Concatenate” section of the Pandas documentation.\\nSpecifying Set Arithmetic for Joins\\nIn all the preceding examples we have glossed over one important consideration in\\nperforming a join: the type of set arithmetic used in the join. This comes up when a\\nvalue appears in one key column but not the other. Consider this example:\\nIn[13]: df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\\n                            'food': ['fish', 'beans', 'bread']},\\n                           columns=['name', 'food'])\\n        df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\\n                            'drink': ['wine', 'beer']},\\n                           columns=['name', 'drink'])\\n        print(df6); print(df7); print(pd.merge(df6, df7))\\n152 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 170}, page_content=\"df6                 df7               pd.merge(df6, df7)\\n    name   food          name drink       name   food  drink\\n0  Peter   fish     0    Mary  wine   0   Mary  bread   wine\\n1   Paul  beans     1  Joseph  beer\\n2   Mary  bread\\nHere we have merged two datasets that have only a single “name” entry in common:\\nMary. By default, the result contains the intersection of the two sets of inputs; this is\\nwhat is known as an inner join. We can specify this explicitly using the how keyword,\\nwhich defaults to 'inner':\\nIn[14]: pd.merge(df6, df7, how='inner')\\nOut[14]:    name   food drink\\n         0  Mary  bread  wine\\nOther options for the how keyword are 'outer', 'left', and 'right'. An outer join\\nreturns a join over the union of the input columns, and fills in all missing values with\\nNAs:\\nIn[15]: print(df6); print(df7); print(pd.merge(df6, df7, how='outer'))\\ndf6                 df7                pd.merge(df6, df7, how='outer')\\n     name   food         name drink         name   food drink\\n0  Peter   fish     0    Mary  wine    0   Peter   fish   NaN\\n1   Paul  beans     1  Joseph  beer    1    Paul  beans   NaN\\n2   Mary  bread                        2    Mary  bread  wine\\n                                       3  Joseph    NaN  beer\\nThe left join and right join return join over the left entries and right entries, respec‐\\ntively. For example:\\nIn[16]: print(df6); print(df7); print(pd.merge(df6, df7, how='left'))\\ndf6                 df7                pd.merge(df6, df7, how='left')\\n     name   food         name drink         name   food drink\\n0  Peter   fish     0    Mary  wine    0   Peter   fish   NaN\\n1   Paul  beans     1  Joseph  beer    1    Paul  beans   NaN\\n2   Mary  bread                        2    Mary  bread  wine\\nThe output rows now correspond to the entries in the left input. Using how='right'\\nworks in a similar manner.\\nAll of these options can be applied straightforwardly to any of the preceding join\\ntypes.\\nOverlapping Column Names: The suffixes Keyword\\nFinally, you may end up in a case where your two input DataFrames have conflicting\\ncolumn names. Consider this example:\\nIn[17]: df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\\n                            'rank': [1, 2, 3, 4]})\\nCombining Datasets: Merge and Join \\n| \\n153\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 171}, page_content='df9 = pd.DataFrame({\\'name\\': [\\'Bob\\', \\'Jake\\', \\'Lisa\\', \\'Sue\\'],\\n                            \\'rank\\': [3, 1, 4, 2]})\\n        print(df8); print(df9); print(pd.merge(df8, df9, on=\"name\"))\\ndf8                df9               pd.merge(df8, df9, on=\"name\")\\n    name  rank         name  rank       name  rank_x  rank_y\\n0   Bob     1      0   Bob     3     0   Bob       1       3\\n1  Jake     2      1  Jake     1     1  Jake       2       1\\n2  Lisa     3      2  Lisa     4     2  Lisa       3       4\\n3   Sue     4      3   Sue     2     3   Sue       4       2\\nBecause the output would have two conflicting column names, the merge function\\nautomatically appends a suffix _x or _y to make the output columns unique. If these\\ndefaults are inappropriate, it is possible to specify a custom suffix using the suffixes\\nkeyword:\\nIn[18]:\\nprint(df8); print(df9);\\nprint(pd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"]))\\ndf8                df9\\n    name  rank         name  rank\\n0   Bob     1      0   Bob     3\\n1  Jake     2      1  Jake     1\\n2  Lisa     3      2  Lisa     4\\n3   Sue     4      3   Sue     2\\npd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"])\\n   name  rank_L  rank_R\\n0   Bob       1       3\\n1  Jake       2       1\\n2  Lisa       3       4\\n3   Sue       4       2\\nThese suffixes work in any of the possible join patterns, and work also if there are\\nmultiple overlapping columns.\\nFor more information on these patterns, see “Aggregation and Grouping” on page\\n158, where we dive a bit deeper into relational algebra. Also see the “Merge, Join, and\\nConcatenate” section of the Pandas documentation for further discussion of these\\ntopics.\\nExample: US States Data\\nMerge and join operations come up most often when one is combining data from dif‐\\nferent sources. Here we will consider an example of some data about US states and\\ntheir populations. The data files can be found at http://github.com/jakevdp/data-\\nUSstates/:\\nIn[19]:\\n# Following are shell commands to download the data\\n154 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 172}, page_content=\"# !curl -O https://raw.githubusercontent.com/jakevdp/\\n#    data-USstates/master/state-population.csv\\n# !curl -O https://raw.githubusercontent.com/jakevdp/\\n#    data-USstates/master/state-areas.csv\\n# !curl -O https://raw.githubusercontent.com/jakevdp/\\n#    data-USstates/master/state-abbrevs.csv\\nLet’s take a look at the three datasets, using the Pandas read_csv() function:\\nIn[20]: pop = pd.read_csv('state-population.csv')\\n        areas = pd.read_csv('state-areas.csv')\\n        abbrevs = pd.read_csv('state-abbrevs.csv')\\n        print(pop.head()); print(areas.head()); print(abbrevs.head())\\npop.head()                                    areas.head()\\n  state/region     ages  year  population             state  area (sq. mi)\\n0           AL  under18  2012   1117489.0     0     Alabama          52423\\n1           AL    total  2012   4817528.0     1      Alaska         656425\\n2           AL  under18  2010   1130966.0     2     Arizona         114006\\n3           AL    total  2010   4785570.0     3    Arkansas          53182\\n4           AL  under18  2011   1125763.0     3    Arkansas          53182\\n                                              4  California         163707\\nabbrevs.head()\\n        state abbreviation\\n0     Alabama           AL\\n1      Alaska           AK\\n2     Arizona           AZ\\n3    Arkansas           AR\\n4  California           CA\\nGiven this information, say we want to compute a relatively straightforward result:\\nrank US states and territories by their 2010 population density. We clearly have the\\ndata here to find this result, but we’ll have to combine the datasets to get it.\\nWe’ll start with a many-to-one merge that will give us the full state name within the\\npopulation DataFrame. We want to merge based on the state/region column of pop,\\nand the abbreviation column of abbrevs. We’ll use how='outer' to make sure no\\ndata is thrown away due to mismatched labels.\\nIn[21]: merged = pd.merge(pop, abbrevs, how='outer',\\n                          left_on='state/region', right_on='abbreviation')\\n        merged = merged.drop('abbreviation', 1) # drop duplicate info\\n        merged.head()\\nOut[21]:   state/region     ages  year  population    state\\n         0           AL  under18  2012   1117489.0  Alabama\\n         1           AL    total  2012   4817528.0  Alabama\\n         2           AL  under18  2010   1130966.0  Alabama\\n         3           AL    total  2010   4785570.0  Alabama\\n         4           AL  under18  2011   1125763.0  Alabama\\nCombining Datasets: Merge and Join \\n| \\n155\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 173}, page_content=\"Let’s double-check whether there were any mismatches here, which we can do by\\nlooking for rows with nulls:\\nIn[22]: merged.isnull().any()\\nOut[22]: state/region    False\\n         ages            False\\n         year            False\\n         population       True\\n         state            True\\n         dtype: bool\\nSome of the population info is null; let’s figure out which these are!\\nIn[23]: merged[merged['population'].isnull()].head()\\nOut[23]:      state/region     ages  year  population state\\n         2448           PR  under18  1990         NaN   NaN\\n         2449           PR    total  1990         NaN   NaN\\n         2450           PR    total  1991         NaN   NaN\\n         2451           PR  under18  1991         NaN   NaN\\n         2452           PR    total  1993         NaN   NaN\\nIt appears that all the null population values are from Puerto Rico prior to the year\\n2000; this is likely due to this data not being available from the original source.\\nMore importantly, we see also that some of the new state entries are also null, which\\nmeans that there was no corresponding entry in the abbrevs key! Let’s figure out\\nwhich regions lack this match:\\nIn[24]: merged.loc[merged['state'].isnull(), 'state/region'].unique()\\nOut[24]: array(['PR', 'USA'], dtype=object)\\nWe can quickly infer the issue: our population data includes entries for Puerto Rico\\n(PR) and the United States as a whole (USA), while these entries do not appear in the\\nstate abbreviation key. We can fix these quickly by filling in appropriate entries:\\nIn[25]: merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'\\n        merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'\\n        merged.isnull().any()\\nOut[25]: state/region    False\\n         ages            False\\n         year            False\\n         population       True\\n         state           False\\n         dtype: bool\\nNo more nulls in the state column: we’re all set!\\nNow we can merge the result with the area data using a similar procedure. Examining\\nour results, we will want to join on the state column in both:\\n156 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 174}, page_content='In[26]: final = pd.merge(merged, areas, on=\\'state\\', how=\\'left\\')\\n        final.head()\\nOut[26]:   state/region     ages  year  population    state  area (sq. mi)\\n         0           AL  under18  2012   1117489.0  Alabama        52423.0\\n         1           AL    total  2012   4817528.0  Alabama        52423.0\\n         2           AL  under18  2010   1130966.0  Alabama        52423.0\\n         3           AL    total  2010   4785570.0  Alabama        52423.0\\n         4           AL  under18  2011   1125763.0  Alabama        52423.0\\nAgain, let’s check for nulls to see if there were any mismatches:\\nIn[27]: final.isnull().any()\\nOut[27]: state/region     False\\n         ages             False\\n         year             False\\n         population        True\\n         state            False\\n         area (sq. mi)     True\\n         dtype: bool\\nThere are nulls in the area column; we can take a look to see which regions were\\nignored here:\\nIn[28]: final[\\'state\\'][final[\\'area (sq. mi)\\'].isnull()].unique()\\nOut[28]: array([\\'United States\\'], dtype=object)\\nWe see that our areas DataFrame does not contain the area of the United States as a\\nwhole. We could insert the appropriate value (using the sum of all state areas, for\\ninstance), but in this case we’ll just drop the null values because the population den‐\\nsity of the entire United States is not relevant to our current discussion:\\nIn[29]: final.dropna(inplace=True)\\n        final.head()\\nOut[29]:   state/region     ages  year  population    state  area (sq. mi)\\n         0           AL  under18  2012   1117489.0  Alabama        52423.0\\n         1           AL    total  2012   4817528.0  Alabama        52423.0\\n         2           AL  under18  2010   1130966.0  Alabama        52423.0\\n         3           AL    total  2010   4785570.0  Alabama        52423.0\\n         4           AL  under18  2011   1125763.0  Alabama        52423.0\\nNow we have all the data we need. To answer the question of interest, let’s first select\\nthe portion of the data corresponding with the year 2000, and the total population.\\nWe’ll use the query() function to do this quickly (this requires the numexpr package\\nto be installed; see “High-Performance Pandas: eval() and query()” on page 208):\\nIn[30]: data2010 = final.query(\"year == 2010 & ages == \\'total\\'\")\\n        data2010.head()\\nOut[30]:     state/region   ages  year  population       state  area (sq. mi)\\n         3             AL  total  2010   4785570.0     Alabama        52423.0\\n         91            AK  total  2010    713868.0      Alaska       656425.0\\nCombining Datasets: Merge and Join \\n| \\n157'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 175}, page_content=\"101           AZ  total  2010   6408790.0     Arizona       114006.0\\n         189           AR  total  2010   2922280.0    Arkansas        53182.0\\n         197           CA  total  2010  37333601.0  California       163707.0\\nNow let’s compute the population density and display it in order. We’ll start by rein‐\\ndexing our data on the state, and then compute the result:\\nIn[31]: data2010.set_index('state', inplace=True)\\n        density = data2010['population'] / data2010['area (sq. mi)']\\nIn[32]: density.sort_values(ascending=False, inplace=True)\\n        density.head()\\nOut[32]: state\\n         District of Columbia    8898.897059\\n         Puerto Rico             1058.665149\\n         New Jersey              1009.253268\\n         Rhode Island             681.339159\\n         Connecticut              645.600649\\n         dtype: float64\\nThe result is a ranking of US states plus Washington, DC, and Puerto Rico in order of\\ntheir 2010 population density, in residents per square mile. We can see that by far the\\ndensest region in this dataset is Washington, DC (i.e., the District of Columbia);\\namong states, the densest is New Jersey.\\nWe can also check the end of the list:\\nIn[33]: density.tail()\\nOut[33]: state\\n         South Dakota    10.583512\\n         North Dakota     9.537565\\n         Montana          6.736171\\n         Wyoming          5.768079\\n         Alaska           1.087509\\n         dtype: float64\\nWe see that the least dense state, by far, is Alaska, averaging slightly over one resident\\nper square mile.\\nThis type of messy data merging is a common task when one is trying to answer\\nquestions using real-world data sources. I hope that this example has given you an\\nidea of the ways you can combine tools we’ve covered in order to gain insight from\\nyour data!\\nAggregation and Grouping\\nAn essential piece of analysis of large data is efficient summarization: computing\\naggregations like sum(), mean(), median(), min(), and max(), in which a single num‐\\nber gives insight into the nature of a potentially large dataset. In this section, we’ll\\n158 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 176}, page_content=\"explore aggregations in Pandas, from simple operations akin to what we’ve seen on\\nNumPy arrays, to more sophisticated operations based on the concept of a groupby.\\nPlanets Data\\nHere we will use the Planets dataset, available via the Seaborn package (see “Visuali‐\\nzation with Seaborn” on page 311). It gives information on planets that astronomers\\nhave discovered around other stars (known as extrasolar planets or exoplanets for\\nshort). It can be downloaded with a simple Seaborn command:\\nIn[2]: import seaborn as sns\\n       planets = sns.load_dataset('planets')\\n       planets.shape\\nOut[2]: (1035, 6)\\nIn[3]: planets.head()\\nOut[3]:    method           number  orbital_period  mass   distance  year\\n        0  Radial Velocity  1       269.300         7.10   77.40     2006\\n        1  Radial Velocity  1       874.774         2.21   56.95     2008\\n        2  Radial Velocity  1       763.000         2.60   19.84     2011\\n        3  Radial Velocity  1       326.030         19.40  110.62    2007\\n        4  Radial Velocity  1       516.220         10.50  119.47    2009\\nThis has some details on the 1,000+ exoplanets discovered up to 2014.\\nSimple Aggregation in Pandas\\nEarlier we explored some of the data aggregations available for NumPy arrays\\n(“Aggregations: Min, Max, and Everything in Between” on page 58). As with a one-\\ndimensional NumPy array, for a Pandas Series the aggregates return a single value:\\nIn[4]: rng = np.random.RandomState(42)\\n       ser = pd.Series(rng.rand(5))\\n       ser\\nOut[4]: 0    0.374540\\n        1    0.950714\\n        2    0.731994\\n        3    0.598658\\n        4    0.156019\\n        dtype: float64\\nIn[5]: ser.sum()\\nOut[5]: 2.8119254917081569\\nIn[6]: ser.mean()\\nOut[6]: 0.56238509834163142\\nFor a DataFrame, by default the aggregates return results within each column:\\nAggregation and Grouping \\n| \\n159\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 177}, page_content=\"In[7]: df = pd.DataFrame({'A': rng.rand(5),\\n                          'B': rng.rand(5)})\\n       df\\nOut[7]:           A         B\\n        0  0.155995  0.020584\\n        1  0.058084  0.969910\\n        2  0.866176  0.832443\\n        3  0.601115  0.212339\\n        4  0.708073  0.181825\\nIn[8]: df.mean()\\nOut[8]: A    0.477888\\n        B    0.443420\\n        dtype: float64\\nBy specifying the axis argument, you can instead aggregate within each row:\\nIn[9]: df.mean(axis='columns')\\nOut[9]: 0    0.088290\\n        1    0.513997\\n        2    0.849309\\n        3    0.406727\\n        4    0.444949\\n        dtype: float64\\nPandas Series and DataFrames include all of the common aggregates mentioned in\\n“Aggregations: Min, Max, and Everything in Between” on page 58; in addition, there\\nis a convenience method describe() that computes several common aggregates for\\neach column and returns the result. Let’s use this on the Planets data, for now drop‐\\nping rows with missing values:\\nIn[10]: planets.dropna().describe()\\nOut[10]:           number  orbital_period        mass    distance         year\\n         count  498.00000      498.000000  498.000000  498.000000   498.000000\\n         mean     1.73494      835.778671    2.509320   52.068213  2007.377510\\n         std      1.17572     1469.128259    3.636274   46.596041     4.167284\\n         min      1.00000        1.328300    0.003600    1.350000  1989.000000\\n         25%      1.00000       38.272250    0.212500   24.497500  2005.000000\\n         50%      1.00000      357.000000    1.245000   39.940000  2009.000000\\n         75%      2.00000      999.600000    2.867500   59.332500  2011.000000\\n         max      6.00000    17337.500000   25.000000  354.000000  2014.000000\\nThis can be a useful way to begin understanding the overall properties of a dataset.\\nFor example, we see in the year column that although exoplanets were discovered as\\nfar back as 1989, half of all known exoplanets were not discovered until 2010 or after.\\nThis is largely thanks to the Kepler mission, which is a space-based telescope specifi‐\\ncally designed for finding eclipsing planets around other stars.\\nTable 3-3 summarizes some other built-in Pandas aggregations.\\n160 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 178}, page_content='Table 3-3. Listing of Pandas aggregation methods\\nAggregation\\nDescription\\ncount()\\nTotal number of items\\nfirst(), last()\\nFirst and last item\\nmean(), median() Mean and median\\nmin(), max()\\nMinimum and maximum\\nstd(), var()\\nStandard deviation and variance\\nmad()\\nMean absolute deviation\\nprod()\\nProduct of all items\\nsum()\\nSum of all items\\nThese are all methods of DataFrame and Series objects.\\nTo go deeper into the data, however, simple aggregates are often not enough. The\\nnext level of data summarization is the groupby operation, which allows you to\\nquickly and efficiently compute aggregates on subsets of data.\\nGroupBy: Split, Apply, Combine\\nSimple aggregations can give you a flavor of your dataset, but often we would prefer\\nto aggregate conditionally on some label or index: this is implemented in the so-\\ncalled groupby operation. The name “group by” comes from a command in the SQL\\ndatabase language, but it is perhaps more illuminative to think of it in the terms first\\ncoined by Hadley Wickham of Rstats fame: split, apply, combine.\\nSplit, apply, combine\\nA canonical example of this split-apply-combine operation, where the “apply” is a\\nsummation aggregation, is illustrated in Figure 3-1.\\nFigure 3-1 makes clear what the GroupBy accomplishes:\\n• The split step involves breaking up and grouping a DataFrame depending on the\\nvalue of the specified key.\\n• The apply step involves computing some function, usually an aggregate, transfor‐\\nmation, or filtering, within the individual groups.\\n• The combine step merges the results of these operations into an output array.\\nAggregation and Grouping \\n| \\n161'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 179}, page_content=\"Figure 3-1. A visual representation of a groupby operation\\nWhile we could certainly do this manually using some combination of the masking,\\naggregation, and merging commands covered earlier, it’s important to realize that the\\nintermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can\\n(often) do this in a single pass over the data, updating the sum, mean, count, min, or\\nother aggregate for each group along the way. The power of the GroupBy is that it\\nabstracts away these steps: the user need not think about how the computation is\\ndone under the hood, but rather thinks about the operation as a whole.\\nAs a concrete example, let’s take a look at using Pandas for the computation shown in\\nFigure 3-1. We’ll start by creating the input DataFrame:\\nIn[11]: df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\\n                           'data': range(6)}, columns=['key', 'data'])\\n        df\\nOut[11]:   key  data\\n         0   A     0\\n         1   B     1\\n         2   C     2\\n         3   A     3\\n         4   B     4\\n         5   C     5\\nWe can compute the most basic split-apply-combine operation with the groupby()\\nmethod of DataFrames, passing the name of the desired key column:\\nIn[12]: df.groupby('key')\\nOut[12]: <pandas.core.groupby.DataFrameGroupBy object at 0x117272160>\\n162 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 180}, page_content=\"Notice that what is returned is not a set of DataFrames, but a DataFrameGroupBy\\nobject. This object is where the magic is: you can think of it as a special view of the\\nDataFrame, which is poised to dig into the groups but does no actual computation\\nuntil the aggregation is applied. This “lazy evaluation” approach means that common\\naggregates can be implemented very efficiently in a way that is almost transparent to\\nthe user.\\nTo produce a result, we can apply an aggregate to this DataFrameGroupBy object,\\nwhich will perform the appropriate apply/combine steps to produce the desired\\nresult:\\nIn[13]: df.groupby('key').sum()\\nOut[13]:      data\\n         key\\n         A       3\\n         B       5\\n         C       7\\nThe sum() method is just one possibility here; you can apply virtually any common\\nPandas or NumPy aggregation function, as well as virtually any valid DataFrame\\noperation, as we will see in the following discussion.\\nThe GroupBy object\\nThe GroupBy object is a very flexible abstraction. In many ways, you can simply treat\\nit as if it’s a collection of DataFrames, and it does the difficult things under the hood.\\nLet’s see some examples using the Planets data.\\nPerhaps the most important operations made available by a GroupBy are aggregate,\\nfilter, transform, and apply. We’ll discuss each of these more fully in “Aggregate, filter,\\ntransform, apply” on page 165, but before that let’s introduce some of the other func‐\\ntionality that can be used with the basic GroupBy operation.\\nColumn indexing.    The GroupBy object supports column indexing in the same way as\\nthe DataFrame, and returns a modified GroupBy object. For example:\\nIn[14]: planets.groupby('method')\\nOut[14]: <pandas.core.groupby.DataFrameGroupBy object at 0x1172727b8>\\nIn[15]: planets.groupby('method')['orbital_period']\\nOut[15]: <pandas.core.groupby.SeriesGroupBy object at 0x117272da0>\\nHere we’ve selected a particular Series group from the original DataFrame group by\\nreference to its column name. As with the GroupBy object, no computation is done\\nuntil we call some aggregate on the object:\\nIn[16]: planets.groupby('method')['orbital_period'].median()\\nAggregation and Grouping \\n| \\n163\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 181}, page_content='Out[16]: method\\n         Astrometry                         631.180000\\n         Eclipse Timing Variations         4343.500000\\n         Imaging                          27500.000000\\n         Microlensing                      3300.000000\\n         Orbital Brightness Modulation        0.342887\\n         Pulsar Timing                       66.541900\\n         Pulsation Timing Variations       1170.000000\\n         Radial Velocity                    360.200000\\n         Transit                              5.714932\\n         Transit Timing Variations           57.011000\\n         Name: orbital_period, dtype: float64\\nThis gives an idea of the general scale of orbital periods (in days) that each method is\\nsensitive to.\\nIteration over groups.    The GroupBy object supports direct iteration over the groups,\\nreturning each group as a Series or DataFrame:\\nIn[17]: for (method, group) in planets.groupby(\\'method\\'):\\n            print(\"{0:30s} shape={1}\".format(method, group.shape))\\nAstrometry                     shape=(2, 6)\\nEclipse Timing Variations      shape=(9, 6)\\nImaging                        shape=(38, 6)\\nMicrolensing                   shape=(23, 6)\\nOrbital Brightness Modulation  shape=(3, 6)\\nPulsar Timing                  shape=(5, 6)\\nPulsation Timing Variations    shape=(1, 6)\\nRadial Velocity                shape=(553, 6)\\nTransit                        shape=(397, 6)\\nTransit Timing Variations      shape=(4, 6)\\nThis can be useful for doing certain things manually, though it is often much faster to\\nuse the built-in apply functionality, which we will discuss momentarily.\\nDispatch methods.    Through some Python class magic, any method not explicitly\\nimplemented by the GroupBy object will be passed through and called on the groups,\\nwhether they are DataFrame or Series objects. For example, you can use the \\ndescribe() method of DataFrames to perform a set of aggregations that describe each\\ngroup in the data:\\nIn[18]: planets.groupby(\\'method\\')[\\'year\\'].describe().unstack()\\nOut[18]:\\n                               count         mean       std     min      25%  \\\\\\\\\\nmethod\\nAstrometry                       2.0  2011.500000  2.121320  2010.0  2010.75\\nEclipse Timing Variations        9.0  2010.000000  1.414214  2008.0  2009.00\\nImaging                         38.0  2009.131579  2.781901  2004.0  2008.00\\nMicrolensing                    23.0  2009.782609  2.859697  2004.0  2008.00\\nOrbital Brightness Modulation    3.0  2011.666667  1.154701  2011.0  2011.00\\n164 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 182}, page_content=\"Pulsar Timing                    5.0  1998.400000  8.384510  1992.0  1992.00\\nPulsation Timing Variations      1.0  2007.000000       NaN  2007.0  2007.00\\nRadial Velocity                553.0  2007.518987  4.249052  1989.0  2005.00\\nTransit                        397.0  2011.236776  2.077867  2002.0  2010.00\\nTransit Timing Variations        4.0  2012.500000  1.290994  2011.0  2011.75\\n                                  50%      75%     max\\nmethod\\nAstrometry                     2011.5  2012.25  2013.0\\nEclipse Timing Variations      2010.0  2011.00  2012.0\\nImaging                        2009.0  2011.00  2013.0\\nMicrolensing                   2010.0  2012.00  2013.0\\nOrbital Brightness Modulation  2011.0  2012.00  2013.0\\nPulsar Timing                  1994.0  2003.00  2011.0\\nPulsation Timing Variations    2007.0  2007.00  2007.0\\nRadial Velocity                2009.0  2011.00  2014.0\\nTransit                        2012.0  2013.00  2014.0\\nTransit Timing Variations      2012.5  2013.25  2014.0\\nLooking at this table helps us to better understand the data: for example, the vast\\nmajority of planets have been discovered by the Radial Velocity and Transit methods,\\nthough the latter only became common (due to new, more accurate telescopes) in the\\nlast decade. The newest methods seem to be Transit Timing Variation and Orbital\\nBrightness Modulation, which were not used to discover a new planet until 2011.\\nThis is just one example of the utility of dispatch methods. Notice that they are\\napplied to each individual group, and the results are then combined within GroupBy\\nand returned. Again, any valid DataFrame/Series method can be used on the corre‐\\nsponding GroupBy object, which allows for some very flexible and powerful\\noperations!\\nAggregate, filter, transform, apply\\nThe preceding discussion focused on aggregation for the combine operation, but\\nthere are more options available. In particular, GroupBy objects have aggregate(),\\nfilter(), transform(), and apply() methods that efficiently implement a variety of\\nuseful operations before combining the grouped data.\\nFor the purpose of the following subsections, we’ll use this DataFrame:\\nIn[19]: rng = np.random.RandomState(0)\\n        df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\\n                           'data1': range(6),\\n                           'data2': rng.randint(0, 10, 6)},\\n                           columns = ['key', 'data1', 'data2'])\\n        df\\nOut[19]:   key  data1  data2\\n         0   A      0      5\\n         1   B      1      0\\n         2   C      2      3\\nAggregation and Grouping \\n| \\n165\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 183}, page_content=\"3   A      3      3\\n         4   B      4      7\\n         5   C      5      9\\nAggregation.    We’re now familiar with GroupBy aggregations with sum(), median(),\\nand the like, but the aggregate() method allows for even more flexibility. It can take\\na string, a function, or a list thereof, and compute all the aggregates at once. Here is a\\nquick example combining all these:\\nIn[20]: df.groupby('key').aggregate(['min', np.median, max])\\nOut[20]:       data1            data2\\n               min median max   min median max\\n         key\\n         A       0    1.5   3     3    4.0   5\\n         B       1    2.5   4     0    3.5   7\\n         C       2    3.5   5     3    6.0   9\\nAnother useful pattern is to pass a dictionary mapping column names to operations\\nto be applied on that column:\\nIn[21]: df.groupby('key').aggregate({'data1': 'min',\\n                                     'data2': 'max'})\\nOut[21]:      data1  data2\\n         key\\n         A        0      5\\n         B        1      7\\n         C        2      9\\nFiltering.    A filtering operation allows you to drop data based on the group proper‐\\nties. For example, we might want to keep all groups in which the standard deviation is\\nlarger than some critical value:\\nIn[22]:\\ndef filter_func(x):\\n    return x['data2'].std() > 4\\nprint(df); print(df.groupby('key').std());\\nprint(df.groupby('key').filter(filter_func))\\ndf                      df.groupby('key').std()\\n   key  data1  data2    key      data1     data2\\n0   A      0      5     A    2.12132  1.414214\\n1   B      1      0     B    2.12132  4.949747\\n2   C      2      3     C    2.12132  4.242641\\n3   A      3      3\\n4   B      4      7\\n5   C      5      9\\ndf.groupby('key').filter(filter_func)\\n  key  data1  data2\\n1   B      1      0\\n166 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 184}, page_content=\"2   C      2      3\\n4   B      4      7\\n5   C      5      9\\nThe filter() function should return a Boolean value specifying whether the group\\npasses the filtering. Here because group A does not have a standard deviation greater\\nthan 4, it is dropped from the result.\\nTransformation.    While aggregation must return a reduced version of the data, trans‐\\nformation can return some transformed version of the full data to recombine. For\\nsuch a transformation, the output is the same shape as the input. A common example\\nis to center the data by subtracting the group-wise mean:\\nIn[23]: df.groupby('key').transform(lambda x: x - x.mean())\\nOut[23]:   data1  data2\\n         0   -1.5    1.0\\n         1   -1.5   -3.5\\n         2   -1.5   -3.0\\n         3    1.5   -1.0\\n         4    1.5    3.5\\n         5    1.5    3.0\\nThe apply() method.    The apply() method lets you apply an arbitrary function to the\\ngroup results. The function should take a DataFrame, and return either a Pandas\\nobject (e.g., DataFrame, Series) or a scalar; the combine operation will be tailored to\\nthe type of output returned.\\nFor example, here is an apply() that normalizes the first column by the sum of the\\nsecond:\\nIn[24]: def norm_by_data2(x):\\n            # x is a DataFrame of group values\\n            x['data1'] /= x['data2'].sum()\\n            return x\\n        print(df); print(df.groupby('key').apply(norm_by_data2))\\ndf                     df.groupby('key').apply(norm_by_data2)\\n  key  data1  data2       key     data1  data2\\n0   A      0      5     0   A  0.000000      5\\n1   B      1      0     1   B  0.142857      0\\n2   C      2      3     2   C  0.166667      3\\n3   A      3      3     3   A  0.375000      3\\n4   B      4      7     4   B  0.571429      7\\n5   C      5      9     5   C  0.416667      9\\napply() within a GroupBy is quite flexible: the only criterion is that the function takes\\na DataFrame and returns a Pandas object or scalar; what you do in the middle is up to\\nyou!\\nAggregation and Grouping \\n| \\n167\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 185}, page_content=\"Specifying the split key\\nIn the simple examples presented before, we split the DataFrame on a single column\\nname. This is just one of many options by which the groups can be defined, and we’ll\\ngo through some other options for group specification here.\\nA list, array, series, or index providing the grouping keys.    The key can be any series or list\\nwith a length matching that of the DataFrame. For example:\\nIn[25]: L = [0, 1, 0, 1, 2, 0]\\nprint(df); print(df.groupby(L).sum())\\ndf                     df.groupby(L).sum()\\n  key  data1  data2        data1  data2\\n0   A      0      5     0      7     17\\n1   B      1      0     1      4      3\\n2   C      2      3     2      4      7\\n3   A      3      3\\n4   B      4      7\\n5   C      5      9\\nOf course, this means there’s another, more verbose way of accomplishing the\\ndf.groupby('key') from before:\\nIn[26]: print(df); print(df.groupby(df['key']).sum())\\ndf                        df.groupby(df['key']).sum()\\n  key  data1  data2            data1  data2\\n0   A      0      5       A        3      8\\n1   B      1      0       B        5      7\\n2   C      2      3       C        7     12\\n3   A      3      3\\n4   B      4      7\\n5   C      5      9\\nA dictionary or series mapping index to group.    Another method is to provide a dictionary\\nthat maps index values to the group keys:\\nIn[27]: df2 = df.set_index('key')\\n        mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}\\n        print(df2); print(df2.groupby(mapping).sum())\\ndf2                       df2.groupby(mapping).sum()\\nkey  data1  data2                    data1  data2\\nA        0      5         consonant     12     19\\nB        1      0         vowel          3      8\\nC        2      3\\nA        3      3\\nB        4      7\\nC        5      9\\n168 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 186}, page_content=\"Any Python function.    Similar to mapping, you can pass any Python function that will\\ninput the index value and output the group:\\nIn[28]: print(df2); print(df2.groupby(str.lower).mean())\\ndf2                       df2.groupby(str.lower).mean()\\nkey  data1  data2            data1  data2\\nA        0      5         a    1.5    4.0\\nB        1      0         b    2.5    3.5\\nC        2      3         c    3.5    6.0\\nA        3      3\\nB        4      7\\nC        5      9\\nA list of valid keys.    Further, any of the preceding key choices can be combined to\\ngroup on a multi-index:\\nIn[29]: df2.groupby([str.lower, mapping]).mean()\\nOut[29]:              data1  data2\\n         a vowel        1.5    4.0\\n         b consonant    2.5    3.5\\n         c consonant    3.5    6.0\\nGrouping example\\nAs an example of this, in a couple lines of Python code we can put all these together\\nand count discovered planets by method and by decade:\\nIn[30]: decade = 10 * (planets['year'] // 10)\\n        decade = decade.astype(str) + 's'\\n        decade.name = 'decade'\\n        planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)\\nOut[30]: decade                         1980s  1990s  2000s  2010s\\n         method\\n         Astrometry                       0.0    0.0    0.0    2.0\\n         Eclipse Timing Variations        0.0    0.0    5.0   10.0\\n         Imaging                          0.0    0.0   29.0   21.0\\n         Microlensing                     0.0    0.0   12.0   15.0\\n         Orbital Brightness Modulation    0.0    0.0    0.0    5.0\\n         Pulsar Timing                    0.0    9.0    1.0    1.0\\n         Pulsation Timing Variations      0.0    0.0    1.0    0.0\\n         Radial Velocity                  1.0   52.0  475.0  424.0\\n         Transit                          0.0    0.0   64.0  712.0\\n         Transit Timing Variations        0.0    0.0    0.0    9.0\\nThis shows the power of combining many of the operations we’ve discussed up to this\\npoint when looking at realistic datasets. We immediately gain a coarse understanding\\nof when and how planets have been discovered over the past several decades!\\nAggregation and Grouping \\n| \\n169\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 187}, page_content=\"Here I would suggest digging into these few lines of code, and evaluating the individ‐\\nual steps to make sure you understand exactly what they are doing to the result. It’s\\ncertainly a somewhat complicated example, but understanding these pieces will give\\nyou the means to similarly explore your own data.\\nPivot Tables\\nWe have seen how the GroupBy abstraction lets us explore relationships within a data‐\\nset. A pivot table is a similar operation that is commonly seen in spreadsheets and\\nother programs that operate on tabular data. The pivot table takes simple column-\\nwise data as input, and groups the entries into a two-dimensional table that provides\\na multidimensional summarization of the data. The difference between pivot tables\\nand GroupBy can sometimes cause confusion; it helps me to think of pivot tables as\\nessentially a multidimensional version of GroupBy aggregation. That is, you split-\\napply-combine, but both the split and the combine happen across not a one-\\ndimensional index, but across a two-dimensional grid.\\nMotivating Pivot Tables\\nFor the examples in this section, we’ll use the database of passengers on the Titanic,\\navailable through the Seaborn library (see “Visualization with Seaborn” on page 311):\\nIn[1]: import numpy as np\\n       import pandas as pd\\n       import seaborn as sns\\n       titanic = sns.load_dataset('titanic')\\nIn[2]: titanic.head()\\nOut[2]:\\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\\\\\\\\n0         0       3    male  22.0      1      0   7.2500        S  Third\\n1         1       1  female  38.0      1      0  71.2833        C  First\\n2         1       3  female  26.0      0      0   7.9250        S  Third\\n3         1       1  female  35.0      1      0  53.1000        S  First\\n4         0       3    male  35.0      0      0   8.0500        S  Third\\n     who adult_male deck  embark_town alive  alone\\n0    man       True  NaN  Southampton    no  False\\n1  woman      False    C    Cherbourg   yes  False\\n2  woman      False  NaN  Southampton   yes   True\\n3  woman      False    C  Southampton   yes  False\\n4    man       True  NaN  Southampton    no   True\\nThis contains a wealth of information on each passenger of that ill-fated voyage,\\nincluding gender, age, class, fare paid, and much more.\\n170 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 188}, page_content=\"Pivot Tables by Hand\\nTo start learning more about this data, we might begin by grouping it according to\\ngender, survival status, or some combination thereof. If you have read the previous\\nsection, you might be tempted to apply a GroupBy operation—for example, let’s look\\nat survival rate by gender:\\nIn[3]: titanic.groupby('sex')[['survived']].mean()\\nOut[3]:        survived\\n        sex\\n        female  0.742038\\n        male    0.188908\\nThis immediately gives us some insight: overall, three of every four females on board\\nsurvived, while only one in five males survived!\\nThis is useful, but we might like to go one step deeper and look at survival by both sex\\nand, say, class. Using the vocabulary of GroupBy, we might proceed using something\\nlike this: we group by class and gender, select survival, apply a mean aggregate, com‐\\nbine the resulting groups, and then unstack the hierarchical index to reveal the hidden\\nmultidimensionality. In code:\\nIn[4]: titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()\\nOut[4]: class      First    Second     Third\\n        sex\\n        female  0.968085  0.921053  0.500000\\n        male    0.368852  0.157407  0.135447\\nThis gives us a better idea of how both gender and class affected survival, but the\\ncode is starting to look a bit garbled. While each step of this pipeline makes sense in\\nlight of the tools we’ve previously discussed, the long string of code is not particularly\\neasy to read or use. This two-dimensional GroupBy is common enough that Pandas\\nincludes a convenience routine, pivot_table, which succinctly handles this type of\\nmultidimensional aggregation.\\nPivot Table Syntax\\nHere is the equivalent to the preceding operation using the pivot_table method of\\nDataFrames:\\nIn[5]: titanic.pivot_table('survived', index='sex', columns='class')\\nOut[5]: class      First    Second     Third\\n        sex\\n        female  0.968085  0.921053  0.500000\\n        male    0.368852  0.157407  0.135447\\nThis is eminently more readable than the GroupBy approach, and produces the same\\nresult. As you might expect of an early 20th-century transatlantic cruise, the survival\\nPivot Tables \\n| \\n171\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 189}, page_content=\"gradient favors both women and higher classes. First-class women survived with near\\ncertainty (hi, Rose!), while only one in ten third-class men survived (sorry, Jack!).\\nMultilevel pivot tables\\nJust as in the GroupBy, the grouping in pivot tables can be specified with multiple lev‐\\nels, and via a number of options. For example, we might be interested in looking at\\nage as a third dimension. We’ll bin the age using the pd.cut function:\\nIn[6]: age = pd.cut(titanic['age'], [0, 18, 80])\\n       titanic.pivot_table('survived', ['sex', age], 'class')\\nOut[6]:   class               First    Second     Third\\n          sex    age\\n          female (0, 18]   0.909091  1.000000  0.511628\\n                 (18, 80]  0.972973  0.900000  0.423729\\n          male   (0, 18]   0.800000  0.600000  0.215686\\n                 (18, 80]  0.375000  0.071429  0.133663\\nWe can apply this same strategy when working with the columns as well; let’s add info\\non the fare paid using pd.qcut to automatically compute quantiles:\\nIn[7]: fare = pd.qcut(titanic['fare'], 2)\\n       titanic.pivot_table('survived', ['sex', age], [fare, 'class'])\\nOut[7]:\\nfare            [0, 14.454]\\nclass                 First    Second     Third      \\\\\\\\\\nsex    age\\nfemale (0, 18]          NaN  1.000000  0.714286\\n       (18, 80]         NaN  0.880000  0.444444\\nmale   (0, 18]          NaN  0.000000  0.260870\\n       (18, 80]         0.0  0.098039  0.125000\\nfare            (14.454, 512.329]\\nclass                 First    Second     Third\\nsex    age\\nfemale (0, 18]     0.909091  1.000000  0.318182\\n       (18, 80]    0.972973  0.914286  0.391304\\nmale   (0, 18]     0.800000  0.818182  0.178571\\n       (18, 80]    0.391304  0.030303  0.192308\\nThe result is a four-dimensional aggregation with hierarchical indices (see “Hierarch‐\\nical Indexing” on page 128), shown in a grid demonstrating the relationship between\\nthe values.\\n172 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 190}, page_content='Additional pivot table options\\nThe full call signature of the pivot_table method of DataFrames is as follows:\\n# call signature as of Pandas 0.18\\nDataFrame.pivot_table(data, values=None, index=None, columns=None,\\n                      aggfunc=\\'mean\\', fill_value=None, margins=False,\\n                      dropna=True, margins_name=\\'All\\')\\nWe’ve already seen examples of the first three arguments; here we’ll take a quick look\\nat the remaining ones. Two of the options, fill_value and dropna, have to do with\\nmissing data and are fairly straightforward; we will not show examples of them here.\\nThe aggfunc keyword controls what type of aggregation is applied, which is a mean\\nby default. As in the GroupBy, the aggregation specification can be a string represent‐\\ning one of several common choices (\\'sum\\', \\'mean\\', \\'count\\', \\'min\\', \\'max\\', etc.) or a\\nfunction that implements an aggregation (np.sum(), min(), sum(), etc.). Additionally,\\nit can be specified as a dictionary mapping a column to any of the above desired\\noptions:\\nIn[8]: titanic.pivot_table(index=\\'sex\\', columns=\\'class\\',\\n                           aggfunc={\\'survived\\':sum, \\'fare\\':\\'mean\\'})\\nOut[8]:              fare                           survived\\n        class        First     Second      Third    First Second Third\\n        sex\\n        female  106.125798  21.970121  16.118810     91.0   70.0  72.0\\n        male     67.226127  19.741782  12.661633     45.0   17.0  47.0\\nNotice also here that we’ve omitted the values keyword; when you’re specifying a\\nmapping for aggfunc, this is determined automatically.\\nAt times it’s useful to compute totals along each grouping. This can be done via the\\nmargins keyword:\\nIn[9]: titanic.pivot_table(\\'survived\\', index=\\'sex\\', columns=\\'class\\', margins=True)\\nOut[9]: class      First    Second     Third       All\\n        sex\\n        female  0.968085  0.921053  0.500000  0.742038\\n        male    0.368852  0.157407  0.135447  0.188908\\n        All     0.629630  0.472826  0.242363  0.383838\\nHere this automatically gives us information about the class-agnostic survival rate by\\ngender, the gender-agnostic survival rate by class, and the overall survival rate of 38%.\\nThe margin label can be specified with the margins_name keyword, which defaults to\\n\"All\".\\nPivot Tables \\n| \\n173'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 191}, page_content=\"Example: Birthrate Data\\nAs a more interesting example, let’s take a look at the freely available data on births in\\nthe United States, provided by the Centers for Disease Control (CDC). This data can\\nbe \\nfound \\nat \\nhttps://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/\\nbirths.csv (this dataset has been analyzed rather extensively by Andrew Gelman and\\nhis group; see, for example, this blog post):\\nIn[10]:\\n# shell command to download the data:\\n# !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/\\n# master/births.csv\\nIn[11]: births = pd.read_csv('births.csv')\\nTaking a look at the data, we see that it’s relatively simple—it contains the number of\\nbirths grouped by date and gender:\\nIn[12]: births.head()\\nOut[12]:   year  month day gender  births\\n         0  1969      1   1      F    4046\\n         1  1969      1   1      M    4440\\n         2  1969      1   2      F    4454\\n         3  1969      1   2      M    4548\\n         4  1969      1   3      F    4548\\nWe can start to understand this data a bit more by using a pivot table. Let’s add a dec‐\\nade column, and take a look at male and female births as a function of decade:\\nIn[13]:\\nbirths['decade'] = 10 * (births['year'] // 10)\\nbirths.pivot_table('births', index='decade', columns='gender', aggfunc='sum')\\nOut[13]: gender         F         M\\n         decade\\n         1960     1753634   1846572\\n         1970    16263075  17121550\\n         1980    18310351  19243452\\n         1990    19479454  20420553\\n         2000    18229309  19106428\\nWe immediately see that male births outnumber female births in every decade. To see\\nthis trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual‐\\nize the total number of births by year (Figure 3-2; see Chapter 4 for a discussion of\\nplotting with Matplotlib):\\nIn[14]:\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nsns.set()  # use Seaborn styles\\nbirths.pivot_table('births', index='year', columns='gender', aggfunc='sum').plot()\\nplt.ylabel('total births per year');\\n174 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 192}, page_content=\"1 You can learn more about sigma-clipping operations in a book I coauthored with Željko Ivezić, Andrew J.\\nConnolly, and Alexander Gray: Statistics, Data Mining, and Machine Learning in Astronomy: A Practical\\nPython Guide for the Analysis of Survey Data (Princeton University Press, 2014).\\nFigure 3-2. Total number of US births by year and gender\\nWith a simple pivot table and plot() method, we can immediately see the annual\\ntrend in births by gender. By eye, it appears that over the past 50 years male births\\nhave outnumbered female births by around 5%.\\nFurther data exploration\\nThough this doesn’t necessarily relate to the pivot table, there are a few more interest‐\\ning features we can pull out of this dataset using the Pandas tools covered up to this\\npoint. We must start by cleaning the data a bit, removing outliers caused by mistyped\\ndates (e.g., June 31st) or missing values (e.g., June 99th). One easy way to remove\\nthese all at once is to cut outliers; we’ll do this via a robust sigma-clipping operation:1\\nIn[15]: quartiles = np.percentile(births['births'], [25, 50, 75])\\n        mu = quartiles[1]\\n        sig = 0.74 * (quartiles[2] - quartiles[0])\\nThis final line is a robust estimate of the sample mean, where the 0.74 comes from the\\ninterquartile range of a Gaussian distribution. With this we can use the query()\\nmethod (discussed further in “High-Performance Pandas: eval() and query()” on\\npage 208) to filter out rows with births outside these values:\\nIn[16]:\\nbirths = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')\\nPivot Tables \\n| \\n175\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 193}, page_content=\"Next we set the day column to integers; previously it had been a string because some\\ncolumns in the dataset contained the value 'null':\\nIn[17]: # set 'day' column to integer; it originally was a string due to nulls\\n        births['day'] = births['day'].astype(int)\\nFinally, we can combine the day, month, and year to create a Date index (see “Work‐\\ning with Time Series” on page 188). This allows us to quickly compute the weekday\\ncorresponding to each row:\\nIn[18]: # create a datetime index from the year, month, day\\n        births.index = pd.to_datetime(10000 * births.year +\\n                                      100 * births.month +\\n                                      births.day, format='%Y%m%d')\\n        births['dayofweek'] = births.index.dayofweek\\nUsing this we can plot births by weekday for several decades (Figure 3-3):\\nIn[19]:\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nbirths.pivot_table('births', index='dayofweek',\\n                    columns='decade', aggfunc='mean').plot()\\nplt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'])\\nplt.ylabel('mean births by day');\\nFigure 3-3. Average daily births by day of week and decade\\nApparently births are slightly less common on weekends than on weekdays! Note that\\nthe 1990s and 2000s are missing because the CDC data contains only the month of\\nbirth starting in 1989.\\n176 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 194}, page_content=\"Another interesting view is to plot the mean number of births by the day of the year.\\nLet’s first group the data by month and day separately:\\nIn[20]:\\nbirths_by_date = births.pivot_table('births',\\n                                    [births.index.month, births.index.day])\\nbirths_by_date.head()\\nOut[20]: 1  1    4009.225\\n            2    4247.400\\n            3    4500.900\\n            4    4571.350\\n            5    4603.625\\n         Name: births, dtype: float64\\nThe result is a multi-index over months and days. To make this easily plottable, let’s\\nturn these months and days into a date by associating them with a dummy year vari‐\\nable (making sure to choose a leap year so February 29th is correctly handled!)\\nIn[21]: births_by_date.index = [pd.datetime(2012, month, day)\\n                                for (month, day) in births_by_date.index]\\n        births_by_date.head()\\nOut[21]: 2012-01-01    4009.225\\n         2012-01-02    4247.400\\n         2012-01-03    4500.900\\n         2012-01-04    4571.350\\n         2012-01-05    4603.625\\n         Name: births, dtype: float64\\nFocusing on the month and day only, we now have a time series reflecting the average\\nnumber of births by date of the year. From this, we can use the plot method to plot\\nthe data (Figure 3-4). It reveals some interesting trends:\\nIn[22]: # Plot the results\\n        fig, ax = plt.subplots(figsize=(12, 4))\\n        births_by_date.plot(ax=ax);\\nFigure 3-4. Average daily births by date\\nPivot Tables \\n| \\n177\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 195}, page_content=\"In particular, the striking feature of this graph is the dip in birthrate on US holidays\\n(e.g., Independence Day, Labor Day, Thanksgiving, Christmas, New Year’s Day)\\nalthough this likely reflects trends in scheduled/induced births rather than some deep\\npsychosomatic effect on natural births. For more discussion on this trend, see the\\nanalysis and links in Andrew Gelman’s blog post on the subject. We’ll return to this\\nfigure in “Example: Effect of Holidays on US Births” on page 269, where we will use\\nMatplotlib’s tools to annotate this plot.\\nLooking at this short example, you can see that many of the Python and Pandas tools\\nwe’ve seen to this point can be combined and used to gain insight from a variety of\\ndatasets. We will see some more sophisticated applications of these data manipula‐\\ntions in future sections!\\nVectorized String Operations\\nOne strength of Python is its relative ease in handling and manipulating string data.\\nPandas builds on this and provides a comprehensive set of vectorized string operations\\nthat become an essential piece of the type of munging required when one is working\\nwith (read: cleaning up) real-world data. In this section, we’ll walk through some of\\nthe Pandas string operations, and then take a look at using them to partially clean up\\na very messy dataset of recipes collected from the Internet.\\nIntroducing Pandas String Operations\\nWe saw in previous sections how tools like NumPy and Pandas generalize arithmetic\\noperations so that we can easily and quickly perform the same operation on many\\narray elements. For example:\\nIn[1]: import numpy as np\\n       x = np.array([2, 3, 5, 7, 11, 13])\\n       x * 2\\nOut[1]: array([ 4,  6, 10, 14, 22, 26])\\nThis vectorization of operations simplifies the syntax of operating on arrays of data:\\nwe no longer have to worry about the size or shape of the array, but just about what\\noperation we want done. For arrays of strings, NumPy does not provide such simple\\naccess, and thus you’re stuck using a more verbose loop syntax:\\nIn[2]: data = ['peter', 'Paul', 'MARY', 'gUIDO']\\n       [s.capitalize() for s in data]\\nOut[2]: ['Peter', 'Paul', 'Mary', 'Guido']\\nThis is perhaps sufficient to work with some data, but it will break if there are any\\nmissing values. For example:\\nIn[3]: data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\\n       [s.capitalize() for s in data]\\n178 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 196}, page_content=\"---------------------------------------------------------------------------\\n---------------------------------------------------------------------------\\nAttributeError                            Traceback (most recent call last)\\n<ipython-input-3-fc1d891ab539> in <module>()\\n      1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\\n----> 2 [s.capitalize() for s in data]\\n<ipython-input-3-fc1d891ab539> in <listcomp>(.0)\\n      1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\\n----> 2 [s.capitalize() for s in data]\\nAttributeError: 'NoneType' object has no attribute 'capitalize'\\n---------------------------------------------------------------------------\\nPandas includes features to address both this need for vectorized string operations\\nand for correctly handling missing data via the str attribute of Pandas Series and\\nIndex objects containing strings. So, for example, suppose we create a Pandas Series\\nwith this data:\\nIn[4]: import pandas as pd\\n       names = pd.Series(data)\\n       names\\nOut[4]: 0    peter\\n        1     Paul\\n        2     None\\n        3     MARY\\n        4    gUIDO\\n        dtype: object\\nWe can now call a single method that will capitalize all the entries, while skipping\\nover any missing values:\\nIn[5]: names.str.capitalize()\\nOut[5]: 0    Peter\\n        1     Paul\\n        2     None\\n        3     Mary\\n        4    Guido\\n        dtype: object\\nUsing tab completion on this str attribute will list all the vectorized string methods\\navailable to Pandas.\\nVectorized String Operations \\n| \\n179\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 197}, page_content=\"Tables of Pandas String Methods\\nIf you have a good understanding of string manipulation in Python, most of Pandas’\\nstring syntax is intuitive enough that it’s probably sufficient to just list a table of avail‐\\nable methods; we will start with that here, before diving deeper into a few of the sub‐\\ntleties. The examples in this section use the following series of names:\\nIn[6]: monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\\n                          'Eric Idle', 'Terry Jones', 'Michael Palin'])\\nMethods similar to Python string methods\\nNearly all Python’s built-in string methods are mirrored by a Pandas vectorized string\\nmethod. Here is a list of Pandas str methods that mirror Python string methods:\\nlen()\\nlower()\\ntranslate()\\nislower()\\nljust()\\nupper()\\nstartswith()\\nisupper()\\nrjust()\\nfind()\\nendswith()\\nisnumeric()\\ncenter()\\nrfind()\\nisalnum()\\nisdecimal()\\nzfill()\\nindex()\\nisalpha()\\nsplit()\\nstrip()\\nrindex()\\nisdigit()\\nrsplit()\\nrstrip()\\ncapitalize()\\nisspace()\\npartition()\\nlstrip()\\nswapcase()\\nistitle()\\nrpartition()\\nNotice that these have various return values. Some, like lower(), return a series of\\nstrings:\\nIn[7]: monte.str.lower()\\nOut[7]: 0    graham chapman\\n        1       john cleese\\n        2     terry gilliam\\n        3         eric idle\\n        4       terry jones\\n        5     michael palin\\n        dtype: object\\nBut some others return numbers:\\nIn[8]: monte.str.len()\\nOut[8]: 0    14\\n        1    11\\n        2    13\\n        3     9\\n        4    11\\n        5    13\\n        dtype: int64\\n180 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 198}, page_content=\"Or Boolean values:\\nIn[9]: monte.str.startswith('T')\\nOut[9]: 0    False\\n        1    False\\n        2     True\\n        3    False\\n        4     True\\n        5    False\\n        dtype: bool\\nStill others return lists or other compound values for each element:\\nIn[10]: monte.str.split()\\nOut[10]: 0    [Graham, Chapman]\\n         1       [John, Cleese]\\n         2     [Terry, Gilliam]\\n         3         [Eric, Idle]\\n         4       [Terry, Jones]\\n         5     [Michael, Palin]\\n         dtype: object\\nWe’ll see further manipulations of this kind of series-of-lists object as we continue\\nour discussion.\\nMethods using regular expressions\\nIn addition, there are several methods that accept regular expressions to examine the\\ncontent of each string element, and follow some of the API conventions of Python’s\\nbuilt-in re module (see Table 3-4).\\nTable 3-4. Mapping between Pandas methods and functions in Python’s re module\\nMethod\\nDescription\\nmatch()\\nCall re.match() on each element, returning a Boolean.\\nextract()\\nCall re.match() on each element, returning matched groups as strings.\\nfindall()\\nCall re.findall() on each element.\\nreplace()\\nReplace occurrences of pattern with some other string.\\ncontains() Call re.search() on each element, returning a Boolean.\\ncount()\\nCount occurrences of pattern.\\nsplit()\\nEquivalent to str.split(), but accepts regexps.\\nrsplit()\\nEquivalent to str.rsplit(), but accepts regexps.\\nWith these, you can do a wide range of interesting operations. For example, we can\\nextract the first name from each by asking for a contiguous group of characters at the\\nbeginning of each element:\\nVectorized String Operations \\n| \\n181\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 199}, page_content=\"In[11]: monte.str.extract('([A-Za-z]+)')\\nOut[11]: 0     Graham\\n         1       John\\n         2      Terry\\n         3       Eric\\n         4      Terry\\n         5    Michael\\n         dtype: object\\nOr we can do something more complicated, like finding all names that start and end\\nwith a consonant, making use of the start-of-string (^) and end-of-string ($) regular\\nexpression characters:\\nIn[12]: monte.str.findall(r'^[^AEIOU].*[^aeiou]$')\\nOut[12]: 0    [Graham Chapman]\\n         1                  []\\n         2     [Terry Gilliam]\\n         3                  []\\n         4       [Terry Jones]\\n         5     [Michael Palin]\\n         dtype: object\\nThe ability to concisely apply regular expressions across Series or DataFrame entries\\nopens up many possibilities for analysis and cleaning of data.\\nMiscellaneous methods\\nFinally, there are some miscellaneous methods that enable other convenient opera‐\\ntions (see Table 3-5).\\nTable 3-5. Other Pandas string methods\\nMethod\\nDescription\\nget()\\nIndex each element\\nslice()\\nSlice each element\\nslice_replace()\\nReplace slice in each element with passed value\\ncat()\\nConcatenate strings\\nrepeat()\\nRepeat values\\nnormalize()\\nReturn Unicode form of string\\npad()\\nAdd whitespace to left, right, or both sides of strings\\nwrap()\\nSplit long strings into lines with length less than a given width\\njoin()\\nJoin strings in each element of the Series with passed separator\\nget_dummies()\\nExtract dummy variables as a DataFrame\\n182 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 200}, page_content=\"Vectorized item access and slicing.    The get() and slice() operations, in particular,\\nenable vectorized element access from each array. For example, we can get a slice of\\nthe first three characters of each array using str.slice(0, 3). Note that this behav‐\\nior is also available through Python’s normal indexing syntax—for example,\\ndf.str.slice(0, 3) is equivalent to df.str[0:3]:\\nIn[13]: monte.str[0:3]\\nOut[13]: 0    Gra\\n         1    Joh\\n         2    Ter\\n         3    Eri\\n         4    Ter\\n         5    Mic\\n         dtype: object\\nIndexing via df.str.get(i) and df.str[i] is similar.\\nThese get() and slice() methods also let you access elements of arrays returned by\\nsplit(). For example, to extract the last name of each entry, we can combine\\nsplit() and get():\\nIn[14]: monte.str.split().str.get(-1)\\nOut[14]: 0    Chapman\\n         1     Cleese\\n         2    Gilliam\\n         3       Idle\\n         4      Jones\\n         5      Palin\\n         dtype: object\\nIndicator variables.    Another method that requires a bit of extra explanation is the\\nget_dummies() method. This is useful when your data has a column containing some\\nsort of coded indicator. For example, we might have a dataset that contains informa‐\\ntion in the form of codes, such as A=“born in America,” B=“born in the United King‐\\ndom,” C=“likes cheese,” D=“likes spam”:\\nIn[15]:\\nfull_monte = pd.DataFrame({'name': monte,\\n                           'info': ['B|C|D', 'B|D', 'A|C', 'B|D', 'B|C',\\n                           'B|C|D']})\\nfull_monte\\nOut[15]:     info            name\\n         0  B|C|D  Graham Chapman\\n         1    B|D     John Cleese\\n         2    A|C   Terry Gilliam\\n         3    B|D       Eric Idle\\n         4    B|C     Terry Jones\\n         5  B|C|D   Michael Palin\\nVectorized String Operations \\n| \\n183\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 201}, page_content='The get_dummies() routine lets you quickly split out these indicator variables into a\\nDataFrame:\\nIn[16]: full_monte[\\'info\\'].str.get_dummies(\\'|\\')\\nOut[16]:    A  B  C  D\\n         0  0  1  1  1\\n         1  0  1  0  1\\n         2  1  0  1  0\\n         3  0  1  0  1\\n         4  0  1  1  0\\n         5  0  1  1  1\\nWith these operations as building blocks, you can construct an endless range of string\\nprocessing procedures when cleaning your data.\\nWe won’t dive further into these methods here, but I encourage you to read through\\n“Working with Text Data” in the pandas online documentation, or to refer to the\\nresources listed in “Further Resources” on page 215.\\nExample: Recipe Database\\nThese vectorized string operations become most useful in the process of cleaning up\\nmessy, real-world data. Here I’ll walk through an example of that, using an open\\nrecipe database compiled from various sources on the Web. Our goal will be to parse\\nthe recipe data into ingredient lists, so we can quickly find a recipe based on some\\ningredients we have on hand.\\nThe scripts used to compile this can be found at https://github.com/fictivekin/openre\\ncipes, and the link to the current version of the database is found there as well.\\nAs of spring 2016, this database is about 30 MB, and can be downloaded and unzip‐\\nped with these commands:\\nIn[17]: # !curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz\\n        # !gunzip recipeitems-latest.json.gz\\nThe database is in JSON format, so we will try pd.read_json to read it:\\nIn[18]: try:\\n            recipes = pd.read_json(\\'recipeitems-latest.json\\')\\n        except ValueError as e:\\n            print(\"ValueError:\", e)\\nValueError: Trailing data\\nOops! We get a ValueError mentioning that there is “trailing data.” Searching for this\\nerror on the Internet, it seems that it’s due to using a file in which each line is itself a\\nvalid JSON, but the full file is not. Let’s check if this interpretation is true:\\n184 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 202}, page_content='In[19]: with open(\\'recipeitems-latest.json\\') as f:\\n            line = f.readline()\\n        pd.read_json(line).shape\\nOut[19]: (2, 12)\\nYes, apparently each line is a valid JSON, so we’ll need to string them together. One\\nway we can do this is to actually construct a string representation containing all these\\nJSON entries, and then load the whole thing with pd.read_json:\\nIn[20]: # read the entire file into a Python array\\n        with open(\\'recipeitems-latest.json\\', \\'r\\') as f:\\n            # Extract each line\\n            data = (line.strip() for line in f)\\n            # Reformat so each line is the element of a list\\n            data_json = \"[{0}]\".format(\\',\\'.join(data))\\n        # read the result as a JSON\\n        recipes = pd.read_json(data_json)\\nIn[21]: recipes.shape\\nOut[21]: (173278, 17)\\nWe see there are nearly 200,000 recipes, and 17 columns. Let’s take a look at one row\\nto see what we have:\\nIn[22]: recipes.iloc[0]\\nOut[22]:\\n_id                                {\\'$oid\\': \\'5160756b96cc62079cc2db15\\'}\\ncookTime                                                          PT30M\\ncreator                                                             NaN\\ndateModified                                                        NaN\\ndatePublished                                                2013-03-11\\ndescription           Late Saturday afternoon, after Marlboro Man ha...\\nimage                 http://static.thepioneerwoman.com/cooking/file...\\ningredients           Biscuits\\\\n3 cups All-purpose Flour\\\\n2 Tablespo...\\nname                                    Drop Biscuits and Sausage Gravy\\nprepTime                                                          PT10M\\nrecipeCategory                                                      NaN\\nrecipeInstructions                                                  NaN\\nrecipeYield                                                          12\\nsource                                                  thepioneerwoman\\ntotalTime                                                           NaN\\nts                                             {\\'$date\\': 1365276011104}\\nurl                   http://thepioneerwoman.com/cooking/2013/03/dro...\\nName: 0, dtype: object\\nThere is a lot of information there, but much of it is in a very messy form, as is typical\\nof data scraped from the Web. In particular, the ingredient list is in string format;\\nwe’re going to have to carefully extract the information we’re interested in. Let’s start\\nby taking a closer look at the ingredients:\\nIn[23]: recipes.ingredients.str.len().describe()\\nVectorized String Operations \\n| \\n185'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 203}, page_content=\"Out[23]: count    173278.000000\\n         mean        244.617926\\n         std         146.705285\\n         min           0.000000\\n         25%         147.000000\\n         50%         221.000000\\n         75%         314.000000\\n         max        9067.000000\\n         Name: ingredients, dtype: float64\\nThe ingredient lists average 250 characters long, with a minimum of 0 and a maxi‐\\nmum of nearly 10,000 characters!\\nJust out of curiosity, let’s see which recipe has the longest ingredient list:\\nIn[24]: recipes.name[np.argmax(recipes.ingredients.str.len())]\\nOut[24]: 'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream\\n&amp; Cream Cheese Frosting and Marzipan Carrots'\\nThat certainly looks like an involved recipe.\\nWe can do other aggregate explorations; for example, let’s see how many of the rec‐\\nipes are for breakfast food:\\nIn[33]: recipes.description.str.contains('[Bb]reakfast').sum()\\nOut[33]: 3524\\nOr how many of the recipes list cinnamon as an ingredient:\\nIn[34]: recipes.ingredients.str.contains('[Cc]innamon').sum()\\nOut[34]: 10526\\nWe could even look to see whether any recipes misspell the ingredient as “cinamon”:\\nIn[27]: recipes.ingredients.str.contains('[Cc]inamon').sum()\\nOut[27]: 11\\nThis is the type of essential data exploration that is possible with Pandas string tools.\\nIt is data munging like this that Python really excels at.\\nA simple recipe recommender\\nLet’s go a bit further, and start working on a simple recipe recommendation system:\\ngiven a list of ingredients, find a recipe that uses all those ingredients. While concep‐\\ntually straightforward, the task is complicated by the heterogeneity of the data: there\\nis no easy operation, for example, to extract a clean list of ingredients from each row.\\nSo we will cheat a bit: we’ll start with a list of common ingredients, and simply search\\nto see whether they are in each recipe’s ingredient list. For simplicity, let’s just stick\\nwith herbs and spices for the time being:\\n186 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 204}, page_content=\"In[28]: spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',\\n                      'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']\\nWe can then build a Boolean DataFrame consisting of True and False values, indicat‐\\ning whether this ingredient appears in the list:\\nIn[29]:\\nimport re\\nspice_df = pd.DataFrame(\\n           dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))\\n                                                        for spice in spice_list))\\nspice_df.head()\\nOut[29]:\\n   cumin oregano paprika parsley pepper rosemary   sage   salt tarragon  thyme\\n0  False   False   False   False  False    False   True  False    False  False\\n1  False   False   False   False  False    False  False  False    False  False\\n2   True   False   False   False   True    False  False   True    False  False\\n3  False   False   False   False  False    False  False  False    False  False\\n4  False   False   False   False  False    False  False  False    False  False\\nNow, as an example, let’s say we’d like to find a recipe that uses parsley, paprika, and\\ntarragon. We can compute this very quickly using the query() method of Data\\nFrames, discussed in “High-Performance Pandas: eval() and query()” on page 208:\\nIn[30]: selection = spice_df.query('parsley & paprika & tarragon')\\n        len(selection)\\nOut[30]: 10\\nWe find only 10 recipes with this combination; let’s use the index returned by this\\nselection to discover the names of the recipes that have this combination:\\nIn[31]: recipes.name[selection.index]\\nOut[31]: 2069      All cremat with a Little Gem, dandelion and wa...\\n         74964                         Lobster with Thermidor butter\\n         93768      Burton's Southern Fried Chicken with White Gravy\\n         113926                     Mijo's Slow Cooker Shredded Beef\\n         137686                     Asparagus Soup with Poached Eggs\\n         140530                                 Fried Oyster Po’boys\\n         158475                Lamb shank tagine with herb tabbouleh\\n         158486                 Southern fried chicken in buttermilk\\n         163175            Fried Chicken Sliders with Pickles + Slaw\\n         165243                        Bar Tartine Cauliflower Salad\\n         Name: name, dtype: object\\nNow that we have narrowed down our recipe selection by a factor of almost 20,000,\\nwe are in a position to make a more informed decision about what we’d like to cook\\nfor dinner.\\nVectorized String Operations \\n| \\n187\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 205}, page_content='Going further with recipes\\nHopefully this example has given you a bit of a flavor (ba-dum!) for the types of data\\ncleaning operations that are efficiently enabled by Pandas string methods. Of course,\\nbuilding a very robust recipe recommendation system would require a lot more\\nwork! Extracting full ingredient lists from each recipe would be an important piece of\\nthe task; unfortunately, the wide variety of formats used makes this a relatively time-\\nconsuming process. This points to the truism that in data science, cleaning and\\nmunging of real-world data often comprises the majority of the work, and Pandas\\nprovides the tools that can help you do this efficiently.\\nWorking with Time Series\\nPandas was developed in the context of financial modeling, so as you might expect, it\\ncontains a fairly extensive set of tools for working with dates, times, and time-\\nindexed data. Date and time data comes in a few flavors, which we will discuss here:\\n• Time stamps reference particular moments in time (e.g., July 4th, 2015, at 7:00\\na.m.).\\n• Time intervals and periods reference a length of time between a particular begin‐\\nning and end point—for example, the year 2015. Periods usually reference a spe‐\\ncial case of time intervals in which each interval is of uniform length and does\\nnot overlap (e.g., 24 hour-long periods constituting days).\\n• Time deltas or durations reference an exact length of time (e.g., a duration of\\n22.56 seconds).\\nIn this section, we will introduce how to work with each of these types of date/time\\ndata in Pandas. This short section is by no means a complete guide to the time series\\ntools available in Python or Pandas, but instead is intended as a broad overview of\\nhow you as a user should approach working with time series. We will start with a\\nbrief discussion of tools for dealing with dates and times in Python, before moving\\nmore specifically to a discussion of the tools provided by Pandas. After listing some\\nresources that go into more depth, we will review some short examples of working\\nwith time series data in Pandas.\\nDates and Times in Python\\nThe Python world has a number of available representations of dates, times, deltas,\\nand timespans. While the time series tools provided by Pandas tend to be the most\\nuseful for data science applications, it is helpful to see their relationship to other\\npackages used in Python.\\n188 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 206}, page_content='Native Python dates and times: datetime and dateutil\\nPython’s basic objects for working with dates and times reside in the built-in date\\ntime module. Along with the third-party dateutil module, you can use it to quickly\\nperform a host of useful functionalities on dates and times. For example, you can\\nmanually build a date using the datetime type:\\nIn[1]: from datetime import datetime\\n       datetime(year=2015, month=7, day=4)\\nOut[1]: datetime.datetime(2015, 7, 4, 0, 0)\\nOr, using the dateutil module, you can parse dates from a variety of string formats:\\nIn[2]: from dateutil import parser\\n       date = parser.parse(\"4th of July, 2015\")\\n       date\\nOut[2]: datetime.datetime(2015, 7, 4, 0, 0)\\nOnce you have a datetime object, you can do things like printing the day of the week:\\nIn[3]: date.strftime(\\'%A\\')\\nOut[3]: \\'Saturday\\'\\nIn the final line, we’ve used one of the standard string format codes for printing dates\\n(\"%A\"), which you can read about in the strftime section of Python’s datetime docu‐\\nmentation. Documentation of other useful date utilities can be found in dateutil’s\\nonline documentation. A related package to be aware of is pytz, which contains tools\\nfor working with the most migraine-inducing piece of time series data: time zones.\\nThe power of datetime and dateutil lies in their flexibility and easy syntax: you can\\nuse these objects and their built-in methods to easily perform nearly any operation\\nyou might be interested in. Where they break down is when you wish to work with\\nlarge arrays of dates and times: just as lists of Python numerical variables are subopti‐\\nmal compared to NumPy-style typed numerical arrays, lists of Python datetime\\nobjects are suboptimal compared to typed arrays of encoded dates.\\nTyped arrays of times: NumPy’s datetime64\\nThe weaknesses of Python’s datetime format inspired the NumPy team to add a set of\\nnative time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit\\nintegers, and thus allows arrays of dates to be represented very compactly. The date\\ntime64 requires a very specific input format:\\nIn[4]: import numpy as np\\n       date = np.array(\\'2015-07-04\\', dtype=np.datetime64)\\n       date\\nOut[4]: array(datetime.date(2015, 7, 4), dtype=\\'datetime64[D]\\')\\nWorking with Time Series \\n| \\n189'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 207}, page_content=\"Once we have this date formatted, however, we can quickly do vectorized operations\\non it:\\nIn[5]: date + np.arange(12)\\nOut[5]:\\narray(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',\\n        '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',\\n        '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],\\n        dtype='datetime64[D]')\\nBecause of the uniform type in NumPy datetime64 arrays, this type of operation can\\nbe accomplished much more quickly than if we were working directly with Python’s\\ndatetime objects, especially as arrays get large (we introduced this type of vectoriza‐\\ntion in “Computation on NumPy Arrays: Universal Functions” on page 50).\\nOne detail of the datetime64 and timedelta64 objects is that they are built on a fun‐\\ndamental time unit. Because the datetime64 object is limited to 64-bit precision, the\\nrange of encodable times is 264 times this fundamental unit. In other words, date\\ntime64 imposes a trade-off between time resolution and maximum time span.\\nFor example, if you want a time resolution of one nanosecond, you only have enough\\ninformation to encode a range of 264 nanoseconds, or just under 600 years. NumPy\\nwill infer the desired unit from the input; for example, here is a day-based datetime:\\nIn[6]: np.datetime64('2015-07-04')\\nOut[6]: numpy.datetime64('2015-07-04')\\nHere is a minute-based datetime:\\nIn[7]: np.datetime64('2015-07-04 12:00')\\nOut[7]: numpy.datetime64('2015-07-04T12:00')\\nNotice that the time zone is automatically set to the local time on the computer exe‐\\ncuting the code. You can force any desired fundamental unit using one of many for‐\\nmat codes; for example, here we’ll force a nanosecond-based time:\\nIn[8]: np.datetime64('2015-07-04 12:59:59.50', 'ns')\\nOut[8]: numpy.datetime64('2015-07-04T12:59:59.500000000')\\nTable 3-6, drawn from the NumPy datetime64 documentation, lists the available for‐\\nmat codes along with the relative and absolute timespans that they can encode.\\nTable 3-6. Description of date and time codes\\nCode Meaning\\nTime span (relative) Time span (absolute)\\nY\\nYear\\n± 9.2e18 years\\n[9.2e18 BC, 9.2e18 AD]\\nM\\nMonth\\n± 7.6e17 years\\n[7.6e17 BC, 7.6e17 AD]\\nW\\nWeek\\n± 1.7e17 years\\n[1.7e17 BC, 1.7e17 AD]\\n190 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 208}, page_content='Code Meaning\\nTime span (relative) Time span (absolute)\\nD\\nDay\\n± 2.5e16 years\\n[2.5e16 BC, 2.5e16 AD]\\nh\\nHour\\n± 1.0e15 years\\n[1.0e15 BC, 1.0e15 AD]\\nm\\nMinute\\n± 1.7e13 years\\n[1.7e13 BC, 1.7e13 AD]\\ns\\nSecond\\n± 2.9e12 years\\n[ 2.9e9 BC, 2.9e9 AD]\\nms\\nMillisecond\\n± 2.9e9 years\\n[ 2.9e6 BC, 2.9e6 AD]\\nus\\nMicrosecond\\n± 2.9e6 years\\n[290301 BC, 294241 AD]\\nns\\nNanosecond\\n± 292 years\\n[ 1678 AD, 2262 AD]\\nps\\nPicosecond\\n± 106 days\\n[ 1969 AD, 1970 AD]\\nfs\\nFemtosecond ± 2.6 hours\\n[ 1969 AD, 1970 AD]\\nas\\nAttosecond\\n± 9.2 seconds\\n[ 1969 AD, 1970 AD]\\nFor the types of data we see in the real world, a useful default is datetime64[ns], as it\\ncan encode a useful range of modern dates with a suitably fine precision.\\nFinally, we will note that while the datetime64 data type addresses some of the defi‐\\nciencies of the built-in Python datetime type, it lacks many of the convenient meth‐\\nods and functions provided by datetime and especially dateutil. More information\\ncan be found in NumPy’s datetime64 documentation.\\nDates and times in Pandas: Best of both worlds\\nPandas builds upon all the tools just discussed to provide a Timestamp object, which\\ncombines the ease of use of datetime and dateutil with the efficient storage and\\nvectorized interface of numpy.datetime64. From a group of these Timestamp objects,\\nPandas can construct a DatetimeIndex that can be used to index data in a Series or\\nDataFrame; we’ll see many examples of this below.\\nFor example, we can use Pandas tools to repeat the demonstration from above. We\\ncan parse a flexibly formatted string date, and use format codes to output the day of\\nthe week:\\nIn[9]: import pandas as pd\\n       date = pd.to_datetime(\"4th of July, 2015\")\\n       date\\nOut[9]: Timestamp(\\'2015-07-04 00:00:00\\')\\nIn[10]: date.strftime(\\'%A\\')\\nOut[10]: \\'Saturday\\'\\nAdditionally, we can do NumPy-style vectorized operations directly on this same\\nobject:\\nIn[11]: date + pd.to_timedelta(np.arange(12), \\'D\\')\\nWorking with Time Series \\n| \\n191'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 209}, page_content=\"Out[11]: DatetimeIndex(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',\\n                        '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',\\n                        '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],\\n                       dtype='datetime64[ns]', freq=None)\\nIn the next section, we will take a closer look at manipulating time series data with\\nthe tools provided by Pandas.\\nPandas Time Series: Indexing by Time\\nWhere the Pandas time series tools really become useful is when you begin to index\\ndata by timestamps. For example, we can construct a Series object that has time-\\nindexed data:\\nIn[12]: index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\\n                                  '2015-07-04', '2015-08-04'])\\n        data = pd.Series([0, 1, 2, 3], index=index)\\n        data\\nOut[12]: 2014-07-04    0\\n         2014-08-04    1\\n         2015-07-04    2\\n         2015-08-04    3\\n         dtype: int64\\nNow that we have this data in a Series, we can make use of any of the Series index‐\\ning patterns we discussed in previous sections, passing values that can be coerced into\\ndates:\\nIn[13]: data['2014-07-04':'2015-07-04']\\nOut[13]: 2014-07-04    0\\n         2014-08-04    1\\n         2015-07-04    2\\n         dtype: int64\\nThere are additional special date-only indexing operations, such as passing a year to\\nobtain a slice of all data from that year:\\nIn[14]: data['2015']\\nOut[14]: 2015-07-04    2\\n         2015-08-04    3\\n         dtype: int64\\nLater, we will see additional examples of the convenience of dates-as-indices. But first,\\nlet’s take a closer look at the available time series data structures.\\nPandas Time Series Data Structures\\nThis section will introduce the fundamental Pandas data structures for working with\\ntime series data:\\n192 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 210}, page_content=\"• For time stamps, Pandas provides the Timestamp type. As mentioned before, it is\\nessentially a replacement for Python’s native datetime, but is based on the more\\nefficient numpy.datetime64 data type. The associated index structure is\\nDatetimeIndex.\\n• For time periods, Pandas provides the Period type. This encodes a fixed-\\nfrequency interval based on numpy.datetime64. The associated index structure is\\nPeriodIndex.\\n• For time deltas or durations, Pandas provides the Timedelta type. Timedelta is a\\nmore efficient replacement for Python’s native datetime.timedelta type, and is\\nbased on numpy.timedelta64. The associated index structure is TimedeltaIndex.\\nThe most fundamental of these date/time objects are the Timestamp and DatetimeIn\\ndex objects. While these class objects can be invoked directly, it is more common to\\nuse the pd.to_datetime() function, which can parse a wide variety of formats. Pass‐\\ning a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by\\ndefault yields a DatetimeIndex:\\nIn[15]: dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\\n                               '2015-Jul-6', '07-07-2015', '20150708'])\\n        dates\\nOut[15]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',\\n                        '2015-07-08'],\\n                       dtype='datetime64[ns]', freq=None)\\nAny DatetimeIndex can be converted to a PeriodIndex with the to_period() func‐\\ntion with the addition of a frequency code; here we’ll use 'D' to indicate daily\\nfrequency:\\nIn[16]: dates.to_period('D')\\nOut[16]: PeriodIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',\\n                      '2015-07-08'],\\n                     dtype='int64', freq='D')\\nA TimedeltaIndex is created, for example, when one date is subtracted from another:\\nIn[17]: dates - dates[0]\\nOut[17]:\\nTimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'],\\n               dtype='timedelta64[ns]', freq=None)\\nRegular sequences: pd.date_range()\\nTo make the creation of regular date sequences more convenient, Pandas offers a few\\nfunctions for this purpose: pd.date_range() for timestamps, pd.period_range() for\\nperiods, and pd.timedelta_range() for time deltas. We’ve seen that Python’s\\nWorking with Time Series \\n| \\n193\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 211}, page_content=\"range() and NumPy’s np.arange() turn a startpoint, endpoint, and optional stepsize\\ninto a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an\\noptional frequency code to create a regular sequence of dates. By default, the fre‐\\nquency is one day:\\nIn[18]: pd.date_range('2015-07-03', '2015-07-10')\\nOut[18]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',\\n                        '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],\\n                       dtype='datetime64[ns]', freq='D')\\nAlternatively, the date range can be specified not with a start- and endpoint, but with\\na startpoint and a number of periods:\\nIn[19]: pd.date_range('2015-07-03', periods=8)\\nOut[19]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',\\n                        '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],\\n                       dtype='datetime64[ns]', freq='D')\\nYou can modify the spacing by altering the freq argument, which defaults to D. For\\nexample, here we will construct a range of hourly timestamps:\\nIn[20]: pd.date_range('2015-07-03', periods=8, freq='H')\\nOut[20]: DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00',\\n                        '2015-07-03 02:00:00', '2015-07-03 03:00:00',\\n                        '2015-07-03 04:00:00', '2015-07-03 05:00:00',\\n                        '2015-07-03 06:00:00', '2015-07-03 07:00:00'],\\n                       dtype='datetime64[ns]', freq='H')\\nTo create regular sequences of period or time delta values, the very similar\\npd.period_range() and pd.timedelta_range() functions are useful. Here are some\\nmonthly periods:\\nIn[21]: pd.period_range('2015-07', periods=8, freq='M')\\nOut[21]:\\nPeriodIndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12',\\n             '2016-01', '2016-02'],\\n            dtype='int64', freq='M')\\nAnd a sequence of durations increasing by an hour:\\nIn[22]: pd.timedelta_range(0, periods=10, freq='H')\\nOut[22]:\\nTimedeltaIndex(['00:00:00', '01:00:00', '02:00:00', '03:00:00', '04:00:00',\\n                '05:00:00', '06:00:00', '07:00:00', '08:00:00', '09:00:00'],\\n               dtype='timedelta64[ns]', freq='H')\\nAll of these require an understanding of Pandas frequency codes, which we’ll summa‐\\nrize in the next section.\\n194 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 212}, page_content='Frequencies and Offsets\\nFundamental to these Pandas time series tools is the concept of a frequency or date\\noffset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes\\nto specify any desired frequency spacing. Table 3-7 summarizes the main codes\\navailable.\\nTable 3-7. Listing of Pandas frequency codes\\nCode Description\\nCode Description\\nD\\nCalendar day\\nB\\nBusiness day\\nW\\nWeekly\\nM\\nMonth end\\nBM\\nBusiness month end\\nQ\\nQuarter end\\nBQ\\nBusiness quarter end\\nA\\nYear end\\nBA\\nBusiness year end\\nH\\nHours\\nBH\\nBusiness hours\\nT\\nMinutes\\nS\\nSeconds\\nL\\nMilliseonds\\nU\\nMicroseconds\\nN\\nNanoseconds\\nThe monthly, quarterly, and annual frequencies are all marked at the end of the speci‐\\nfied period. Adding an S suffix to any of these marks it instead at the beginning\\n(Table 3-8).\\nTable 3-8. Listing of start-indexed frequency codes\\nCode Description\\nMS\\nMonth start\\nBMS\\nBusiness month start\\nQS\\nQuarter start\\nBQS\\nBusiness quarter start\\nAS\\nYear start\\nBAS\\nBusiness year start\\nWorking with Time Series \\n| \\n195'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 213}, page_content='Additionally, you can change the month used to mark any quarterly or annual code\\nby adding a three-letter month code as a suffix:\\n• Q-JAN, BQ-FEB, QS-MAR, BQS-APR, etc.\\n• A-JAN, BA-FEB, AS-MAR, BAS-APR, etc.\\nIn the same way, you can modify the split-point of the weekly frequency by adding a\\nthree-letter weekday code:\\n• W-SUN, W-MON, W-TUE, W-WED, etc.\\nOn top of this, codes can be combined with numbers to specify other frequencies. For\\nexample, for a frequency of 2 hours 30 minutes, we can combine the hour (H) and\\nminute (T) codes as follows:\\nIn[23]: pd.timedelta_range(0, periods=9, freq=\"2H30T\")\\nOut[23]:\\nTimedeltaIndex([\\'00:00:00\\', \\'02:30:00\\', \\'05:00:00\\', \\'07:30:00\\', \\'10:00:00\\',\\n                \\'12:30:00\\', \\'15:00:00\\', \\'17:30:00\\', \\'20:00:00\\'],\\n               dtype=\\'timedelta64[ns]\\', freq=\\'150T\\')\\nAll of these short codes refer to specific instances of Pandas time series offsets, which\\ncan be found in the pd.tseries.offsets module. For example, we can create a busi‐\\nness day offset directly as follows:\\nIn[24]: from pandas.tseries.offsets import BDay\\n        pd.date_range(\\'2015-07-01\\', periods=5, freq=BDay())\\nOut[24]: DatetimeIndex([\\'2015-07-01\\', \\'2015-07-02\\', \\'2015-07-03\\', \\'2015-07-06\\',\\n                        \\'2015-07-07\\'],\\n                       dtype=\\'datetime64[ns]\\', freq=\\'B\\')\\nFor more discussion of the use of frequencies and offsets, see the “DateOffset objects”\\nsection of the Pandas online documentation.\\nResampling, Shifting, and Windowing\\nThe ability to use dates and times as indices to intuitively organize and access data is\\nan important piece of the Pandas time series tools. The benefits of indexed data in\\ngeneral (automatic alignment during operations, intuitive data slicing and access,\\netc.) still apply, and Pandas provides several additional time series–specific\\noperations.\\nWe will take a look at a few of those here, using some stock price data as an example.\\nBecause Pandas was developed largely in a finance context, it includes some very spe‐\\ncific tools for financial data. For example, the accompanying pandas-datareader\\npackage (installable via conda install pandas-datareader) knows how to import\\n196 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 214}, page_content=\"financial data from a number of available sources, including Yahoo finance, Google\\nFinance, and others. Here we will load Google’s closing price history:\\nIn[25]: from pandas_datareader import data\\n        goog = data.DataReader('GOOG', start='2004', end='2016',\\n                               data_source='google')\\n        goog.head()\\nOut[25]:              Open   High    Low  Close  Volume\\n         Date\\n         2004-08-19  49.96  51.98  47.93  50.12     NaN\\n         2004-08-20  50.69  54.49  50.20  54.10     NaN\\n         2004-08-23  55.32  56.68  54.47  54.65     NaN\\n         2004-08-24  55.56  55.74  51.73  52.38     NaN\\n         2004-08-25  52.43  53.95  51.89  52.95     NaN\\nFor simplicity, we’ll use just the closing price:\\nIn[26]: goog = goog['Close']\\nWe can visualize this using the plot() method, after the normal Matplotlib setup\\nboilerplate (Figure 3-5):\\nIn[27]: %matplotlib inline\\n        import matplotlib.pyplot as plt\\n        import seaborn; seaborn.set()\\nIn[28]: goog.plot();\\nFigure 3-5. Google’s closing stock price over time\\nResampling and converting frequencies\\nOne common need for time series data is resampling at a higher or lower frequency.\\nYou can do this using the resample() method, or the much simpler asfreq()\\nWorking with Time Series \\n| \\n197\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 215}, page_content='method. The primary difference between the two is that resample() is fundamentally\\na data aggregation, while asfreq() is fundamentally a data selection.\\nTaking a look at the Google closing price, let’s compare what the two return when we\\ndown-sample the data. Here we will resample the data at the end of business year\\n(Figure 3-6):\\nIn[29]: goog.plot(alpha=0.5, style=\\'-\\')\\n        goog.resample(\\'BA\\').mean().plot(style=\\':\\')\\n        goog.asfreq(\\'BA\\').plot(style=\\'--\\');\\n        plt.legend([\\'input\\', \\'resample\\', \\'asfreq\\'],\\n                   loc=\\'upper left\\');\\nFigure 3-6. Resamplings of Google’s stock price\\nNotice the difference: at each point, resample reports the average of the previous year,\\nwhile asfreq reports the value at the end of the year.\\nFor up-sampling, resample() and asfreq() are largely equivalent, though resample\\nhas many more options available. In this case, the default for both methods is to leave\\nthe up-sampled points empty—that is, filled with NA values. Just as with the\\npd.fillna() function discussed previously, asfreq() accepts a method argument to\\nspecify how values are imputed. Here, we will resample the business day data at a\\ndaily frequency (i.e., including weekends); see Figure 3-7:\\nIn[30]: fig, ax = plt.subplots(2, sharex=True)\\n        data = goog.iloc[:10]\\n        data.asfreq(\\'D\\').plot(ax=ax[0], marker=\\'o\\')\\n        data.asfreq(\\'D\\', method=\\'bfill\\').plot(ax=ax[1], style=\\'-o\\')\\n        data.asfreq(\\'D\\', method=\\'ffill\\').plot(ax=ax[1], style=\\'--o\\')\\n        ax[1].legend([\"back-fill\", \"forward-fill\"]);\\n198 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 216}, page_content=\"Figure 3-7. Comparison between forward-fill and back-fill interpolation\\nThe top panel is the default: non-business days are left as NA values and do not\\nappear on the plot. The bottom panel shows the differences between two strategies\\nfor filling the gaps: forward-filling and backward-filling.\\nTime-shifts\\nAnother common time series–specific operation is shifting of data in time. Pandas\\nhas two closely related methods for computing this: shift() and tshift(). In short,\\nthe difference between them is that shift() shifts the data, while tshift() shifts the\\nindex. In both cases, the shift is specified in multiples of the frequency.\\nHere we will both shift() and tshift() by 900 days (Figure 3-8):\\nIn[31]: fig, ax = plt.subplots(3, sharey=True)\\n        # apply a frequency to the data\\n        goog = goog.asfreq('D', method='pad')\\n        goog.plot(ax=ax[0])\\n        goog.shift(900).plot(ax=ax[1])\\n        goog.tshift(900).plot(ax=ax[2])\\n        # legends and annotations\\n        local_max = pd.to_datetime('2007-11-05')\\n        offset = pd.Timedelta(900, 'D')\\n        ax[0].legend(['input'], loc=2)\\n        ax[0].get_xticklabels()[4].set(weight='heavy', color='red')\\n        ax[0].axvline(local_max, alpha=0.3, color='red')\\nWorking with Time Series \\n| \\n199\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 217}, page_content=\"ax[1].legend(['shift(900)'], loc=2)\\n        ax[1].get_xticklabels()[4].set(weight='heavy', color='red')\\n        ax[1].axvline(local_max + offset, alpha=0.3, color='red')\\n        ax[2].legend(['tshift(900)'], loc=2)\\n        ax[2].get_xticklabels()[1].set(weight='heavy', color='red')\\n        ax[2].axvline(local_max + offset, alpha=0.3, color='red');\\nFigure 3-8. Comparison between shift and tshift\\nWe see here that shift(900) shifts the data by 900 days, pushing some of it off the\\nend of the graph (and leaving NA values at the other end), while tshift(900) shifts\\nthe index values by 900 days.\\nA common context for this type of shift is computing differences over time. For\\nexample, we use shifted values to compute the one-year return on investment for\\nGoogle stock over the course of the dataset (Figure 3-9):\\nIn[32]: ROI = 100 * (goog.tshift(-365) / goog - 1)\\n        ROI.plot()\\n        plt.ylabel('% Return on Investment');\\n200 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 218}, page_content=\"Figure 3-9. Return on investment to present day for Google stock\\nThis helps us to see the overall trend in Google stock: thus far, the most profitable\\ntimes to invest in Google have been (unsurprisingly, in retrospect) shortly after its\\nIPO, and in the middle of the 2009 recession.\\nRolling windows\\nRolling statistics are a third type of time series–specific operation implemented by\\nPandas. These can be accomplished via the rolling() attribute of Series and Data\\nFrame objects, which returns a view similar to what we saw with the groupby opera‐\\ntion (see “Aggregation and Grouping” on page 158). This rolling view makes available\\na number of aggregation operations by default.\\nFor example, here is the one-year centered rolling mean and standard deviation of the\\nGoogle stock prices (Figure 3-10):\\nIn[33]: rolling = goog.rolling(365, center=True)\\n        data = pd.DataFrame({'input': goog,\\n                             'one-year rolling_mean': rolling.mean(),\\n                             'one-year rolling_std': rolling.std()})\\n        ax = data.plot(style=['-', '--', ':'])\\n        ax.lines[0].set_alpha(0.3)\\nWorking with Time Series \\n| \\n201\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 219}, page_content='Figure 3-10. Rolling statistics on Google stock prices\\nAs with groupby operations, the aggregate() and apply() methods can be used for\\ncustom rolling computations.\\nWhere to Learn More\\nThis section has provided only a brief summary of some of the most essential features\\nof time series tools provided by Pandas; for a more complete discussion, you can refer\\nto the “Time Series/Date” section of the Pandas online documentation.\\nAnother excellent resource is the textbook Python for Data Analysis by Wes McKin‐\\nney (O’Reilly, 2012). Although it is now a few years old, it is an invaluable resource on\\nthe use of Pandas. In particular, this book emphasizes time series tools in the context\\nof business and finance, and focuses much more on particular details of business cal‐\\nendars, time zones, and related topics.\\nAs always, you can also use the IPython help functionality to explore and try further\\noptions available to the functions and methods discussed here. I find this often is the\\nbest way to learn a new Python tool.\\nExample: Visualizing Seattle Bicycle Counts\\nAs a more involved example of working with some time series data, let’s take a look at\\nbicycle counts on Seattle’s Fremont Bridge. This data comes from an automated bicy‐\\ncle counter, installed in late 2012, which has inductive sensors on the east and west\\nsidewalks of the bridge. The hourly bicycle counts can be downloaded from http://\\ndata.seattle.gov/; here is the direct link to the dataset.\\nAs of summer 2016, the CSV can be downloaded as follows:\\n202 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 220}, page_content=\"In[34]:\\n# !curl -o FremontBridge.csv\\n# https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD\\nOnce this dataset is downloaded, we can use Pandas to read the CSV output into a\\nDataFrame. We will specify that we want the Date as an index, and we want these\\ndates to be automatically parsed:\\nIn[35]:\\ndata = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\\ndata.head()\\nOut[35]:                     Fremont Bridge West Sidewalk  \\\\\\\\\\n         Date\\n         2012-10-03 00:00:00                           4.0\\n         2012-10-03 01:00:00                           4.0\\n         2012-10-03 02:00:00                           1.0\\n         2012-10-03 03:00:00                           2.0\\n         2012-10-03 04:00:00                           6.0\\n                              Fremont Bridge East Sidewalk\\n         Date\\n         2012-10-03 00:00:00                           9.0\\n         2012-10-03 01:00:00                           6.0\\n         2012-10-03 02:00:00                           1.0\\n         2012-10-03 03:00:00                           3.0\\n         2012-10-03 04:00:00                           1.0\\nFor convenience, we’ll further process this dataset by shortening the column names\\nand adding a “Total” column:\\nIn[36]: data.columns = ['West', 'East']\\n        data['Total'] = data.eval('West + East')\\nNow let’s take a look at the summary statistics for this data:\\nIn[37]: data.dropna().describe()\\nOut[37]:                West          East         Total\\n         count  33544.000000  33544.000000  33544.000000\\n         mean      61.726568     53.541706    115.268275\\n         std       83.210813     76.380678    144.773983\\n         min        0.000000      0.000000      0.000000\\n         25%        8.000000      7.000000     16.000000\\n         50%       33.000000     28.000000     64.000000\\n         75%       80.000000     66.000000    151.000000\\n         max      825.000000    717.000000   1186.000000\\nWorking with Time Series \\n| \\n203\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 221}, page_content=\"Visualizing the data\\nWe can gain some insight into the dataset by visualizing it. Let’s start by plotting the\\nraw data (Figure 3-11):\\nIn[38]: %matplotlib inline\\n        import seaborn; seaborn.set()\\nIn[39]: data.plot()\\n        plt.ylabel('Hourly Bicycle Count');\\nFigure 3-11. Hourly bicycle counts on Seattle’s Fremont bridge\\nThe ~25,000 hourly samples are far too dense for us to make much sense of. We can\\ngain more insight by resampling the data to a coarser grid. Let’s resample by week\\n(Figure 3-12):\\nIn[40]: weekly = data.resample('W').sum()\\n        weekly.plot(style=[':', '--', '-'])\\n        plt.ylabel('Weekly bicycle count');\\nThis shows us some interesting seasonal trends: as you might expect, people bicycle\\nmore in the summer than in the winter, and even within a particular season the bicy‐\\ncle use varies from week to week (likely dependent on weather; see “In Depth: Linear\\nRegression” on page 390 where we explore this further).\\n204 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 222}, page_content=\"Figure 3-12. Weekly bicycle crossings of Seattle’s Fremont bridge\\nAnother way that comes in handy for aggregating the data is to use a rolling mean,\\nutilizing the pd.rolling_mean() function. Here we’ll do a 30-day rolling mean of our\\ndata, making sure to center the window (Figure 3-13):\\nIn[41]: daily = data.resample('D').sum()\\n        daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])\\n        plt.ylabel('mean hourly count');\\nFigure 3-13. Rolling mean of weekly bicycle counts\\nWorking with Time Series \\n| \\n205\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 223}, page_content=\"The jaggedness of the result is due to the hard cutoff of the window. We can get a\\nsmoother version of a rolling mean using a window function—for example, a Gaus‐\\nsian window. The following code (visualized in Figure 3-14) specifies both the width\\nof the window (we chose 50 days) and the width of the Gaussian within the window\\n(we chose 10 days):\\nIn[42]:\\ndaily.rolling(50, center=True,\\n              win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);\\nFigure 3-14. Gaussian smoothed weekly bicycle counts\\nDigging into the data\\nWhile the smoothed data views in Figure 3-14 are useful to get an idea of the general\\ntrend in the data, they hide much of the interesting structure. For example, we might\\nwant to look at the average traffic as a function of the time of day. We can do this\\nusing the GroupBy functionality discussed in “Aggregation and Grouping” on page\\n158 (Figure 3-15):\\nIn[43]: by_time = data.groupby(data.index.time).mean()\\n        hourly_ticks = 4 * 60 * 60 * np.arange(6)\\n        by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);\\nThe hourly traffic is a strongly bimodal distribution, with peaks around 8:00 in the\\nmorning and 5:00 in the evening. This is likely evidence of a strong component of\\ncommuter traffic crossing the bridge. This is further evidenced by the differences\\nbetween the western sidewalk (generally used going toward downtown Seattle),\\nwhich peaks more strongly in the morning, and the eastern sidewalk (generally used\\ngoing away from downtown Seattle), which peaks more strongly in the evening.\\n206 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 224}, page_content=\"Figure 3-15. Average hourly bicycle counts\\nWe also might be curious about how things change based on the day of the week.\\nAgain, we can do this with a simple groupby (Figure 3-16):\\nIn[44]: by_weekday = data.groupby(data.index.dayofweek).mean()\\n        by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\\n        by_weekday.plot(style=[':', '--', '-']);\\nFigure 3-16. Average daily bicycle counts\\nThis shows a strong distinction between weekday and weekend totals, with around\\ntwice as many average riders crossing the bridge on Monday through Friday than on\\nSaturday and Sunday.\\nWorking with Time Series \\n| \\n207\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 225}, page_content=\"With this in mind, let’s do a compound groupby and look at the hourly trend on\\nweekdays versus weekends. We’ll start by grouping by both a flag marking the week‐\\nend, and the time of day:\\nIn[45]: weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')\\n        by_time = data.groupby([weekend, data.index.time]).mean()\\nNow we’ll use some of the Matplotlib tools described in “Multiple Subplots” on page\\n262 to plot two panels side by side (Figure 3-17):\\nIn[46]: import matplotlib.pyplot as plt\\n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\\n        by_time.ix['Weekday'].plot(ax=ax[0], title='Weekdays',\\n                                   xticks=hourly_ticks, style=[':', '--', '-'])\\n        by_time.ix['Weekend'].plot(ax=ax[1], title='Weekends',\\n                                   xticks=hourly_ticks, style=[':', '--', '-']);\\nFigure 3-17. Average hourly bicycle counts by weekday and weekend\\nThe result is very interesting: we see a bimodal commute pattern during the work\\nweek, and a unimodal recreational pattern during the weekends. It would be interest‐\\ning to dig through this data in more detail, and examine the effect of weather, temper‐\\nature, time of year, and other factors on people’s commuting patterns; for further\\ndiscussion, see my blog post “Is Seattle Really Seeing an Uptick In Cycling?”, which\\nuses a subset of this data. We will also revisit this dataset in the context of modeling in\\n“In Depth: Linear Regression” on page 390.\\nHigh-Performance Pandas: eval() and query()\\nAs we’ve already seen in previous chapters, the power of the PyData stack is built\\nupon the ability of NumPy and Pandas to push basic operations into C via an intu‐\\nitive syntax: examples are vectorized/broadcasted operations in NumPy, and\\ngrouping-type operations in Pandas. While these abstractions are efficient and effec‐\\n208 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 226}, page_content='tive for many common use cases, they often rely on the creation of temporary inter‐\\nmediate objects, which can cause undue overhead in computational time and\\nmemory use.\\nAs of version 0.13 (released January 2014), Pandas includes some experimental tools\\nthat allow you to directly access C-speed operations without costly allocation of inter‐\\nmediate arrays. These are the eval() and query() functions, which rely on the\\nNumexpr package. In this notebook we will walk through their use and give some\\nrules of thumb about when you might think about using them.\\nMotivating query() and eval(): Compound Expressions\\nWe’ve seen previously that NumPy and Pandas support fast vectorized operations; for\\nexample, when you are adding the elements of two arrays:\\nIn[1]: import numpy as np\\n       rng = np.random.RandomState(42)\\n       x = rng.rand(1E6)\\n       y = rng.rand(1E6)\\n       %timeit x + y\\n100 loops, best of 3: 3.39 ms per loop\\nAs discussed in “Computation on NumPy Arrays: Universal Functions” on page 50,\\nthis is much faster than doing the addition via a Python loop or comprehension:\\nIn[2]:\\n%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)),\\n                     dtype=x.dtype, count=len(x))\\n1 loop, best of 3: 266 ms per loop\\nBut this abstraction can become less efficient when you are computing compound\\nexpressions. For example, consider the following expression:\\nIn[3]: mask = (x > 0.5) & (y < 0.5)\\nBecause NumPy evaluates each subexpression, this is roughly equivalent to the\\nfollowing:\\nIn[4]: tmp1 = (x > 0.5)\\n       tmp2 = (y < 0.5)\\n       mask = tmp1 & tmp2\\nIn other words, every intermediate step is explicitly allocated in memory. If the x and y\\narrays are very large, this can lead to significant memory and computational over‐\\nhead. The Numexpr library gives you the ability to compute this type of compound\\nexpression element by element, without the need to allocate full intermediate arrays.\\nThe Numexpr documentation has more details, but for the time being it is sufficient\\nto say that the library accepts a string giving the NumPy-style expression you’d like to\\ncompute:\\nHigh-Performance Pandas: eval() and query() \\n| \\n209'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 227}, page_content=\"In[5]: import numexpr\\n       mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')\\n       np.allclose(mask, mask_numexpr)\\nOut[5]: True\\nThe benefit here is that Numexpr evaluates the expression in a way that does not use\\nfull-sized temporary arrays, and thus can be much more efficient than NumPy, espe‐\\ncially for large arrays. The Pandas eval() and query() tools that we will discuss here\\nare conceptually similar, and depend on the Numexpr package.\\npandas.eval() for Efficient Operations\\nThe eval() function in Pandas uses string expressions to efficiently compute opera‐\\ntions using DataFrames. For example, consider the following DataFrames:\\nIn[6]: import pandas as pd\\n       nrows, ncols = 100000, 100\\n       rng = np.random.RandomState(42)\\n       df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols))\\n                             for i in range(4))\\nTo compute the sum of all four DataFrames using the typical Pandas approach, we can\\njust write the sum:\\nIn[7]: %timeit df1 + df2 + df3 + df4\\n10 loops, best of 3: 87.1 ms per loop\\nWe can compute the same result via pd.eval by constructing the expression as a\\nstring:\\nIn[8]: %timeit pd.eval('df1 + df2 + df3 + df4')\\n10 loops, best of 3: 42.2 ms per loop\\nThe eval() version of this expression is about 50% faster (and uses much less mem‐\\nory), while giving the same result:\\nIn[9]: np.allclose(df1 + df2 + df3 + df4,\\n                   pd.eval('df1 + df2 + df3 + df4'))\\nOut[9]: True\\nOperations supported by pd.eval()\\nAs of Pandas v0.16, pd.eval() supports a wide range of operations. To demonstrate\\nthese, we’ll use the following integer DataFrames:\\nIn[10]: df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3)))\\n                                   for i in range(5))\\nArithmetic operators.    pd.eval() supports all arithmetic operators. For example:\\n210 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 228}, page_content=\"In[11]: result1 = -df1 * df2 / (df3 + df4) - df5\\n        result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')\\n        np.allclose(result1, result2)\\nOut[11]: True\\nComparison operators.    pd.eval() supports all comparison operators, including\\nchained expressions:\\nIn[12]: result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)\\n        result2 = pd.eval('df1 < df2 <= df3 != df4')\\n        np.allclose(result1, result2)\\nOut[12]: True\\nBitwise operators.    pd.eval() supports the & and | bitwise operators:\\nIn[13]: result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)\\n        result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')\\n        np.allclose(result1, result2)\\nOut[13]: True\\nIn addition, it supports the use of the literal and and or in Boolean expressions:\\nIn[14]: result3 = pd.eval('(df1 < 0.5) and (df2 < 0.5) or (df3 < df4)')\\n        np.allclose(result1, result3)\\nOut[14]: True\\nObject attributes and indices.    pd.eval() supports access to object attributes via the\\nobj.attr syntax, and indexes via the obj[index] syntax:\\nIn[15]: result1 = df2.T[0] + df3.iloc[1]\\n        result2 = pd.eval('df2.T[0] + df3.iloc[1]')\\n        np.allclose(result1, result2)\\nOut[15]: True\\nOther operations.    Other operations, such as function calls, conditional statements,\\nloops, and other more involved constructs, are currently not implemented in\\npd.eval(). If you’d like to execute these more complicated types of expressions, you\\ncan use the Numexpr library itself.\\nDataFrame.eval() for Column-Wise Operations\\nJust as Pandas has a top-level pd.eval() function, DataFrames have an eval()\\nmethod that works in similar ways. The benefit of the eval() method is that columns\\ncan be referred to by name. We’ll use this labeled array as an example:\\nIn[16]: df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C'])\\n        df.head()\\nHigh-Performance Pandas: eval() and query() \\n| \\n211\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 229}, page_content='Out[16]:           A         B         C\\n         0  0.375506  0.406939  0.069938\\n         1  0.069087  0.235615  0.154374\\n         2  0.677945  0.433839  0.652324\\n         3  0.264038  0.808055  0.347197\\n         4  0.589161  0.252418  0.557789\\nUsing pd.eval() as above, we can compute expressions with the three columns like\\nthis:\\nIn[17]: result1 = (df[\\'A\\'] + df[\\'B\\']) / (df[\\'C\\'] - 1)\\n        result2 = pd.eval(\"(df.A + df.B) / (df.C - 1)\")\\n        np.allclose(result1, result2)\\nOut[17]: True\\nThe DataFrame.eval() method allows much more succinct evaluation of expressions\\nwith the columns:\\nIn[18]: result3 = df.eval(\\'(A + B) / (C - 1)\\')\\n        np.allclose(result1, result3)\\nOut[18]: True\\nNotice here that we treat column names as variables within the evaluated expression,\\nand the result is what we would wish.\\nAssignment in DataFrame.eval()\\nIn addition to the options just discussed, DataFrame.eval() also allows assignment\\nto any column. Let’s use the DataFrame from before, which has columns \\'A\\', \\'B\\', and\\n\\'C\\':\\nIn[19]: df.head()\\nOut[19]:           A         B         C\\n         0  0.375506  0.406939  0.069938\\n         1  0.069087  0.235615  0.154374\\n         2  0.677945  0.433839  0.652324\\n         3  0.264038  0.808055  0.347197\\n         4  0.589161  0.252418  0.557789\\nWe can use df.eval() to create a new column \\'D\\' and assign to it a value computed\\nfrom the other columns:\\nIn[20]: df.eval(\\'D = (A + B) / C\\', inplace=True)\\n        df.head()\\nOut[20]:           A         B         C          D\\n         0  0.375506  0.406939  0.069938  11.187620\\n         1  0.069087  0.235615  0.154374   1.973796\\n         2  0.677945  0.433839  0.652324   1.704344\\n         3  0.264038  0.808055  0.347197   3.087857\\n         4  0.589161  0.252418  0.557789   1.508776\\n212 \\n| \\nChapter 3: Data Manipulation with Pandas'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 230}, page_content=\"In the same way, any existing column can be modified:\\nIn[21]: df.eval('D = (A - B) / C', inplace=True)\\n        df.head()\\nOut[21]:          A         B         C         D\\n        0  0.375506  0.406939  0.069938 -0.449425\\n         1  0.069087  0.235615  0.154374 -1.078728\\n         2  0.677945  0.433839  0.652324  0.374209\\n         3  0.264038  0.808055  0.347197 -1.566886\\n         4  0.589161  0.252418  0.557789  0.603708\\nLocal variables in DataFrame.eval()\\nThe DataFrame.eval() method supports an additional syntax that lets it work with\\nlocal Python variables. Consider the following:\\nIn[22]: column_mean = df.mean(1)\\n        result1 = df['A'] + column_mean\\n        result2 = df.eval('A + @column_mean')\\n        np.allclose(result1, result2)\\nOut[22]: True\\nThe @ character here marks a variable name rather than a column name, and lets you\\nefficiently evaluate expressions involving the two “namespaces”: the namespace of\\ncolumns, and the namespace of Python objects. Notice that this @ character is only\\nsupported by the DataFrame.eval() method, not by the pandas.eval() function,\\nbecause the pandas.eval() function only has access to the one (Python) namespace.\\nDataFrame.query() Method\\nThe DataFrame has another method based on evaluated strings, called the query()\\nmethod. Consider the following:\\nIn[23]: result1 = df[(df.A < 0.5) & (df.B < 0.5)]\\n        result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]')\\n        np.allclose(result1, result2)\\nOut[23]: True\\nAs with the example used in our discussion of DataFrame.eval(), this is an expres‐\\nsion involving columns of the DataFrame. It cannot be expressed using the Data\\nFrame.eval() syntax, however! Instead, for this type of filtering operation, you can\\nuse the query() method:\\nIn[24]: result2 = df.query('A < 0.5 and B < 0.5')\\n        np.allclose(result1, result2)\\nOut[24]: True\\nHigh-Performance Pandas: eval() and query() \\n| \\n213\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 231}, page_content=\"In addition to being a more efficient computation, compared to the masking expres‐\\nsion this is much easier to read and understand. Note that the query() method also\\naccepts the @ flag to mark local variables:\\nIn[25]: Cmean = df['C'].mean()\\n        result1 = df[(df.A < Cmean) & (df.B < Cmean)]\\n        result2 = df.query('A < @Cmean and B < @Cmean')\\n        np.allclose(result1, result2)\\nOut[25]: True\\nPerformance: When to Use These Functions\\nWhen considering whether to use these functions, there are two considerations: com‐\\nputation time and memory use. Memory use is the most predictable aspect. As already\\nmentioned, every compound expression involving NumPy arrays or Pandas Data\\nFrames will result in implicit creation of temporary arrays: For example, this:\\nIn[26]: x = df[(df.A < 0.5) & (df.B < 0.5)]\\nis roughly equivalent to this:\\nIn[27]: tmp1 = df.A < 0.5\\n        tmp2 = df.B < 0.5\\n        tmp3 = tmp1 & tmp2\\n        x = df[tmp3]\\nIf the size of the temporary DataFrames is significant compared to your available sys‐\\ntem memory (typically several gigabytes), then it’s a good idea to use an eval() or\\nquery() expression. You can check the approximate size of your array in bytes using\\nthis:\\nIn[28]: df.values.nbytes\\nOut[28]: 32000\\nOn the performance side, eval() can be faster even when you are not maxing out\\nyour system memory. The issue is how your temporary DataFrames compare to the\\nsize of the L1 or L2 CPU cache on your system (typically a few megabytes in 2016); if\\nthey are much bigger, then eval() can avoid some potentially slow movement of val‐\\nues between the different memory caches. In practice, I find that the difference in\\ncomputation time between the traditional methods and the eval/query method is\\nusually not significant—if anything, the traditional method is faster for smaller\\narrays! The benefit of eval/query is mainly in the saved memory, and the sometimes\\ncleaner syntax they offer.\\nWe’ve covered most of the details of eval() and query() here; for more information\\non these, you can refer to the Pandas documentation. In particular, different parsers\\nand engines can be specified for running these queries; for details on this, see the dis‐\\ncussion within the “Enhancing Performance” section.\\n214 \\n| \\nChapter 3: Data Manipulation with Pandas\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 232}, page_content='Further Resources\\nIn this chapter, we’ve covered many of the basics of using Pandas effectively for data\\nanalysis. Still, much has been omitted from our discussion. To learn more about Pan‐\\ndas, I recommend the following resources:\\nPandas online documentation\\nThis is the go-to source for complete documentation of the package. While the\\nexamples in the documentation tend to be small generated datasets, the descrip‐\\ntion of the options is complete and generally very useful for understanding the\\nuse of various functions.\\nPython for Data Analysis\\nWritten by Wes McKinney (the original creator of Pandas), this book contains\\nmuch more detail on the package than we had room for in this chapter. In partic‐\\nular, he takes a deep dive into tools for time series, which were his bread and but‐\\nter as a financial consultant. The book also has many entertaining examples of\\napplying Pandas to gain insight from real-world datasets. Keep in mind, though,\\nthat the book is now several years old, and the Pandas package has quite a few\\nnew features that this book does not cover (but be on the lookout for a new edi‐\\ntion in 2017).\\nPandas on Stack Overflow\\nPandas has so many users that any question you have has likely been asked and\\nanswered on Stack Overflow. Using Pandas is a case where some Google-Fu is\\nyour best friend. Simply go to your favorite search engine and type in the ques‐\\ntion, problem, or error you’re coming across—more than likely you’ll find your\\nanswer on a Stack Overflow page.\\nPandas on PyVideo\\nFrom PyCon to SciPy to PyData, many conferences have featured tutorials from\\nPandas developers and power users. The PyCon tutorials in particular tend to be\\ngiven by very well-vetted presenters.\\nMy hope is that, by using these resources, combined with the walk-through given in\\nthis chapter, you’ll be poised to use Pandas to tackle any data analysis problem you\\ncome across!\\nFurther Resources \\n| \\n215'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 233}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 234}, page_content='CHAPTER 4\\nVisualization with Matplotlib\\nWe’ll now take an in-depth look at the Matplotlib tool for visualization in Python.\\nMatplotlib is a multiplatform data visualization library built on NumPy arrays, and\\ndesigned to work with the broader SciPy stack. It was conceived by John Hunter in\\n2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting\\nvia gnuplot from the IPython command line. IPython’s creator, Fernando Perez, was\\nat the time scrambling to finish his PhD, and let John know he wouldn’t have time to\\nreview the patch for several months. John took this as a cue to set out on his own, and\\nthe Matplotlib package was born, with version 0.1 released in 2003. It received an\\nearly boost when it was adopted as the plotting package of choice of the Space Tele‐\\nscope Science Institute (the folks behind the Hubble Telescope), which financially\\nsupported Matplotlib’s development and greatly expanded its capabilities.\\nOne of Matplotlib’s most important features is its ability to play well with many oper‐\\nating systems and graphics backends. Matplotlib supports dozens of backends and\\noutput types, which means you can count on it to work regardless of which operating\\nsystem you are using or which output format you wish. This cross-platform,\\neverything-to-everyone approach has been one of the great strengths of Matplotlib. It\\nhas led to a large userbase, which in turn has led to an active developer base and Mat‐\\nplotlib’s powerful tools and ubiquity within the scientific Python world.\\nIn recent years, however, the interface and style of Matplotlib have begun to show\\ntheir age. Newer tools like ggplot and ggvis in the R language, along with web visuali‐\\nzation toolkits based on D3js and HTML5 canvas, often make Matplotlib feel clunky\\nand old-fashioned. Still, I’m of the opinion that we cannot ignore Matplotlib’s\\nstrength as a well-tested, cross-platform graphics engine. Recent Matplotlib versions\\nmake it relatively easy to set new global plotting styles (see “Customizing Matplotlib:\\nConfigurations and Stylesheets” on page 282), and people have been developing new\\npackages that build on its powerful internals to drive Matplotlib via cleaner, more\\n217'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 235}, page_content=\"modern APIs—for example, Seaborn (discussed in “Visualization with Seaborn” on\\npage 311), ggplot, HoloViews, Altair, and even Pandas itself can be used as wrappers\\naround Matplotlib’s API. Even with wrappers like these, it is still often useful to dive\\ninto Matplotlib’s syntax to adjust the final plot output. For this reason, I believe that\\nMatplotlib itself will remain a vital piece of the data visualization stack, even if new\\ntools mean the community gradually moves away from using the Matplotlib API\\ndirectly.\\nGeneral Matplotlib Tips\\nBefore we dive into the details of creating visualizations with Matplotlib, there are a\\nfew useful things you should know about using the package.\\nImporting matplotlib\\nJust as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will\\nuse some standard shorthands for Matplotlib imports:\\nIn[1]: import matplotlib as mpl\\n       import matplotlib.pyplot as plt\\nThe plt interface is what we will use most often, as we’ll see throughout this chapter.\\nSetting Styles\\nWe will use the plt.style directive to choose appropriate aesthetic styles for our fig‐\\nures. Here we will set the classic style, which ensures that the plots we create use the\\nclassic Matplotlib style:\\nIn[2]: plt.style.use('classic')\\nThroughout this section, we will adjust this style as needed. Note that the stylesheets\\nused here are supported as of Matplotlib version 1.5; if you are using an earlier ver‐\\nsion of Matplotlib, only the default style is available. For more information on style‐\\nsheets, see “Customizing Matplotlib: Configurations and Stylesheets” on page 282.\\nshow() or No show()? How to Display Your Plots\\nA visualization you can’t see won’t be of much use, but just how you view your Mat‐\\nplotlib plots depends on the context. The best use of Matplotlib differs depending on\\nhow you are using it; roughly, the three applicable contexts are using Matplotlib in a\\nscript, in an IPython terminal, or in an IPython notebook.\\n218 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 236}, page_content='Plotting from a script\\nIf you are using Matplotlib from within a script, the function plt.show() is your\\nfriend. plt.show() starts an event loop, looks for all currently active figure objects,\\nand opens one or more interactive windows that display your figure or figures.\\nSo, for example, you may have a file called myplot.py containing the following:\\n# ------- file: myplot.py ------\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.linspace(0, 10, 100)\\nplt.plot(x, np.sin(x))\\nplt.plot(x, np.cos(x))\\nplt.show()\\nYou can then run this script from the command-line prompt, which will result in a\\nwindow opening with your figure displayed:\\n$ python myplot.py\\nThe plt.show() command does a lot under the hood, as it must interact with your\\nsystem’s interactive graphical backend. The details of this operation can vary greatly\\nfrom system to system and even installation to installation, but Matplotlib does its\\nbest to hide all these details from you.\\nOne thing to be aware of: the plt.show() command should be used only once per\\nPython session, and is most often seen at the very end of the script. Multiple show()\\ncommands can lead to unpredictable backend-dependent behavior, and should\\nmostly be avoided.\\nPlotting from an IPython shell\\nIt can be very convenient to use Matplotlib interactively within an IPython shell (see\\nChapter 1). IPython is built to work well with Matplotlib if you specify Matplotlib\\nmode. To enable this mode, you can use the %matplotlib magic command after start‐\\ning ipython:\\nIn [1]: %matplotlib\\nUsing matplotlib backend: TkAgg\\nIn [2]: import matplotlib.pyplot as plt\\nAt this point, any plt plot command will cause a figure window to open, and further\\ncommands can be run to update the plot. Some changes (such as modifying proper‐\\nties of lines that are already drawn) will not draw automatically; to force an update,\\nuse plt.draw(). Using plt.show() in Matplotlib mode is not required.\\nGeneral Matplotlib Tips \\n| \\n219'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 237}, page_content=\"Plotting from an IPython notebook\\nThe IPython notebook is a browser-based interactive data analysis tool that can com‐\\nbine narrative, code, graphics, HTML elements, and much more into a single exe‐\\ncutable document (see Chapter 1).\\nPlotting interactively within an IPython notebook can be done with the %matplotlib\\ncommand, and works in a similar way to the IPython shell. In the IPython notebook,\\nyou also have the option of embedding graphics directly in the notebook, with two\\npossible options:\\n• %matplotlib notebook will lead to interactive plots embedded within the\\nnotebook\\n• %matplotlib inline will lead to static images of your plot embedded in the\\nnotebook\\nFor this book, we will generally opt for %matplotlib inline:\\nIn[3]: %matplotlib inline\\nAfter you run this command (it needs to be done only once per kernel/session), any\\ncell within the notebook that creates a plot will embed a PNG image of the resulting\\ngraphic (Figure 4-1):\\nIn[4]: import numpy as np\\n       x = np.linspace(0, 10, 100)\\n       fig = plt.figure()\\n       plt.plot(x, np.sin(x), '-')\\n       plt.plot(x, np.cos(x), '--');\\nFigure 4-1. Basic plotting example\\n220 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 238}, page_content=\"Saving Figures to File\\nOne nice feature of Matplotlib is the ability to save figures in a wide variety of for‐\\nmats. You can save a figure using the savefig() command. For example, to save the\\nprevious figure as a PNG file, you can run this:\\nIn[5]: fig.savefig('my_figure.png')\\nWe now have a file called my_figure.png in the current working directory:\\nIn[6]: !ls -lh my_figure.png\\n-rw-r--r--  1 jakevdp  staff    16K Aug 11 10:59 my_figure.png\\nTo confirm that it contains what we think it contains, let’s use the IPython Image\\nobject to display the contents of this file (Figure 4-2):\\nIn[7]: from IPython.display import Image\\n       Image('my_figure.png')\\nFigure 4-2. PNG rendering of the basic plot\\nIn savefig(), the file format is inferred from the extension of the given filename.\\nDepending on what backends you have installed, many different file formats are\\navailable. You can find the list of supported file types for your system by using the\\nfollowing method of the figure canvas object:\\nIn[8]: fig.canvas.get_supported_filetypes()\\nOut[8]: {'eps': 'Encapsulated Postscript',\\n         'jpeg': 'Joint Photographic Experts Group',\\n         'jpg': 'Joint Photographic Experts Group',\\n         'pdf': 'Portable Document Format',\\n         'pgf': 'PGF code for LaTeX',\\n         'png': 'Portable Network Graphics',\\n         'ps': 'Postscript',\\n         'raw': 'Raw RGBA bitmap',\\n         'rgba': 'Raw RGBA bitmap',\\nGeneral Matplotlib Tips \\n| \\n221\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 239}, page_content=\"'svg': 'Scalable Vector Graphics',\\n         'svgz': 'Scalable Vector Graphics',\\n         'tif': 'Tagged Image File Format',\\n         'tiff': 'Tagged Image File Format'}\\nNote that when saving your figure, it’s not necessary to use plt.show() or related\\ncommands discussed earlier.\\nTwo Interfaces for the Price of One\\nA potentially confusing feature of Matplotlib is its dual interfaces: a convenient\\nMATLAB-style state-based interface, and a more powerful object-oriented interface.\\nWe’ll quickly highlight the differences between the two here.\\nMATLAB-style interface\\nMatplotlib was originally written as a Python alternative for MATLAB users, and\\nmuch of its syntax reflects that fact. The MATLAB-style tools are contained in the\\npyplot (plt) interface. For example, the following code will probably look quite\\nfamiliar to MATLAB users (Figure 4-3):\\nIn[9]: plt.figure()  # create a plot figure\\n       # create the first of two panels and set current axis\\n       plt.subplot(2, 1, 1) # (rows, columns, panel number)\\n       plt.plot(x, np.sin(x))\\n       # create the second panel and set current axis\\n       plt.subplot(2, 1, 2)\\n       plt.plot(x, np.cos(x));\\nFigure 4-3. Subplots using the MATLAB-style interface\\nIt’s important to note that this interface is stateful: it keeps track of the “current” figure\\nand axes, which are where all plt commands are applied. You can get a reference to\\n222 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 240}, page_content='these using the plt.gcf() (get current figure) and plt.gca() (get current axes)\\nroutines.\\nWhile this stateful interface is fast and convenient for simple plots, it is easy to run\\ninto problems. For example, once the second panel is created, how can we go back\\nand add something to the first? This is possible within the MATLAB-style interface,\\nbut a bit clunky. Fortunately, there is a better way.\\nObject-oriented interface\\nThe object-oriented interface is available for these more complicated situations, and\\nfor when you want more control over your figure. Rather than depending on some\\nnotion of an “active” figure or axes, in the object-oriented interface the plotting func‐\\ntions are methods of explicit Figure and Axes objects. To re-create the previous plot\\nusing this style of plotting, you might do the following (Figure 4-4):\\nIn[10]: # First create a grid of plots\\n        # ax will be an array of two Axes objects\\n        fig, ax = plt.subplots(2)\\n        # Call plot() method on the appropriate object\\n        ax[0].plot(x, np.sin(x))\\n        ax[1].plot(x, np.cos(x));\\nFigure 4-4. Subplots using the object-oriented interface\\nFor more simple plots, the choice of which style to use is largely a matter of prefer‐\\nence, but the object-oriented approach can become a necessity as plots become more\\ncomplicated. Throughout this chapter, we will switch between the MATLAB-style\\nand object-oriented interfaces, depending on what is most convenient. In most cases,\\nthe difference is as small as switching plt.plot() to ax.plot(), but there are a few\\ngotchas that we will highlight as they come up in the following sections.\\nTwo Interfaces for the Price of One \\n| \\n223'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 241}, page_content=\"Simple Line Plots\\nPerhaps the simplest of all plots is the visualization of a single function y = f x . Here\\nwe will take a first look at creating a simple plot of this type. As with all the following\\nsections, we’ll start by setting up the notebook for plotting and importing the func‐\\ntions we will use:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-whitegrid')\\n       import numpy as np\\nFor all Matplotlib plots, we start by creating a figure and an axes. In their simplest\\nform, a figure and axes can be created as follows (Figure 4-5):\\nIn[2]: fig = plt.figure()\\n       ax = plt.axes()\\nFigure 4-5. An empty gridded axes\\nIn Matplotlib, the figure (an instance of the class plt.Figure) can be thought of as a\\nsingle container that contains all the objects representing axes, graphics, text, and\\nlabels. The axes (an instance of the class plt.Axes) is what we see above: a bounding\\nbox with ticks and labels, which will eventually contain the plot elements that make\\nup our visualization. Throughout this book, we’ll commonly use the variable name\\nfig to refer to a figure instance, and ax to refer to an axes instance or group of axes\\ninstances.\\nOnce we have created an axes, we can use the ax.plot function to plot some data.\\nLet’s start with a simple sinusoid (Figure 4-6):\\nIn[3]: fig = plt.figure()\\n       ax = plt.axes()\\n       x = np.linspace(0, 10, 1000)\\n       ax.plot(x, np.sin(x));\\n224 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 242}, page_content='Figure 4-6. A simple sinusoid\\nAlternatively, we can use the pylab interface and let the figure and axes be created for\\nus in the background (Figure 4-7; see “Two Interfaces for the Price of One” on page\\n222 for a discussion of these two interfaces):\\nIn[4]: plt.plot(x, np.sin(x));\\nFigure 4-7. A simple sinusoid via the object-oriented interface\\nIf we want to create a single figure with multiple lines, we can simply call the plot\\nfunction multiple times (Figure 4-8):\\nIn[5]: plt.plot(x, np.sin(x))\\n       plt.plot(x, np.cos(x));\\nSimple Line Plots \\n| \\n225'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 243}, page_content=\"Figure 4-8. Over-plotting multiple lines\\nThat’s all there is to plotting simple functions in Matplotlib! We’ll now dive into some\\nmore details about how to control the appearance of the axes and lines.\\nAdjusting the Plot: Line Colors and Styles\\nThe first adjustment you might wish to make to a plot is to control the line colors and\\nstyles. The plt.plot() function takes additional arguments that can be used to spec‐\\nify these. To adjust the color, you can use the color keyword, which accepts a string\\nargument representing virtually any imaginable color. The color can be specified in a\\nvariety of ways (Figure 4-9):\\nIn[6]:\\nplt.plot(x, np.sin(x - 0), color='blue')        # specify color by name\\nplt.plot(x, np.sin(x - 1), color='g')           # short color code (rgbcmyk)\\nplt.plot(x, np.sin(x - 2), color='0.75')        # Grayscale between 0 and 1\\nplt.plot(x, np.sin(x - 3), color='#FFDD44')     # Hex code (RRGGBB from 00 to FF)\\nplt.plot(x, np.sin(x - 4), color=(1.0,0.2,0.3)) # RGB tuple, values 0 and 1\\nplt.plot(x, np.sin(x - 5), color='chartreuse'); # all HTML color names supported\\nFigure 4-9. Controlling the color of plot elements\\n226 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 244}, page_content=\"If no color is specified, Matplotlib will automatically cycle through a set of default\\ncolors for multiple lines.\\nSimilarly, you can adjust the line style using the linestyle keyword (Figure 4-10):\\nIn[7]: plt.plot(x, x + 0, linestyle='solid')\\n       plt.plot(x, x + 1, linestyle='dashed')\\n       plt.plot(x, x + 2, linestyle='dashdot')\\n       plt.plot(x, x + 3, linestyle='dotted');\\n       # For short, you can use the following codes:\\n       plt.plot(x, x + 4, linestyle='-')  # solid\\n       plt.plot(x, x + 5, linestyle='--') # dashed\\n       plt.plot(x, x + 6, linestyle='-.') # dashdot\\n       plt.plot(x, x + 7, linestyle=':');  # dotted\\nFigure 4-10. Example of various line styles\\nIf you would like to be extremely terse, these linestyle and color codes can be com‐\\nbined into a single nonkeyword argument to the plt.plot() function (Figure 4-11):\\nIn[8]: plt.plot(x, x + 0, '-g')  # solid green\\n       plt.plot(x, x + 1, '--c') # dashed cyan\\n       plt.plot(x, x + 2, '-.k') # dashdot black\\n       plt.plot(x, x + 3, ':r');  # dotted red\\nSimple Line Plots \\n| \\n227\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 245}, page_content='Figure 4-11. Controlling colors and styles with the shorthand syntax\\nThese single-character color codes reflect the standard abbreviations in the RGB\\n(Red/Green/Blue) and CMYK (Cyan/Magenta/Yellow/blacK) color systems, com‐\\nmonly used for digital color graphics.\\nThere are many other keyword arguments that can be used to fine-tune the appear‐\\nance of the plot; for more details, I’d suggest viewing the docstring of the plt.plot()\\nfunction using IPython’s help tools (see “Help and Documentation in IPython” on\\npage 3).\\nAdjusting the Plot: Axes Limits\\nMatplotlib does a decent job of choosing default axes limits for your plot, but some‐\\ntimes it’s nice to have finer control. The most basic way to adjust axis limits is to use\\nthe plt.xlim() and plt.ylim() methods (Figure 4-12):\\nIn[9]: plt.plot(x, np.sin(x))\\n       plt.xlim(-1, 11)\\n       plt.ylim(-1.5, 1.5);\\nFigure 4-12. Example of setting axis limits\\n228 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 246}, page_content=\"If for some reason you’d like either axis to be displayed in reverse, you can simply\\nreverse the order of the arguments (Figure 4-13):\\nIn[10]: plt.plot(x, np.sin(x))\\n        plt.xlim(10, 0)\\n        plt.ylim(1.2, -1.2);\\nFigure 4-13. Example of reversing the y-axis\\nA useful related method is plt.axis() (note here the potential confusion between\\naxes with an e, and axis with an i). The plt.axis() method allows you to set the x\\nand y limits with a single call, by passing a list that specifies [xmin, xmax, ymin,\\nymax] (Figure 4-14):\\nIn[11]: plt.plot(x, np.sin(x))\\n        plt.axis([-1, 11, -1.5, 1.5]);\\nFigure 4-14. Setting the axis limits with plt.axis\\nThe plt.axis() method goes even beyond this, allowing you to do things like auto‐\\nmatically tighten the bounds around the current plot (Figure 4-15):\\nIn[12]: plt.plot(x, np.sin(x))\\n        plt.axis('tight');\\nSimple Line Plots \\n| \\n229\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 247}, page_content='Figure 4-15. Example of a “tight” layout\\nIt allows even higher-level specifications, such as ensuring an equal aspect ratio so\\nthat on your screen, one unit in x is equal to one unit in y (Figure 4-16):\\nIn[13]: plt.plot(x, np.sin(x))\\n        plt.axis(\\'equal\\');\\nFigure 4-16. Example of an “equal” layout, with units matched to the output resolution\\nFor more information on axis limits and the other capabilities of the plt.axis()\\nmethod, refer to the plt.axis() docstring.\\nLabeling Plots\\nAs the last piece of this section, we’ll briefly look at the labeling of plots: titles, axis\\nlabels, and simple legends.\\nTitles and axis labels are the simplest such labels—there are methods that can be used\\nto quickly set them (Figure 4-17):\\nIn[14]: plt.plot(x, np.sin(x))\\n        plt.title(\"A Sine Curve\")\\n230 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 248}, page_content='plt.xlabel(\"x\")\\n        plt.ylabel(\"sin(x)\");\\nFigure 4-17. Examples of axis labels and title\\nYou can adjust the position, size, and style of these labels using optional arguments to\\nthe function. For more information, see the Matplotlib documentation and the doc‐\\nstrings of each of these functions.\\nWhen multiple lines are being shown within a single axes, it can be useful to create a\\nplot legend that labels each line type. Again, Matplotlib has a built-in way of quickly\\ncreating such a legend. It is done via the (you guessed it) plt.legend() method.\\nThough there are several valid ways of using this, I find it easiest to specify the label\\nof each line using the label keyword of the plot function (Figure 4-18):\\nIn[15]: plt.plot(x, np.sin(x), \\'-g\\', label=\\'sin(x)\\')\\n        plt.plot(x, np.cos(x), \\':b\\', label=\\'cos(x)\\')\\n        plt.axis(\\'equal\\')\\n        plt.legend();\\nFigure 4-18. Plot legend example\\nSimple Line Plots \\n| \\n231'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 249}, page_content=\"As you can see, the plt.legend() function keeps track of the line style and color, and\\nmatches these with the correct label. More information on specifying and formatting\\nplot legends can be found in the plt.legend() docstring; additionally, we will cover\\nsome more advanced legend options in “Customizing Plot Legends” on page 249.\\nMatplotlib Gotchas\\nWhile most plt functions translate directly to ax methods (such as plt.plot() →\\nax.plot(), plt.legend() → ax.legend(), etc.), this is not the case for all com‐\\nmands. In particular, functions to set limits, labels, and titles are slightly modified.\\nFor transitioning between MATLAB-style functions and object-oriented methods,\\nmake the following changes:\\n• plt.xlabel() → ax.set_xlabel()\\n• plt.ylabel() → ax.set_ylabel()\\n• plt.xlim() → ax.set_xlim()\\n• plt.ylim() → ax.set_ylim()\\n• plt.title() → ax.set_title()\\nIn the object-oriented interface to plotting, rather than calling these functions indi‐\\nvidually, it is often more convenient to use the ax.set() method to set all these prop‐\\nerties at once (Figure 4-19):\\nIn[16]: ax = plt.axes()\\n        ax.plot(x, np.sin(x))\\n        ax.set(xlim=(0, 10), ylim=(-2, 2),\\n               xlabel='x', ylabel='sin(x)',\\n               title='A Simple Plot');\\nFigure 4-19. Example of using ax.set to set multiple properties at once\\n232 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 250}, page_content='Simple Scatter Plots\\nAnother commonly used plot type is the simple scatter plot, a close cousin of the line\\nplot. Instead of points being joined by line segments, here the points are represented\\nindividually with a dot, circle, or other shape. We’ll start by setting up the notebook\\nfor plotting and importing the functions we will use:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use(\\'seaborn-whitegrid\\')\\n       import numpy as np\\nScatter Plots with plt.plot\\nIn the previous section, we looked at plt.plot/ax.plot to produce line plots. It turns\\nout that this same function can produce scatter plots as well (Figure 4-20):\\nIn[2]: x = np.linspace(0, 10, 30)\\n       y = np.sin(x)\\n       plt.plot(x, y, \\'o\\', color=\\'black\\');\\nFigure 4-20. Scatter plot example\\nThe third argument in the function call is a character that represents the type of sym‐\\nbol used for the plotting. Just as you can specify options such as \\'-\\' and \\'--\\' to con‐\\ntrol the line style, the marker style has its own set of short string codes. The full list of\\navailable symbols can be seen in the documentation of plt.plot, or in Matplotlib’s\\nonline documentation. Most of the possibilities are fairly intuitive, and we’ll show a\\nnumber of the more common ones here (Figure 4-21):\\nIn[3]: rng = np.random.RandomState(0)\\n       for marker in [\\'o\\', \\'.\\', \\',\\', \\'x\\', \\'+\\', \\'v\\', \\'^\\', \\'<\\', \\'>\\', \\'s\\', \\'d\\']:\\n           plt.plot(rng.rand(5), rng.rand(5), marker,\\n                    label=\"marker=\\'{0}\\'\".format(marker))\\nSimple Scatter Plots \\n| \\n233'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 251}, page_content=\"plt.legend(numpoints=1)\\n       plt.xlim(0, 1.8);\\nFigure 4-21. Demonstration of point numbers\\nFor even more possibilities, these character codes can be used together with line and\\ncolor codes to plot points along with a line connecting them (Figure 4-22):\\nIn[4]: plt.plot(x, y, '-ok');   # line (-), circle marker (o), black (k)\\nFigure 4-22. Combining line and point markers\\nAdditional keyword arguments to plt.plot specify a wide range of properties of the\\nlines and markers (Figure 4-23):\\nIn[5]: plt.plot(x, y, '-p', color='gray',\\n                markersize=15, linewidth=4,\\n                markerfacecolor='white',\\n                markeredgecolor='gray',\\n                markeredgewidth=2)\\n       plt.ylim(-1.2, 1.2);\\n234 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 252}, page_content=\"Figure 4-23. Customizing line and point numbers\\nThis type of flexibility in the plt.plot function allows for a wide variety of possible\\nvisualization options. For a full description of the options available, refer to the\\nplt.plot documentation.\\nScatter Plots with plt.scatter\\nA second, more powerful method of creating scatter plots is the plt.scatter func‐\\ntion, which can be used very similarly to the plt.plot function (Figure 4-24):\\nIn[6]: plt.scatter(x, y, marker='o');\\nFigure 4-24. A simple scatter plot\\nThe primary difference of plt.scatter from plt.plot is that it can be used to create\\nscatter plots where the properties of each individual point (size, face color, edge color,\\netc.) can be individually controlled or mapped to data.\\nLet’s show this by creating a random scatter plot with points of many colors and sizes.\\nIn order to better see the overlapping results, we’ll also use the alpha keyword to\\nadjust the transparency level (Figure 4-25):\\nSimple Scatter Plots \\n| \\n235\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 253}, page_content=\"In[7]: rng = np.random.RandomState(0)\\n       x = rng.randn(100)\\n       y = rng.randn(100)\\n       colors = rng.rand(100)\\n       sizes = 1000 * rng.rand(100)\\n       plt.scatter(x, y, c=colors, s=sizes, alpha=0.3,\\n                   cmap='viridis')\\n       plt.colorbar();  # show color scale\\nFigure 4-25. Changing size, color, and transparency in scatter points\\nNotice that the color argument is automatically mapped to a color scale (shown here\\nby the colorbar() command), and the size argument is given in pixels. In this way,\\nthe color and size of points can be used to convey information in the visualization, in\\norder to illustrate multidimensional data.\\nFor example, we might use the Iris data from Scikit-Learn, where each sample is one\\nof three types of flowers that has had the size of its petals and sepals carefully meas‐\\nured (Figure 4-26):\\nIn[8]: from sklearn.datasets import load_iris\\n       iris = load_iris()\\n       features = iris.data.T\\n       plt.scatter(features[0], features[1], alpha=0.2,\\n                   s=100*features[3], c=iris.target, cmap='viridis')\\n       plt.xlabel(iris.feature_names[0])\\n       plt.ylabel(iris.feature_names[1]);\\n236 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 254}, page_content='Figure 4-26. Using point properties to encode features of the Iris data\\nWe can see that this scatter plot has given us the ability to simultaneously explore\\nfour different dimensions of the data: the (x, y) location of each point corresponds to\\nthe sepal length and width, the size of the point is related to the petal width, and the\\ncolor is related to the particular species of flower. Multicolor and multifeature scatter\\nplots like this can be useful for both exploration and presentation of data.\\nplot Versus scatter: A Note on Efficiency\\nAside from the different features available in plt.plot and plt.scatter, why might\\nyou choose to use one over the other? While it doesn’t matter as much for small\\namounts of data, as datasets get larger than a few thousand points, plt.plot can be\\nnoticeably more efficient than plt.scatter. The reason is that plt.scatter has the\\ncapability to render a different size and/or color for each point, so the renderer must\\ndo the extra work of constructing each point individually. In plt.plot, on the other\\nhand, the points are always essentially clones of each other, so the work of determin‐\\ning the appearance of the points is done only once for the entire set of data. For large\\ndatasets, the difference between these two can lead to vastly different performance,\\nand for this reason, plt.plot should be preferred over plt.scatter for large\\ndatasets.\\nVisualizing Errors\\nFor any scientific measurement, accurate accounting for errors is nearly as important,\\nif not more important, than accurate reporting of the number itself. For example,\\nimagine that I am using some astrophysical observations to estimate the Hubble Con‐\\nstant, the local measurement of the expansion rate of the universe. I know that the\\ncurrent literature suggests a value of around 71 (km/s)/Mpc, and I measure a value of\\n74 (km/s)/Mpc with my method. Are the values consistent? The only correct answer,\\ngiven this information, is this: there is no way to know.\\nVisualizing Errors \\n| \\n237'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 255}, page_content=\"Suppose I augment this information with reported uncertainties: the current litera‐\\nture suggests a value of around 71 ± 2.5 (km/s)/Mpc, and my method has measured a\\nvalue of 74 ± 5 (km/s)/Mpc. Now are the values consistent? That is a question that\\ncan be quantitatively answered.\\nIn visualization of data and results, showing these errors effectively can make a plot\\nconvey much more complete information.\\nBasic Errorbars\\nA basic errorbar can be created with a single Matplotlib function call (Figure 4-27):\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-whitegrid')\\n       import numpy as np\\nIn[2]: x = np.linspace(0, 10, 50)\\n       dy = 0.8\\n       y = np.sin(x) + dy * np.random.randn(50)\\n       plt.errorbar(x, y, yerr=dy, fmt='.k');\\nFigure 4-27. An errorbar example\\nHere the fmt is a format code controlling the appearance of lines and points, and has\\nthe same syntax as the shorthand used in plt.plot, outlined in “Simple Line Plots”\\non page 224 and “Simple Scatter Plots” on page 233.\\nIn addition to these basic options, the errorbar function has many options to fine-\\ntune the outputs. Using these additional options you can easily customize the aesthet‐\\nics of your errorbar plot. I often find it helpful, especially in crowded plots, to make\\nthe errorbars lighter than the points themselves (Figure 4-28):\\nIn[3]: plt.errorbar(x, y, yerr=dy, fmt='o', color='black',\\n                    ecolor='lightgray', elinewidth=3, capsize=0);\\n238 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 256}, page_content=\"Figure 4-28. Customizing errorbars\\nIn addition to these options, you can also specify horizontal errorbars (xerr), one-\\nsided errorbars, and many other variants. For more information on the options avail‐\\nable, refer to the docstring of plt.errorbar.\\nContinuous Errors\\nIn some situations it is desirable to show errorbars on continuous quantities. Though\\nMatplotlib does not have a built-in convenience routine for this type of application,\\nit’s relatively easy to combine primitives like plt.plot and plt.fill_between for a\\nuseful result.\\nHere we’ll perform a simple Gaussian process regression (GPR), using the Scikit-Learn\\nAPI (see “Introducing Scikit-Learn” on page 343 for details). This is a method of fit‐\\nting a very flexible nonparametric function to data with a continuous measure of the\\nuncertainty. We won’t delve into the details of Gaussian process regression at this\\npoint, but will focus instead on how you might visualize such a continuous error\\nmeasurement:\\nIn[4]: from sklearn.gaussian_process import GaussianProcess\\n       # define the model and draw some data\\n       model = lambda x: x * np.sin(x)\\n       xdata = np.array([1, 3, 5, 6, 8])\\n       ydata = model(xdata)\\n       # Compute the Gaussian process fit\\n       gp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1E-1,\\n                            random_start=100)\\n       gp.fit(xdata[:, np.newaxis], ydata)\\n       xfit = np.linspace(0, 10, 1000)\\n       yfit, MSE = gp.predict(xfit[:, np.newaxis], eval_MSE=True)\\n       dyfit = 2 * np.sqrt(MSE)  # 2*sigma ~ 95% confidence region\\nVisualizing Errors \\n| \\n239\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 257}, page_content=\"We now have xfit, yfit, and dyfit, which sample the continuous fit to our data. We\\ncould pass these to the plt.errorbar function as above, but we don’t really want to\\nplot 1,000 points with 1,000 errorbars. Instead, we can use the plt.fill_between\\nfunction with a light color to visualize this continuous error (Figure 4-29):\\nIn[5]: # Visualize the result\\n       plt.plot(xdata, ydata, 'or')\\n       plt.plot(xfit, yfit, '-', color='gray')\\n       plt.fill_between(xfit, yfit - dyfit, yfit + dyfit,\\n                        color='gray', alpha=0.2)\\n       plt.xlim(0, 10);\\nFigure 4-29. Representing continuous uncertainty with filled regions\\nNote what we’ve done here with the fill_between function: we pass an x value, then\\nthe lower y-bound, then the upper y-bound, and the result is that the area between\\nthese regions is filled.\\nThe resulting figure gives a very intuitive view into what the Gaussian process regres‐\\nsion algorithm is doing: in regions near a measured data point, the model is strongly\\nconstrained and this is reflected in the small model errors. In regions far from a\\nmeasured data point, the model is not strongly constrained, and the model errors\\nincrease.\\nFor more information on the options available in plt.fill_between() (and the\\nclosely related plt.fill() function), see the function docstring or the Matplotlib\\ndocumentation.\\nFinally, if this seems a bit too low level for your taste, refer to “Visualization with Sea‐\\nborn” on page 311, where we discuss the Seaborn package, which has a more stream‐\\nlined API for visualizing this type of continuous errorbar.\\n240 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 258}, page_content=\"Density and Contour Plots\\nSometimes it is useful to display three-dimensional data in two dimensions using\\ncontours or color-coded regions. There are three Matplotlib functions that can be\\nhelpful for this task: plt.contour for contour plots, plt.contourf for filled contour\\nplots, and plt.imshow for showing images. This section looks at several examples of\\nusing these. We’ll start by setting up the notebook for plotting and importing the\\nfunctions we will use:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-white')\\n       import numpy as np\\nVisualizing a Three-Dimensional Function\\nWe’ll start by demonstrating a contour plot using a function z = f x, y , using the fol‐\\nlowing particular choice for f  (we’ve seen this before in “Computation on Arrays:\\nBroadcasting” on page 63, when we used it as a motivating example for array\\nbroadcasting):\\nIn[2]: def f(x, y):\\n           return np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\\nA contour plot can be created with the plt.contour function. It takes three argu‐\\nments: a grid of x values, a grid of y values, and a grid of z values. The x and y values\\nrepresent positions on the plot, and the z values will be represented by the contour\\nlevels. Perhaps the most straightforward way to prepare such data is to use the\\nnp.meshgrid function, which builds two-dimensional grids from one-dimensional\\narrays:\\nIn[3]: x = np.linspace(0, 5, 50)\\n       y = np.linspace(0, 5, 40)\\n       X, Y = np.meshgrid(x, y)\\n       Z = f(X, Y)\\nNow let’s look at this with a standard line-only contour plot (Figure 4-30):\\nIn[4]: plt.contour(X, Y, Z, colors='black');\\nDensity and Contour Plots \\n| \\n241\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 259}, page_content=\"Figure 4-30. Visualizing three-dimensional data with contours\\nNotice that by default when a single color is used, negative values are represented by\\ndashed lines, and positive values by solid lines. Alternatively, you can color-code the\\nlines by specifying a colormap with the cmap argument. Here, we’ll also specify that\\nwe want more lines to be drawn—20 equally spaced intervals within the data range\\n(Figure 4-31):\\nIn[5]: plt.contour(X, Y, Z, 20, cmap='RdGy');\\nFigure 4-31. Visualizing three-dimensional data with colored contours\\nHere we chose the RdGy (short for Red-Gray) colormap, which is a good choice for\\ncentered data. Matplotlib has a wide range of colormaps available, which you can\\neasily browse in IPython by doing a tab completion on the plt.cm module:\\nplt.cm.<TAB>\\nOur plot is looking nicer, but the spaces between the lines may be a bit distracting.\\nWe can change this by switching to a filled contour plot using the plt.contourf()\\nfunction (notice the f at the end), which uses largely the same syntax as plt.con\\ntour().\\n242 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 260}, page_content=\"Additionally, we’ll add a plt.colorbar() command, which automatically creates an\\nadditional axis with labeled color information for the plot (Figure 4-32):\\nIn[6]: plt.contourf(X, Y, Z, 20, cmap='RdGy')\\n       plt.colorbar();\\nFigure 4-32. Visualizing three-dimensional data with filled contours\\nThe colorbar makes it clear that the black regions are “peaks,” while the red regions\\nare “valleys.”\\nOne potential issue with this plot is that it is a bit “splotchy.” That is, the color steps\\nare discrete rather than continuous, which is not always what is desired. You could\\nremedy this by setting the number of contours to a very high number, but this results\\nin a rather inefficient plot: Matplotlib must render a new polygon for each step in the\\nlevel. A better way to handle this is to use the plt.imshow() function, which inter‐\\nprets a two-dimensional grid of data as an image.\\nFigure 4-33 shows the result of the following code:\\nIn[7]: plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower',\\n                  cmap='RdGy')\\n       plt.colorbar()\\n       plt.axis(aspect='image');\\nThere are a few potential gotchas with imshow(), however:\\n• plt.imshow() doesn’t accept an x and y grid, so you must manually specify the\\nextent [xmin, xmax, ymin, ymax] of the image on the plot.\\n• plt.imshow() by default follows the standard image array definition where the\\norigin is in the upper left, not in the lower left as in most contour plots. This\\nmust be changed when showing gridded data.\\nDensity and Contour Plots \\n| \\n243\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 261}, page_content=\"• plt.imshow() will automatically adjust the axis aspect ratio to match the input\\ndata; you can change this by setting, for example, plt.axis(aspect='image') to\\nmake x and y units match.\\nFigure 4-33. Representing three-dimensional data as an image\\nFinally, it can sometimes be useful to combine contour plots and image plots. For\\nexample, to create the effect shown in Figure 4-34, we’ll use a partially transparent\\nbackground image (with transparency set via the alpha parameter) and over-plot\\ncontours with labels on the contours themselves (using the plt.clabel() function):\\nIn[8]: contours = plt.contour(X, Y, Z, 3, colors='black')\\n       plt.clabel(contours, inline=True, fontsize=8)\\n       plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower',\\n                  cmap='RdGy', alpha=0.5)\\n       plt.colorbar();\\nFigure 4-34. Labeled contours on top of an image\\nThe combination of these three functions—plt.contour, plt.contourf, and\\nplt.imshow—gives nearly limitless possibilities for displaying this sort of three-\\ndimensional data within a two-dimensional plot. For more information on the\\n244 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 262}, page_content=\"options available in these functions, refer to their docstrings. If you are interested in\\nthree-dimensional visualizations of this type of data, see “Three-Dimensional Plot‐\\nting in Matplotlib” on page 290.\\nHistograms, Binnings, and Density\\nA simple histogram can be a great first step in understanding a dataset. Earlier, we\\nsaw a preview of Matplotlib’s histogram function (see “Comparisons, Masks, and\\nBoolean Logic” on page 70), which creates a basic histogram in one line, once the\\nnormal boilerplate imports are done (Figure 4-35):\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-white')\\n       data = np.random.randn(1000)\\nIn[2]: plt.hist(data);\\nFigure 4-35. A simple histogram\\nThe hist() function has many options to tune both the calculation and the display;\\nhere’s an example of a more customized histogram (Figure 4-36):\\nIn[3]: plt.hist(data, bins=30, normed=True, alpha=0.5,\\n                histtype='stepfilled', color='steelblue',\\n                edgecolor='none');\\nHistograms, Binnings, and Density \\n| \\n245\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 263}, page_content=\"Figure 4-36. A customized histogram\\nThe plt.hist docstring has more information on other customization options avail‐\\nable. I find this combination of histtype='stepfilled' along with some transpar‐\\nency alpha to be very useful when comparing histograms of several distributions\\n(Figure 4-37):\\nIn[4]: x1 = np.random.normal(0, 0.8, 1000)\\n       x2 = np.random.normal(-2, 1, 1000)\\n       x3 = np.random.normal(3, 2, 1000)\\n       kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\\n       plt.hist(x1, **kwargs)\\n       plt.hist(x2, **kwargs)\\n       plt.hist(x3, **kwargs);\\nFigure 4-37. Over-plotting multiple histograms\\nIf you would like to simply compute the histogram (that is, count the number of\\npoints in a given bin) and not display it, the np.histogram() function is available:\\n246 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 264}, page_content=\"In[5]: counts, bin_edges = np.histogram(data, bins=5)\\n       print(counts)\\n[ 12 190 468 301  29]\\nTwo-Dimensional Histograms and Binnings\\nJust as we create histograms in one dimension by dividing the number line into bins,\\nwe can also create histograms in two dimensions by dividing points among two-\\ndimensional bins. We’ll take a brief look at several ways to do this here. We’ll start by\\ndefining some data—an x and y array drawn from a multivariate Gaussian\\ndistribution:\\nIn[6]: mean = [0, 0]\\n       cov = [[1, 1], [1, 2]]\\n       x, y = np.random.multivariate_normal(mean, cov, 10000).T\\nplt.hist2d: Two-dimensional histogram\\nOne straightforward way to plot a two-dimensional histogram is to use Matplotlib’s\\nplt.hist2d function (Figure 4-38):\\nIn[12]: plt.hist2d(x, y, bins=30, cmap='Blues')\\n        cb = plt.colorbar()\\n        cb.set_label('counts in bin')\\nFigure 4-38. A two-dimensional histogram with plt.hist2d\\nJust as with plt.hist, plt.hist2d has a number of extra options to fine-tune the plot\\nand the binning, which are nicely outlined in the function docstring. Further, just as\\nplt.hist has a counterpart in np.histogram, plt.hist2d has a counterpart in\\nnp.histogram2d, which can be used as follows:\\nIn[8]: counts, xedges, yedges = np.histogram2d(x, y, bins=30)\\nFor the generalization of this histogram binning in dimensions higher than two, see\\nthe np.histogramdd function.\\nHistograms, Binnings, and Density \\n| \\n247\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 265}, page_content=\"plt.hexbin: Hexagonal binnings\\nThe two-dimensional histogram creates a tessellation of squares across the axes.\\nAnother natural shape for such a tessellation is the regular hexagon. For this purpose,\\nMatplotlib provides the plt.hexbin routine, which represents a two-dimensional\\ndataset binned within a grid of hexagons (Figure 4-39):\\nIn[9]: plt.hexbin(x, y, gridsize=30, cmap='Blues')\\n       cb = plt.colorbar(label='count in bin')\\nFigure 4-39. A two-dimensional histogram with plt.hexbin\\nplt.hexbin has a number of interesting options, including the ability to specify\\nweights for each point, and to change the output in each bin to any NumPy aggregate\\n(mean of weights, standard deviation of weights, etc.).\\nKernel density estimation\\nAnother common method of evaluating densities in multiple dimensions is kernel\\ndensity estimation (KDE). This will be discussed more fully in “In-Depth: Kernel\\nDensity Estimation” on page 491, but for now we’ll simply mention that KDE can be\\nthought of as a way to “smear out” the points in space and add up the result to obtain\\na smooth function. One extremely quick and simple KDE implementation exists in\\nthe scipy.stats package. Here is a quick example of using the KDE on this data\\n(Figure 4-40):\\nIn[10]: from scipy.stats import gaussian_kde\\n        # fit an array of size [Ndim, Nsamples]\\n        data = np.vstack([x, y])\\n        kde = gaussian_kde(data)\\n        # evaluate on a regular grid\\n        xgrid = np.linspace(-3.5, 3.5, 40)\\n        ygrid = np.linspace(-6, 6, 40)\\n        Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)\\n        Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()]))\\n248 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 266}, page_content='# Plot the result as an image\\n        plt.imshow(Z.reshape(Xgrid.shape),\\n                   origin=\\'lower\\', aspect=\\'auto\\',\\n                   extent=[-3.5, 3.5, -6, 6],\\n                   cmap=\\'Blues\\')\\n        cb = plt.colorbar()\\n        cb.set_label(\"density\")\\nFigure 4-40. A kernel density representation of a distribution\\nKDE has a smoothing length that effectively slides the knob between detail and\\nsmoothness (one example of the ubiquitous bias–variance trade-off). The literature\\non choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of\\nthumb to attempt to find a nearly optimal smoothing length for the input data.\\nOther KDE implementations are available within the SciPy ecosystem, each with its\\nown various strengths and weaknesses; see, for example, sklearn.neighbors.Kernel\\nDensity and statsmodels.nonparametric.kernel_density.KDEMultivariate. For\\nvisualizations based on KDE, using Matplotlib tends to be overly verbose. The Sea‐\\nborn library, discussed in “Visualization with Seaborn” on page 311, provides a much\\nmore terse API for creating KDE-based visualizations.\\nCustomizing Plot Legends\\nPlot legends give meaning to a visualization, assigning labels to the various plot ele‐\\nments. We previously saw how to create a simple legend; here we’ll take a look at cus‐\\ntomizing the placement and aesthetics of the legend in Matplotlib.\\nThe simplest legend can be created with the plt.legend() command, which auto‐\\nmatically creates a legend for any labeled plot elements (Figure 4-41):\\nIn[1]: import matplotlib.pyplot as plt\\n       plt.style.use(\\'classic\\')\\nCustomizing Plot Legends \\n| \\n249'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 267}, page_content=\"In[2]: %matplotlib inline\\n       import numpy as np\\nIn[3]: x = np.linspace(0, 10, 1000)\\n       fig, ax = plt.subplots()\\n       ax.plot(x, np.sin(x), '-b', label='Sine')\\n       ax.plot(x, np.cos(x), '--r', label='Cosine')\\n       ax.axis('equal')\\n       leg = ax.legend();\\nFigure 4-41. A default plot legend\\nBut there are many ways we might want to customize such a legend. For example, we\\ncan specify the location and turn off the frame (Figure 4-42):\\nIn[4]: ax.legend(loc='upper left', frameon=False)\\n       fig\\nFigure 4-42. A customized plot legend\\nWe can use the ncol command to specify the number of columns in the legend\\n(Figure 4-43):\\nIn[5]: ax.legend(frameon=False, loc='lower center', ncol=2)\\n       fig\\n250 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 268}, page_content='Figure 4-43. A two-column plot legend\\nWe can use a rounded box (fancybox) or add a shadow, change the transparency\\n(alpha value) of the frame, or change the padding around the text (Figure 4-44):\\nIn[6]: ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\\n       fig\\nFigure 4-44. A fancybox plot legend\\nFor more information on available legend options, see the plt.legend docstring.\\nChoosing Elements for the Legend\\nAs we’ve already seen, the legend includes all labeled elements by default. If this is not\\nwhat is desired, we can fine-tune which elements and labels appear in the legend by\\nusing the objects returned by plot commands. The plt.plot() command is able to\\ncreate multiple lines at once, and returns a list of created line instances. Passing any of\\nthese to plt.legend() will tell it which to identify, along with the labels we’d like to\\nspecify (Figure 4-45):\\nIn[7]: y = np.sin(x[:, np.newaxis] + np.pi * np.arange(0, 2, 0.5))\\n       lines = plt.plot(x, y)\\nCustomizing Plot Legends \\n| \\n251'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 269}, page_content=\"# lines is a list of plt.Line2D instances\\n       plt.legend(lines[:2], ['first', 'second']);\\nFigure 4-45. Customization of legend elements\\nI generally find in practice that it is clearer to use the first method, applying labels to\\nthe plot elements you’d like to show on the legend (Figure 4-46):\\nIn[8]: plt.plot(x, y[:, 0], label='first')\\n       plt.plot(x, y[:, 1], label='second')\\n       plt.plot(x, y[:, 2:])\\n       plt.legend(framealpha=1, frameon=True);\\nFigure 4-46. Alternative method of customizing legend elements\\nNotice that by default, the legend ignores all elements without a label attribute set.\\nLegend for Size of Points\\nSometimes the legend defaults are not sufficient for the given visualization. For exam‐\\nple, perhaps you’re using the size of points to mark certain features of the data, and\\nwant to create a legend reflecting this. Here is an example where we’ll use the size of\\npoints to indicate populations of California cities. We’d like a legend that specifies the\\n252 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 270}, page_content=\"scale of the sizes of the points, and we’ll accomplish this by plotting some labeled data\\nwith no entries (Figure 4-47):\\nIn[9]: import pandas as pd\\n       cities = pd.read_csv('data/california_cities.csv')\\n       # Extract the data we're interested in\\n       lat, lon = cities['latd'], cities['longd']\\n       population, area = cities['population_total'], cities['area_total_km2']\\n       # Scatter the points, using size and color but no label\\n       plt.scatter(lon, lat, label=None,\\n                   c=np.log10(population), cmap='viridis',\\n                   s=area, linewidth=0, alpha=0.5)\\n       plt.axis(aspect='equal')\\n       plt.xlabel('longitude')\\n       plt.ylabel('latitude')\\n       plt.colorbar(label='log$_{10}$(population)')\\n       plt.clim(3, 7)\\n       # Here we create a legend:\\n       # we'll plot empty lists with the desired size and label\\n       for area in [100, 300, 500]:\\n           plt.scatter([], [], c='k', alpha=0.3, s=area,\\n                       label=str(area) + ' km$^2$')\\n       plt.legend(scatterpoints=1, frameon=False,\\n                  labelspacing=1, title='City Area')\\n       plt.title('California Cities: Area and Population');\\nFigure 4-47. Location, geographic size, and population of California cities\\nThe legend will always reference some object that is on the plot, so if we’d like to dis‐\\nplay a particular shape we need to plot it. In this case, the objects we want (gray cir‐\\ncles) are not on the plot, so we fake them by plotting empty lists. Notice too that the\\nlegend only lists plot elements that have a label specified.\\nCustomizing Plot Legends \\n| \\n253\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 271}, page_content=\"By plotting empty lists, we create labeled plot objects that are picked up by the legend,\\nand now our legend tells us some useful information. This strategy can be useful for\\ncreating more sophisticated visualizations.\\nFinally, note that for geographic data like this, it would be clearer if we could show\\nstate boundaries or other map-specific elements. For this, an excellent choice of tool\\nis Matplotlib’s Basemap add-on toolkit, which we’ll explore in “Geographic Data with\\nBasemap” on page 298.\\nMultiple Legends\\nSometimes when designing a plot you’d like to add multiple legends to the same axes.\\nUnfortunately, Matplotlib does not make this easy: via the standard legend interface,\\nit is only possible to create a single legend for the entire plot. If you try to create a\\nsecond legend using plt.legend() or ax.legend(), it will simply override the first\\none. We can work around this by creating a new legend artist from scratch, and then\\nusing the lower-level ax.add_artist() method to manually add the second artist to\\nthe plot (Figure 4-48):\\nIn[10]: fig, ax = plt.subplots()\\n       lines = []\\n       styles = ['-', '--', '-.', ':']\\n       x = np.linspace(0, 10, 1000)\\n       for i in range(4):\\n           lines += ax.plot(x, np.sin(x - i * np.pi / 2),\\n                            styles[i], color='black')\\n       ax.axis('equal')\\n       # specify the lines and labels of the first legend\\n       ax.legend(lines[:2], ['line A', 'line B'],\\n                 loc='upper right', frameon=False)\\n       # Create the second legend and add the artist manually.\\n       from matplotlib.legend import Legend\\n       leg = Legend(ax, lines[2:], ['line C', 'line D'],\\n                    loc='lower right', frameon=False)\\n       ax.add_artist(leg);\\n254 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 272}, page_content=\"Figure 4-48. A split plot legend\\nThis is a peek into the low-level artist objects that compose any Matplotlib plot. If you\\nexamine the source code of ax.legend() (recall that you can do this within the IPy‐\\nthon notebook using ax.legend??) you’ll see that the function simply consists of\\nsome logic to create a suitable Legend artist, which is then saved in the legend_\\nattribute and added to the figure when the plot is drawn.\\nCustomizing Colorbars\\nPlot legends identify discrete labels of discrete points. For continuous labels based on\\nthe color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat‐\\nplotlib, a colorbar is a separate axes that can provide a key for the meaning of colors\\nin a plot. Because the book is printed in black and white, this section has an accompa‐\\nnying online appendix where you can view the figures in full color (https://\\ngithub.com/jakevdp/PythonDataScienceHandbook). We’ll start by setting up the note‐\\nbook for plotting and importing the functions we will use:\\nIn[1]: import matplotlib.pyplot as plt\\n       plt.style.use('classic')\\nIn[2]: %matplotlib inline\\n       import numpy as np\\nAs we have seen several times throughout this section, the simplest colorbar can be\\ncreated with the plt.colorbar function (Figure 4-49):\\nIn[3]: x = np.linspace(0, 10, 1000)\\n       I = np.sin(x) * np.cos(x[:, np.newaxis])\\n       plt.imshow(I)\\n       plt.colorbar();\\nCustomizing Colorbars \\n| \\n255\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 273}, page_content=\"Figure 4-49. A simple colorbar legend\\nWe’ll now discuss a few ideas for customizing these colorbars and using them effec‐\\ntively in various situations.\\nCustomizing Colorbars\\nWe can specify the colormap using the cmap argument to the plotting function that is\\ncreating the visualization (Figure 4-50):\\nIn[4]: plt.imshow(I, cmap='gray');\\nFigure 4-50. A grayscale colormap\\nAll the available colormaps are in the plt.cm namespace; using IPython’s tab-\\ncompletion feature will give you a full list of built-in possibilities:\\nplt.cm.<TAB>\\nBut being able to choose a colormap is just the first step: more important is how to\\ndecide among the possibilities! The choice turns out to be much more subtle than you\\nmight initially expect.\\n256 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 274}, page_content='Choosing the colormap\\nA full treatment of color choice within visualization is beyond the scope of this book,\\nbut for entertaining reading on this subject and others, see the article “Ten Simple\\nRules for Better Figures”. Matplotlib’s online documentation also has an interesting\\ndiscussion of colormap choice.\\nBroadly, you should be aware of three different categories of colormaps:\\nSequential colormaps\\nThese consist of one continuous sequence of colors (e.g., binary or viridis).\\nDivergent colormaps\\nThese usually contain two distinct colors, which show positive and negative devi‐\\nations from a mean (e.g., RdBu or PuOr).\\nQualitative colormaps\\nThese mix colors with no particular sequence (e.g., rainbow or jet).\\nThe jet colormap, which was the default in Matplotlib prior to version 2.0, is an\\nexample of a qualitative colormap. Its status as the default was quite unfortunate,\\nbecause qualitative maps are often a poor choice for representing quantitative data.\\nAmong the problems is the fact that qualitative maps usually do not display any uni‐\\nform progression in brightness as the scale increases.\\nWe can see this by converting the jet colorbar into black and white (Figure 4-51):\\nIn[5]:\\nfrom matplotlib.colors import LinearSegmentedColormap\\ndef grayscale_cmap(cmap):\\n    \"\"\"Return a grayscale version of the given colormap\"\"\"\\n    cmap = plt.cm.get_cmap(cmap)\\n    colors = cmap(np.arange(cmap.N))\\n    # convert RGBA to perceived grayscale luminance\\n    # cf. http://alienryderflex.com/hsp.html\\n    RGB_weight = [0.299, 0.587, 0.114]\\n    luminance = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))\\n    colors[:, :3] = luminance[:, np.newaxis]\\n    return LinearSegmentedColormap.from_list(cmap.name + \"_gray\", colors, cmap.N)\\ndef view_colormap(cmap):\\n    \"\"\"Plot a colormap with its grayscale equivalent\"\"\"\\n    cmap = plt.cm.get_cmap(cmap)\\n    colors = cmap(np.arange(cmap.N))\\n    cmap = grayscale_cmap(cmap)\\n    grayscale = cmap(np.arange(cmap.N))\\nCustomizing Colorbars \\n| \\n257'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 275}, page_content=\"fig, ax = plt.subplots(2, figsize=(6, 2),\\n                           subplot_kw=dict(xticks=[], yticks=[]))\\n    ax[0].imshow([colors], extent=[0, 10, 0, 1])\\n    ax[1].imshow([grayscale], extent=[0, 10, 0, 1])\\nIn[6]: view_colormap('jet')\\nFigure 4-51. The jet colormap and its uneven luminance scale\\nNotice the bright stripes in the grayscale image. Even in full color, this uneven bright‐\\nness means that the eye will be drawn to certain portions of the color range, which\\nwill potentially emphasize unimportant parts of the dataset. It’s better to use a color‐\\nmap such as viridis (the default as of Matplotlib 2.0), which is specifically construc‐\\nted to have an even brightness variation across the range. Thus, it not only plays well\\nwith our color perception, but also will translate well to grayscale printing\\n(Figure 4-52):\\nIn[7]: view_colormap('viridis')\\nFigure 4-52. The viridis colormap and its even luminance scale\\nIf you favor rainbow schemes, another good option for continuous data is the\\ncubehelix colormap (Figure 4-53):\\nIn[8]: view_colormap('cubehelix')\\nFigure 4-53. The cubehelix colormap and its luminance\\nFor other situations, such as showing positive and negative deviations from some\\nmean, dual-color colorbars such as RdBu (short for Red-Blue) can be useful. However,\\n258 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 276}, page_content=\"as you can see in Figure 4-54, it’s important to note that the positive-negative infor‐\\nmation will be lost upon translation to grayscale!\\nIn[9]: view_colormap('RdBu')\\nFigure 4-54. The RdBu (Red-Blue) colormap and its luminance\\nWe’ll see examples of using some of these color maps as we continue.\\nThere are a large number of colormaps available in Matplotlib; to see a list of them,\\nyou can use IPython to explore the plt.cm submodule. For a more principled\\napproach to colors in Python, you can refer to the tools and documentation within\\nthe Seaborn library (see “Visualization with Seaborn” on page 311).\\nColor limits and extensions\\nMatplotlib allows for a large range of colorbar customization. The colorbar itself is\\nsimply an instance of plt.Axes, so all of the axes and tick formatting tricks we’ve\\nlearned are applicable. The colorbar has some interesting flexibility; for example, we\\ncan narrow the color limits and indicate the out-of-bounds values with a triangular\\narrow at the top and bottom by setting the extend property. This might come in\\nhandy, for example, if you’re displaying an image that is subject to noise\\n(Figure 4-55):\\nIn[10]: # make noise in 1% of the image pixels\\n        speckles = (np.random.random(I.shape) < 0.01)\\n        I[speckles] = np.random.normal(0, 3, np.count_nonzero(speckles))\\n        plt.figure(figsize=(10, 3.5))\\n        plt.subplot(1, 2, 1)\\n        plt.imshow(I, cmap='RdBu')\\n        plt.colorbar()\\n        plt.subplot(1, 2, 2)\\n        plt.imshow(I, cmap='RdBu')\\n        plt.colorbar(extend='both')\\n        plt.clim(-1, 1);\\nCustomizing Colorbars \\n| \\n259\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 277}, page_content=\"Figure 4-55. Specifying colormap extensions\\nNotice that in the left panel, the default color limits respond to the noisy pixels, and\\nthe range of the noise completely washes out the pattern we are interested in. In the\\nright panel, we manually set the color limits, and add extensions to indicate values\\nthat are above or below those limits. The result is a much more useful visualization of\\nour data.\\nDiscrete colorbars\\nColormaps are by default continuous, but sometimes you’d like to represent discrete\\nvalues. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass\\nthe name of a suitable colormap along with the number of desired bins (Figure 4-56):\\nIn[11]: plt.imshow(I, cmap=plt.cm.get_cmap('Blues', 6))\\n        plt.colorbar()\\n        plt.clim(-1, 1);\\nFigure 4-56. A discretized colormap\\nThe discrete version of a colormap can be used just like any other colormap.\\n260 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 278}, page_content=\"Example: Handwritten Digits\\nFor an example of where this might be useful, let’s look at an interesting visualization\\nof some handwritten digits data. This data is included in Scikit-Learn, and consists of\\nnearly 2,000 8×8 thumbnails showing various handwritten digits.\\nFor now, let’s start by downloading the digits data and visualizing several of the exam‐\\nple images with plt.imshow() (Figure 4-57):\\nIn[12]: # load images of the digits 0 through 5 and visualize several of them\\n        from sklearn.datasets import load_digits\\n        digits = load_digits(n_class=6)\\n        fig, ax = plt.subplots(8, 8, figsize=(6, 6))\\n        for i, axi in enumerate(ax.flat):\\n            axi.imshow(digits.images[i], cmap='binary')\\n            axi.set(xticks=[], yticks=[])\\nFigure 4-57. Sample of handwritten digit data\\nBecause each digit is defined by the hue of its 64 pixels, we can consider each digit to\\nbe a point lying in 64-dimensional space: each dimension represents the brightness of\\none pixel. But visualizing relationships in such high-dimensional spaces can be\\nextremely difficult. One way to approach this is to use a dimensionality reduction\\ntechnique such as manifold learning to reduce the dimensionality of the data while\\nmaintaining the relationships of interest. Dimensionality reduction is an example of\\nunsupervised machine learning, and we will discuss it in more detail in “What Is\\nMachine Learning?” on page 332.\\nDeferring the discussion of these details, let’s take a look at a two-dimensional mani‐\\nfold learning projection of this digits data (see “In-Depth: Manifold Learning” on\\npage 445 for details):\\nCustomizing Colorbars \\n| \\n261\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 279}, page_content=\"In[13]: # project the digits into 2 dimensions using IsoMap\\n        from sklearn.manifold import Isomap\\n        iso = Isomap(n_components=2)\\n        projection = iso.fit_transform(digits.data)\\nWe’ll use our discrete colormap to view the results, setting the ticks and clim to\\nimprove the aesthetics of the resulting colorbar (Figure 4-58):\\nIn[14]: # plot the results\\n        plt.scatter(projection[:, 0], projection[:, 1], lw=0.1,\\n                    c=digits.target, cmap=plt.cm.get_cmap('cubehelix', 6))\\n        plt.colorbar(ticks=range(6), label='digit value')\\n        plt.clim(-0.5, 5.5)\\nFigure 4-58. Manifold embedding of handwritten digit pixels\\nThe projection also gives us some interesting insights on the relationships within the\\ndataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating\\nthat some handwritten fives and threes are difficult to distinguish, and therefore\\nmore likely to be confused by an automated classification algorithm. Other values,\\nlike 0 and 1, are more distantly separated, and therefore much less likely to be con‐\\nfused. This observation agrees with our intuition, because 5 and 3 look much more\\nsimilar than do 0 and 1.\\nWe’ll return to manifold learning and digit classification in Chapter 5.\\nMultiple Subplots\\nSometimes it is helpful to compare different views of data side by side. To this end,\\nMatplotlib has the concept of subplots: groups of smaller axes that can exist together\\nwithin a single figure. These subplots might be insets, grids of plots, or other more\\ncomplicated layouts. In this section, we’ll explore four routines for creating subplots\\nin Matplotlib. We’ll start by setting up the notebook for plotting and importing the\\nfunctions we will use:\\n262 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 280}, page_content=\"In[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-white')\\n       import numpy as np\\nplt.axes: Subplots by Hand\\nThe most basic method of creating an axes is to use the plt.axes function. As we’ve\\nseen previously, by default this creates a standard axes object that fills the entire fig‐\\nure. plt.axes also takes an optional argument that is a list of four numbers in the\\nfigure coordinate system. These numbers represent [bottom, left, width,\\nheight] in the figure coordinate system, which ranges from 0 at the bottom left of the\\nfigure to 1 at the top right of the figure.\\nFor example, we might create an inset axes at the top-right corner of another axes by\\nsetting the x and y position to 0.65 (that is, starting at 65% of the width and 65% of\\nthe height of the figure) and the x and y extents to 0.2 (that is, the size of the axes is\\n20% of the width and 20% of the height of the figure). Figure 4-59 shows the result of\\nthis code:\\nIn[2]: ax1 = plt.axes()  # standard axes\\n       ax2 = plt.axes([0.65, 0.65, 0.2, 0.2])\\nFigure 4-59. Example of an inset axes\\nThe equivalent of this command within the object-oriented interface is\\nfig.add_axes(). Let’s use this to create two vertically stacked axes (Figure 4-60):\\nIn[3]: fig = plt.figure()\\n       ax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4],\\n                          xticklabels=[], ylim=(-1.2, 1.2))\\n       ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.4],\\n                          ylim=(-1.2, 1.2))\\n       x = np.linspace(0, 10)\\n       ax1.plot(np.sin(x))\\n       ax2.plot(np.cos(x));\\nMultiple Subplots \\n| \\n263\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 281}, page_content=\"Figure 4-60. Vertically stacked axes example\\nWe now have two axes (the top with no tick labels) that are just touching: the bottom\\nof the upper panel (at position 0.5) matches the top of the lower panel (at position 0.1\\n+ 0.4).\\nplt.subplot: Simple Grids of Subplots\\nAligned columns or rows of subplots are a common enough need that Matplotlib has\\nseveral convenience routines that make them easy to create. The lowest level of these\\nis plt.subplot(), which creates a single subplot within a grid. As you can see, this\\ncommand takes three integer arguments—the number of rows, the number of col‐\\numns, and the index of the plot to be created in this scheme, which runs from the\\nupper left to the bottom right (Figure 4-61):\\nIn[4]: for i in range(1, 7):\\n           plt.subplot(2, 3, i)\\n           plt.text(0.5, 0.5, str((2, 3, i)),\\n                    fontsize=18, ha='center')\\nFigure 4-61. A plt.subplot() example\\n264 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 282}, page_content=\"The command plt.subplots_adjust can be used to adjust the spacing between\\nthese plots. The following code (the result of which is shown in Figure 4-62) uses the\\nequivalent object-oriented command, fig.add_subplot():\\nIn[5]: fig = plt.figure()\\n       fig.subplots_adjust(hspace=0.4, wspace=0.4)\\n       for i in range(1, 7):\\n           ax = fig.add_subplot(2, 3, i)\\n           ax.text(0.5, 0.5, str((2, 3, i)),\\n                  fontsize=18, ha='center')\\nFigure 4-62. plt.subplot() with adjusted margins\\nWe’ve used the hspace and wspace arguments of plt.subplots_adjust, which spec‐\\nify the spacing along the height and width of the figure, in units of the subplot size (in\\nthis case, the space is 40% of the subplot width and height).\\nplt.subplots: The Whole Grid in One Go\\nThe approach just described can become quite tedious when you’re creating a large\\ngrid of subplots, especially if you’d like to hide the x- and y-axis labels on the inner\\nplots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end\\nof subplots). Rather than creating a single subplot, this function creates a full grid of\\nsubplots in a single line, returning them in a NumPy array. The arguments are the\\nnumber of rows and number of columns, along with optional keywords sharex and\\nsharey, which allow you to specify the relationships between different axes.\\nHere we’ll create a 2×3 grid of subplots, where all axes in the same row share their\\ny-axis scale, and all axes in the same column share their x-axis scale (Figure 4-63):\\nIn[6]: fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')\\nMultiple Subplots \\n| \\n265\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 283}, page_content=\"Figure 4-63. Shared x and y axis in plt.subplots()\\nNote that by specifying sharex and sharey, we’ve automatically removed inner labels\\non the grid to make the plot cleaner. The resulting grid of axes instances is returned\\nwithin a NumPy array, allowing for convenient specification of the desired axes using\\nstandard array indexing notation (Figure 4-64):\\nIn[7]: # axes are in a two-dimensional array, indexed by [row, col]\\n       for i in range(2):\\n           for j in range(3):\\n               ax[i, j].text(0.5, 0.5, str((i, j)),\\n                             fontsize=18, ha='center')\\n       fig\\nFigure 4-64. Identifying plots in a subplot grid\\nIn comparison to plt.subplot(), plt.subplots() is more consistent with Python’s\\nconventional 0-based indexing.\\nplt.GridSpec: More Complicated Arrangements\\nTo go beyond a regular grid to subplots that span multiple rows and columns,\\nplt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by\\n266 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 284}, page_content=\"itself; it is simply a convenient interface that is recognized by the plt.subplot()\\ncommand. For example, a gridspec for a grid of two rows and three columns with\\nsome specified width and height space looks like this:\\nIn[8]: grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\\nFrom this we can specify subplot locations and extents using the familiar Python slic‐\\ning syntax (Figure 4-65):\\nIn[9]: plt.subplot(grid[0, 0])\\n       plt.subplot(grid[0, 1:])\\n       plt.subplot(grid[1, :2])\\n       plt.subplot(grid[1, 2]);\\nFigure 4-65. Irregular subplots with plt.GridSpec\\nThis type of flexible grid alignment has a wide range of uses. I most often use it when\\ncreating multi-axes histogram plots like the one shown here (Figure 4-66):\\nIn[10]: # Create some normally distributed data\\n        mean = [0, 0]\\n        cov = [[1, 1], [1, 2]]\\n        x, y = np.random.multivariate_normal(mean, cov, 3000).T\\n        # Set up the axes with gridspec\\n        fig = plt.figure(figsize=(6, 6))\\n        grid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2)\\n        main_ax = fig.add_subplot(grid[:-1, 1:])\\n        y_hist = fig.add_subplot(grid[:-1, 0], xticklabels=[], sharey=main_ax)\\n        x_hist = fig.add_subplot(grid[-1, 1:], yticklabels=[], sharex=main_ax)\\n        # scatter points on the main axes\\n        main_ax.plot(x, y, 'ok', markersize=3, alpha=0.2)\\n        # histogram on the attached axes\\n        x_hist.hist(x, 40, histtype='stepfilled',\\n                    orientation='vertical', color='gray')\\n        x_hist.invert_yaxis()\\nMultiple Subplots \\n| \\n267\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 285}, page_content=\"y_hist.hist(y, 40, histtype='stepfilled',\\n                    orientation='horizontal', color='gray')\\n        y_hist.invert_xaxis()\\nFigure 4-66. Visualizing multidimensional distributions with plt.GridSpec\\nThis type of distribution plotted alongside its margins is common enough that it has\\nits own plotting API in the Seaborn package; see “Visualization with Seaborn” on\\npage 311 for more details.\\nText and Annotation\\nCreating a good visualization involves guiding the reader so that the figure tells a\\nstory. In some cases, this story can be told in an entirely visual manner, without the\\nneed for added text, but in others, small textual cues and labels are necessary. Perhaps\\nthe most basic types of annotations you will use are axes labels and titles, but the\\noptions go beyond this. Let’s take a look at some data and how we might visualize and\\nannotate it to help convey interesting information. We’ll start by setting up the note‐\\nbook for plotting and importing the functions we will use:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import matplotlib as mpl\\n       plt.style.use('seaborn-whitegrid')\\n       import numpy as np\\n       import pandas as pd\\n268 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 286}, page_content=\"Example: Effect of Holidays on US Births\\nLet’s return to some data we worked with earlier in “Example: Birthrate Data” on page\\n174, where we generated a plot of average births over the course of the calendar year;\\nas already mentioned, this data can be downloaded at https://raw.githubusercon\\ntent.com/jakevdp/data-CDCbirths/master/births.csv.\\nWe’ll start with the same cleaning procedure we used there, and plot the results\\n(Figure 4-67):\\nIn[2]:\\nbirths = pd.read_csv('births.csv')\\nquartiles = np.percentile(births['births'], [25, 50, 75])\\nmu, sig = quartiles[1], 0.74 * (quartiles[2] - quartiles[0])\\nbirths = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')\\nbirths['day'] = births['day'].astype(int)\\nbirths.index = pd.to_datetime(10000 * births.year +\\n                              100 * births.month +\\n                              births.day, format='%Y%m%d')\\nbirths_by_date = births.pivot_table('births',\\n                                    [births.index.month, births.index.day])\\nbirths_by_date.index = [pd.datetime(2012, month, day)\\n                        for (month, day) in births_by_date.index]\\nIn[3]: fig, ax = plt.subplots(figsize=(12, 4))\\n       births_by_date.plot(ax=ax);\\nFigure 4-67. Average daily births by date\\nWhen we’re communicating data like this, it is often useful to annotate certain fea‐\\ntures of the plot to draw the reader’s attention. This can be done manually with the\\nplt.text/ax.text command, which will place text at a particular x/y value\\n(Figure 4-68):\\nText and Annotation \\n| \\n269\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 287}, page_content='In[4]: fig, ax = plt.subplots(figsize=(12, 4))\\n       births_by_date.plot(ax=ax)\\n       # Add labels to the plot\\n       style = dict(size=10, color=\\'gray\\')\\n       ax.text(\\'2012-1-1\\', 3950, \"New Year\\'s Day\", **style)\\n       ax.text(\\'2012-7-4\\', 4250, \"Independence Day\", ha=\\'center\\', **style)\\n       ax.text(\\'2012-9-4\\', 4850, \"Labor Day\", ha=\\'center\\', **style)\\n       ax.text(\\'2012-10-31\\', 4600, \"Halloween\", ha=\\'right\\', **style)\\n       ax.text(\\'2012-11-25\\', 4450, \"Thanksgiving\", ha=\\'center\\', **style)\\n       ax.text(\\'2012-12-25\\', 3850, \"Christmas \", ha=\\'right\\', **style)\\n       # Label the axes\\n       ax.set(title=\\'USA births by day of year (1969-1988)\\',\\n              ylabel=\\'average daily births\\')\\n       # Format the x axis with centered month labels\\n       ax.xaxis.set_major_locator(mpl.dates.MonthLocator())\\n       ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))\\n       ax.xaxis.set_major_formatter(plt.NullFormatter())\\n       ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(\\'%h\\'));\\nFigure 4-68. Annotated average daily births by date\\nThe ax.text method takes an x position, a y position, a string, and then optional key‐\\nwords specifying the color, size, style, alignment, and other properties of the text.\\nHere we used ha=\\'right\\' and ha=\\'center\\', where ha is short for horizonal align‐\\nment. See the docstring of plt.text() and of mpl.text.Text() for more information\\non available options.\\nTransforms and Text Position\\nIn the previous example, we anchored our text annotations to data locations. Some‐\\ntimes it’s preferable to anchor the text to a position on the axes or figure, independent\\nof the data. In Matplotlib, we do this by modifying the transform.\\n270 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 288}, page_content='Any graphics display framework needs some scheme for translating between coordi‐\\nnate systems. For example, a data point at x, y = 1, 1  needs to somehow be repre‐\\nsented at a certain location on the figure, which in turn needs to be represented in\\npixels on the screen. Mathematically, such coordinate transformations are relatively\\nstraightforward, and Matplotlib has a well-developed set of tools that it uses inter‐\\nnally to perform them (the tools can be explored in the matplotlib.transforms sub‐\\nmodule).\\nThe average user rarely needs to worry about the details of these transforms, but it is\\nhelpful knowledge to have when considering the placement of text on a figure. There\\nare three predefined transforms that can be useful in this situation:\\nax.transData\\nTransform associated with data coordinates\\nax.transAxes\\nTransform associated with the axes (in units of axes dimensions)\\nfig.transFigure\\nTransform associated with the figure (in units of figure dimensions)\\nHere let’s look at an example of drawing text at various locations using these trans‐\\nforms (Figure 4-69):\\nIn[5]: fig, ax = plt.subplots(facecolor=\\'lightgray\\')\\n       ax.axis([0, 10, 0, 10])\\n       # transform=ax.transData is the default, but we\\'ll specify it anyway\\n       ax.text(1, 5, \". Data: (1, 5)\", transform=ax.transData)\\n       ax.text(0.5, 0.1, \". Axes: (0.5, 0.1)\", transform=ax.transAxes)\\n       ax.text(0.2, 0.2, \". Figure: (0.2, 0.2)\", transform=fig.transFigure);\\nFigure 4-69. Comparing Matplotlib’s coordinate systems\\nText and Annotation \\n| \\n271'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 289}, page_content='Note that by default, the text is aligned above and to the left of the specified coordi‐\\nnates; here the “.” at the beginning of each string will approximately mark the given\\ncoordinate location.\\nThe transData coordinates give the usual data coordinates associated with the x- and\\ny-axis labels. The transAxes coordinates give the location from the bottom-left cor‐\\nner of the axes (here the white box) as a fraction of the axes size. The transFigure\\ncoordinates are similar, but specify the position from the bottom left of the figure\\n(here the gray box) as a fraction of the figure size.\\nNotice now that if we change the axes limits, it is only the transData coordinates that\\nwill be affected, while the others remain stationary (Figure 4-70):\\nIn[6]: ax.set_xlim(0, 2)\\n       ax.set_ylim(-6, 6)\\n       fig\\nFigure 4-70. Comparing Matplotlib’s coordinate systems\\nYou can see this behavior more clearly by changing the axes limits interactively; if you\\nare executing this code in a notebook, you can make that happen by changing %mat\\nplotlib inline to %matplotlib notebook and using each plot’s menu to interact\\nwith the plot.\\nArrows and Annotation\\nAlong with tick marks and text, another useful annotation mark is the simple arrow.\\nDrawing arrows in Matplotlib is often much harder than you might hope. While\\nthere is a plt.arrow() function available, I wouldn’t suggest using it; the arrows it\\ncreates are SVG objects that will be subject to the varying aspect ratio of your plots,\\nand the result is rarely what the user intended. Instead, I’d suggest using the plt.anno\\ntate() function. This function creates some text and an arrow, and the arrows can be\\nvery flexibly specified.\\n272 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 290}, page_content='Here we’ll use annotate with several of its options (Figure 4-71):\\nIn[7]: %matplotlib inline\\n       fig, ax = plt.subplots()\\n       x = np.linspace(0, 20, 1000)\\n       ax.plot(x, np.cos(x))\\n       ax.axis(\\'equal\\')\\n       ax.annotate(\\'local maximum\\', xy=(6.28, 1), xytext=(10, 4),\\n                   arrowprops=dict(facecolor=\\'black\\', shrink=0.05))\\n       ax.annotate(\\'local minimum\\', xy=(5 * np.pi, -1), xytext=(2, -6),\\n                   arrowprops=dict(arrowstyle=\"->\",\\n                                   connectionstyle=\"angle3,angleA=0,angleB=-90\"));\\nFigure 4-71. Annotation examples\\nThe arrow style is controlled through the arrowprops dictionary, which has numer‐\\nous options available. These options are fairly well documented in Matplotlib’s online\\ndocumentation, so rather than repeating them here I’ll quickly show some of the pos‐\\nsibilities. Let’s demonstrate several of the possible options using the birthrate plot\\nfrom before (Figure 4-72):\\nIn[8]:\\nfig, ax = plt.subplots(figsize=(12, 4))\\nbirths_by_date.plot(ax=ax)\\n# Add labels to the plot\\nax.annotate(\"New Year\\'s Day\", xy=(\\'2012-1-1\\', 4100),  xycoords=\\'data\\',\\n            xytext=(50, -30), textcoords=\\'offset points\\',\\n            arrowprops=dict(arrowstyle=\"->\",\\n                            connectionstyle=\"arc3,rad=-0.2\"))\\nax.annotate(\"Independence Day\", xy=(\\'2012-7-4\\', 4250),  xycoords=\\'data\\',\\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\\nText and Annotation \\n| \\n273'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 291}, page_content='xytext=(10, -40), textcoords=\\'offset points\\', ha=\\'center\\',\\n            arrowprops=dict(arrowstyle=\"->\"))\\nax.annotate(\\'Labor Day\\', xy=(\\'2012-9-4\\', 4850), xycoords=\\'data\\', ha=\\'center\\',\\n            xytext=(0, -20), textcoords=\\'offset points\\')\\nax.annotate(\\'\\', xy=(\\'2012-9-1\\', 4850), xytext=(\\'2012-9-7\\', 4850),\\n            xycoords=\\'data\\', textcoords=\\'data\\',\\n            arrowprops={\\'arrowstyle\\': \\'|-|,widthA=0.2,widthB=0.2\\', })\\nax.annotate(\\'Halloween\\', xy=(\\'2012-10-31\\', 4600),  xycoords=\\'data\\',\\n            xytext=(-80, -40), textcoords=\\'offset points\\',\\n            arrowprops=dict(arrowstyle=\"fancy\",\\n                            fc=\"0.6\", ec=\"none\",\\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\"))\\nax.annotate(\\'Thanksgiving\\', xy=(\\'2012-11-25\\', 4500),  xycoords=\\'data\\',\\n            xytext=(-120, -60), textcoords=\\'offset points\\',\\n            bbox=dict(boxstyle=\"round4,pad=.5\", fc=\"0.9\"),\\n            arrowprops=dict(arrowstyle=\"->\",\\n                            connectionstyle=\"angle,angleA=0,angleB=80,rad=20\"))\\nax.annotate(\\'Christmas\\', xy=(\\'2012-12-25\\', 3850),  xycoords=\\'data\\',\\n             xytext=(-30, 0), textcoords=\\'offset points\\',\\n             size=13, ha=\\'right\\', va=\"center\",\\n             bbox=dict(boxstyle=\"round\", alpha=0.1),\\n             arrowprops=dict(arrowstyle=\"wedge,tail_width=0.5\", alpha=0.1));\\n# Label the axes\\nax.set(title=\\'USA births by day of year (1969-1988)\\',\\n       ylabel=\\'average daily births\\')\\n# Format the x axis with centered month labels\\nax.xaxis.set_major_locator(mpl.dates.MonthLocator())\\nax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))\\nax.xaxis.set_major_formatter(plt.NullFormatter())\\nax.xaxis.set_minor_formatter(mpl.dates.DateFormatter(\\'%h\\'));\\nax.set_ylim(3600, 5400);\\n274 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 292}, page_content='Figure 4-72. Annotated average birth rates by day\\nYou’ll notice that the specifications of the arrows and text boxes are very detailed: this\\ngives you the power to create nearly any arrow style you wish. Unfortunately, it also\\nmeans that these sorts of features often must be manually tweaked, a process that can\\nbe very time-consuming when one is producing publication-quality graphics! Finally,\\nI’ll note that the preceding mix of styles is by no means best practice for presenting\\ndata, but rather included as a demonstration of some of the available options.\\nMore discussion and examples of available arrow and annotation styles can be found\\nin the Matplotlib gallery, in particular http://matplotlib.org/examples/pylab_examples/\\nannotation_demo2.html.\\nCustomizing Ticks\\nMatplotlib’s default tick locators and formatters are designed to be generally sufficient\\nin many common situations, but are in no way optimal for every plot. This section\\nwill give several examples of adjusting the tick locations and formatting for the par‐\\nticular plot type you’re interested in.\\nBefore we go into examples, it will be best for us to understand further the object\\nhierarchy of Matplotlib plots. Matplotlib aims to have a Python object representing\\neverything that appears on the plot: for example, recall that the figure is the bound‐\\ning box within which plot elements appear. Each Matplotlib object can also act as a\\ncontainer of sub-objects; for example, each figure can contain one or more axes\\nobjects, each of which in turn contain other objects representing plot contents.\\nThe tick marks are no exception. Each axes has attributes xaxis and yaxis, which in\\nturn have attributes that contain all the properties of the lines, ticks, and labels that\\nmake up the axes.\\nCustomizing Ticks \\n| \\n275'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 293}, page_content=\"Major and Minor Ticks\\nWithin each axis, there is the concept of a major tick mark and a minor tick mark. As\\nthe names would imply, major ticks are usually bigger or more pronounced, while\\nminor ticks are usually smaller. By default, Matplotlib rarely makes use of minor\\nticks, but one place you can see them is within logarithmic plots (Figure 4-73):\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       plt.style.use('seaborn-whitegrid')\\n       import numpy as np\\nIn[2]: ax = plt.axes(xscale='log', yscale='log')\\nFigure 4-73. Example of logarithmic scales and labels\\nWe see here that each major tick shows a large tick mark and a label, while each\\nminor tick shows a smaller tick mark with no label.\\nWe can customize these tick properties—that is, locations and labels—by setting the\\nformatter and locator objects of each axis. Let’s examine these for the x axis of the\\nplot just shown:\\nIn[3]: print(ax.xaxis.get_major_locator())\\n       print(ax.xaxis.get_minor_locator())\\n<matplotlib.ticker.LogLocator object at 0x107530cc0>\\n<matplotlib.ticker.LogLocator object at 0x107530198>\\nIn[4]: print(ax.xaxis.get_major_formatter())\\n       print(ax.xaxis.get_minor_formatter())\\n<matplotlib.ticker.LogFormatterMathtext object at 0x107512780>\\n<matplotlib.ticker.NullFormatter object at 0x10752dc18>\\nWe see that both major and minor tick labels have their locations specified by a\\nLogLocator (which makes sense for a logarithmic plot). Minor ticks, though, have\\ntheir labels formatted by a NullFormatter; this says that no labels will be shown.\\n276 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 294}, page_content='We’ll now show a few examples of setting these locators and formatters for various\\nplots.\\nHiding Ticks or Labels\\nPerhaps the most common tick/label formatting operation is the act of hiding ticks or\\nlabels. We can do this using plt.NullLocator() and plt.NullFormatter(), as\\nshown here (Figure 4-74):\\nIn[5]: ax = plt.axes()\\n       ax.plot(np.random.rand(50))\\n       ax.yaxis.set_major_locator(plt.NullLocator())\\n       ax.xaxis.set_major_formatter(plt.NullFormatter())\\nFigure 4-74. Plot with hidden tick labels (x-axis) and hidden ticks (y-axis)\\nNotice that we’ve removed the labels (but kept the ticks/gridlines) from the x axis,\\nand removed the ticks (and thus the labels as well) from the y axis. Having no ticks at\\nall can be useful in many situations—for example, when you want to show a grid of\\nimages. For instance, consider Figure 4-75, which includes images of different faces,\\nan example often used in supervised machine learning problems (for more informa‐\\ntion, see “In-Depth: Support Vector Machines” on page 405):\\nIn[6]: fig, ax = plt.subplots(5, 5, figsize=(5, 5))\\n       fig.subplots_adjust(hspace=0, wspace=0)\\n       # Get some face data from scikit-learn\\n       from sklearn.datasets import fetch_olivetti_faces\\n       faces = fetch_olivetti_faces().images\\n       for i in range(5):\\n           for j in range(5):\\n               ax[i, j].xaxis.set_major_locator(plt.NullLocator())\\n               ax[i, j].yaxis.set_major_locator(plt.NullLocator())\\n               ax[i, j].imshow(faces[10 * i + j], cmap=\"bone\")\\nCustomizing Ticks \\n| \\n277'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 295}, page_content='Figure 4-75. Hiding ticks within image plots\\nNotice that each image has its own axes, and we’ve set the locators to null because the\\ntick values (pixel number in this case) do not convey relevant information for this\\nparticular visualization.\\nReducing or Increasing the Number of Ticks\\nOne common problem with the default settings is that smaller subplots can end up\\nwith crowded labels. We can see this in the plot grid shown in Figure 4-76:\\nIn[7]: fig, ax = plt.subplots(4, 4, sharex=True, sharey=True)\\nFigure 4-76. A default plot with crowded ticks\\nParticularly for the x ticks, the numbers nearly overlap, making them quite difficult to\\ndecipher. We can fix this with the plt.MaxNLocator(), which allows us to specify the\\nmaximum number of ticks that will be displayed. Given this maximum number, Mat‐\\nplotlib will use internal logic to choose the particular tick locations (Figure 4-77):\\n278 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 296}, page_content=\"In[8]: # For every axis, set the x and y major locator\\n       for axi in ax.flat:\\n           axi.xaxis.set_major_locator(plt.MaxNLocator(3))\\n           axi.yaxis.set_major_locator(plt.MaxNLocator(3))\\n       fig\\nFigure 4-77. Customizing the number of ticks\\nThis makes things much cleaner. If you want even more control over the locations of\\nregularly spaced ticks, you might also use plt.MultipleLocator, which we’ll discuss\\nin the following section.\\nFancy Tick Formats\\nMatplotlib’s default tick formatting can leave a lot to be desired; it works well as a\\nbroad default, but sometimes you’d like to do something more. Consider the plot\\nshown in Figure 4-78, a sine and a cosine:\\nIn[9]: # Plot a sine and cosine curve\\n       fig, ax = plt.subplots()\\n       x = np.linspace(0, 3 * np.pi, 1000)\\n       ax.plot(x, np.sin(x), lw=3, label='Sine')\\n       ax.plot(x, np.cos(x), lw=3, label='Cosine')\\n       # Set up grid, legend, and limits\\n       ax.grid(True)\\n       ax.legend(frameon=False)\\n       ax.axis('equal')\\n       ax.set_xlim(0, 3 * np.pi);\\nCustomizing Ticks \\n| \\n279\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 297}, page_content='Figure 4-78. A default plot with integer ticks\\nThere are a couple changes we might like to make. First, it’s more natural for this data\\nto space the ticks and grid lines in multiples of π. We can do this by setting a Multi\\npleLocator, which locates ticks at a multiple of the number you provide. For good\\nmeasure, we’ll add both major and minor ticks in multiples of π/4 (Figure 4-79):\\nIn[10]: ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))\\n        ax.xaxis.set_minor_locator(plt.MultipleLocator(np.pi / 4))\\n        fig\\nFigure 4-79. Ticks at multiples of pi/2\\nBut now these tick labels look a little bit silly: we can see that they are multiples of π,\\nbut the decimal representation does not immediately convey this. To fix this, we can\\nchange the tick formatter. There’s no built-in formatter for what we want to do, so\\nwe’ll instead use plt.FuncFormatter, which accepts a user-defined function giving\\nfine-grained control over the tick outputs (Figure 4-80):\\nIn[11]: def format_func(value, tick_number):\\n            # find number of multiples of pi/2\\n            N = int(np.round(2 * value / np.pi))\\n            if N == 0:\\n                return \"0\"\\n280 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 298}, page_content='elif N == 1:\\n                return r\"$\\\\pi/2$\"\\n            elif N == 2:\\n                return r\"$\\\\pi$\"\\n            elif N % 2 > 0:\\n                return r\"${0}\\\\pi/2$\".format(N)\\n            else:\\n                return r\"${0}\\\\pi$\".format(N // 2)\\n        ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\\n        fig\\nFigure 4-80. Ticks with custom labels\\nThis is much better! Notice that we’ve made use of Matplotlib’s LaTeX support, speci‐\\nfied by enclosing the string within dollar signs. This is very convenient for display of\\nmathematical symbols and formulae; in this case, \"$\\\\pi$\" is rendered as the Greek\\ncharacter π.\\nThe plt.FuncFormatter() offers extremely fine-grained control over the appearance\\nof your plot ticks, and comes in very handy when you’re preparing plots for presenta‐\\ntion or publication.\\nSummary of Formatters and Locators\\nWe’ve mentioned a couple of the available formatters and locators. We’ll conclude this\\nsection by briefly listing all the built-in locator and formatter options. For more\\ninformation on any of these, refer to the docstrings or to the Matplotlib online docu‐\\nmentation. Each of the following is available in the plt namespace:\\nLocator class\\nDescription\\nNullLocator\\nNo ticks\\nFixedLocator\\nTick locations are fixed\\nIndexLocator\\nLocator for index plots (e.g., where x = range(len(y)))\\nCustomizing Ticks \\n| \\n281'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 299}, page_content=\"Locator class\\nDescription\\nLinearLocator\\nEvenly spaced ticks from min to max\\nLogLocator\\nLogarithmically ticks from min to max\\nMultipleLocator\\nTicks and range are a multiple of base\\nMaxNLocator\\nFinds up to a max number of ticks at nice locations\\nAutoLocator\\n(Default) MaxNLocator with simple defaults\\nAutoMinorLocator Locator for minor ticks\\nFormatter class\\nDescription\\nNullFormatter\\nNo labels on the ticks\\nIndexFormatter\\nSet the strings from a list of labels\\nFixedFormatter\\nSet the strings manually for the labels\\nFuncFormatter\\nUser-defined function sets the labels\\nFormatStrFormatter\\nUse a format string for each value\\nScalarFormatter\\n(Default) Formatter for scalar values\\nLogFormatter\\nDefault formatter for log axes\\nWe’ll see additional examples of these throughout the remainder of the book.\\nCustomizing Matplotlib: Configurations and Stylesheets\\nMatplotlib’s default plot settings are often the subject of complaint among its users.\\nWhile much is slated to change in the 2.0 Matplotlib release, the ability to customize\\ndefault settings helps bring the package in line with your own aesthetic preferences.\\nHere we’ll walk through some of Matplotlib’s runtime configuration (rc) options, and\\ntake a look at the newer stylesheets feature, which contains some nice sets of default\\nconfigurations.\\nPlot Customization by Hand\\nThroughout this chapter, we’ve seen how it is possible to tweak individual plot set‐\\ntings to end up with something that looks a little bit nicer than the default. It’s possi‐\\nble to do these customizations for each individual plot. For example, here is a fairly\\ndrab default histogram (Figure 4-81):\\nIn[1]: import matplotlib.pyplot as plt\\n       plt.style.use('classic')\\n       import numpy as np\\n       %matplotlib inline\\n282 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 300}, page_content=\"In[2]: x = np.random.randn(1000)\\n       plt.hist(x);\\nFigure 4-81. A histogram in Matplotlib’s default style\\nWe can adjust this by hand to make it a much more visually pleasing plot, shown in\\nFigure 4-82:\\nIn[3]: # use a gray background\\n       ax = plt.axes(axisbg='#E6E6E6')\\n       ax.set_axisbelow(True)\\n       # draw solid white grid lines\\n       plt.grid(color='w', linestyle='solid')\\n       # hide axis spines\\n       for spine in ax.spines.values():\\n           spine.set_visible(False)\\n       # hide top and right ticks\\n       ax.xaxis.tick_bottom()\\n       ax.yaxis.tick_left()\\n       # lighten ticks and labels\\n       ax.tick_params(colors='gray', direction='out')\\n       for tick in ax.get_xticklabels():\\n           tick.set_color('gray')\\n       for tick in ax.get_yticklabels():\\n           tick.set_color('gray')\\n       # control face and edge color of histogram\\n       ax.hist(x, edgecolor='#E6E6E6', color='#EE6666');\\nCustomizing Matplotlib: Configurations and Stylesheets \\n| \\n283\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 301}, page_content=\"Figure 4-82. A histogram with manual customizations\\nThis looks better, and you may recognize the look as inspired by the look of the R\\nlanguage’s ggplot visualization package. But this took a whole lot of effort! We defi‐\\nnitely do not want to have to do all that tweaking each time we create a plot. Fortu‐\\nnately, there is a way to adjust these defaults once in a way that will work for all plots.\\nChanging the Defaults: rcParams\\nEach time Matplotlib loads, it defines a runtime configuration (rc) containing the\\ndefault styles for every plot element you create. You can adjust this configuration at\\nany time using the plt.rc convenience routine. Let’s see what it looks like to modify\\nthe rc parameters so that our default plot will look similar to what we did before.\\nWe’ll start by saving a copy of the current rcParams dictionary, so we can easily reset\\nthese changes in the current session:\\nIn[4]: IPython_default = plt.rcParams.copy()\\nNow we can use the plt.rc function to change some of these settings:\\nIn[5]: from matplotlib import cycler\\n       colors = cycler('color',\\n                       ['#EE6666', '#3388BB', '#9988DD',\\n                        '#EECC55', '#88BB44', '#FFBBBB'])\\n       plt.rc('axes', facecolor='#E6E6E6', edgecolor='none',\\n              axisbelow=True, grid=True, prop_cycle=colors)\\n       plt.rc('grid', color='w', linestyle='solid')\\n       plt.rc('xtick', direction='out', color='gray')\\n       plt.rc('ytick', direction='out', color='gray')\\n       plt.rc('patch', edgecolor='#E6E6E6')\\n       plt.rc('lines', linewidth=2)\\nWith these settings defined, we can now create a plot and see our settings in action\\n(Figure 4-83):\\nIn[6]: plt.hist(x);\\n284 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 302}, page_content='Figure 4-83. A customized histogram using rc settings\\nLet’s see what simple line plots look like with these rc parameters (Figure 4-84):\\nIn[7]: for i in range(4):\\n           plt.plot(np.random.rand(10))\\nFigure 4-84. A line plot with customized styles\\nI find this much more aesthetically pleasing than the default styling. If you disagree\\nwith my aesthetic sense, the good news is that you can adjust the rc parameters to\\nsuit your own tastes! These settings can be saved in a .matplotlibrc file, which you can\\nread about in the Matplotlib documentation. That said, I prefer to customize Mat‐\\nplotlib using its stylesheets instead.\\nStylesheets\\nThe version 1.4 release of Matplotlib in August 2014 added a very convenient style\\nmodule, which includes a number of new default stylesheets, as well as the ability to\\ncreate and package your own styles. These stylesheets are formatted similarly to\\nthe .matplotlibrc files mentioned earlier, but must be named with a .mplstyle\\nextension.\\nCustomizing Matplotlib: Configurations and Stylesheets \\n| \\n285'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 303}, page_content=\"Even if you don’t create your own style, the stylesheets included by default are\\nextremely useful. The available styles are listed in plt.style.available—here I’ll list\\nonly the first five for brevity:\\nIn[8]: plt.style.available[:5]\\nOut[8]: ['fivethirtyeight',\\n         'seaborn-pastel',\\n         'seaborn-whitegrid',\\n         'ggplot',\\n         'grayscale']\\nThe basic way to switch to a stylesheet is to call:\\nplt.style.use('stylename')\\nBut keep in mind that this will change the style for the rest of the session! Alterna‐\\ntively, you can use the style context manager, which sets a style temporarily:\\nwith plt.style.context('stylename'):\\n    make_a_plot()\\nLet’s create a function that will make two basic types of plot:\\nIn[9]: def hist_and_lines():\\n           np.random.seed(0)\\n           fig, ax = plt.subplots(1, 2, figsize=(11, 4))\\n           ax[0].hist(np.random.randn(1000))\\n           for i in range(3):\\n               ax[1].plot(np.random.rand(10))\\n           ax[1].legend(['a', 'b', 'c'], loc='lower left')\\nWe’ll use this to explore how these plots look using the various built-in styles.\\nDefault style\\nThe default style is what we’ve been seeing so far throughout the book; we’ll start with\\nthat. First, let’s reset our runtime configuration to the notebook default:\\nIn[10]: # reset rcParams\\n        plt.rcParams.update(IPython_default);\\nNow let’s see how it looks (Figure 4-85):\\nIn[11]: hist_and_lines()\\n286 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 304}, page_content=\"Figure 4-85. Matplotlib’s default style\\nFiveThirtyEight style\\nThe FiveThirtyEight style mimics the graphics found on the popular FiveThirtyEight\\nwebsite. As you can see in Figure 4-86, it is typified by bold colors, thick lines, and\\ntransparent axes.\\nIn[12]: with plt.style.context('fivethirtyeight'):\\n            hist_and_lines()\\nFigure 4-86. The FiveThirtyEight style\\nggplot\\nThe ggplot package in the R language is a very popular visualization tool. Matplot‐\\nlib’s ggplot style mimics the default styles from that package (Figure 4-87):\\nIn[13]: with plt.style.context('ggplot'):\\n            hist_and_lines()\\nCustomizing Matplotlib: Configurations and Stylesheets \\n| \\n287\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 305}, page_content=\"Figure 4-87. The ggplot style\\nBayesian Methods for Hackers style\\nThere is a very nice short online book called Probabilistic Programming and Bayesian\\nMethods for Hackers; it features figures created with Matplotlib, and uses a nice set of\\nrc parameters to create a consistent and visually appealing style throughout the book.\\nThis style is reproduced in the bmh stylesheet (Figure 4-88):\\nIn[14]: with plt.style.context('bmh'):\\n            hist_and_lines()\\nFigure 4-88. The bmh style\\nDark background\\nFor figures used within presentations, it is often useful to have a dark rather than light\\nbackground. The dark_background style provides this (Figure 4-89):\\nIn[15]: with plt.style.context('dark_background'):\\n            hist_and_lines()\\n288 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 306}, page_content=\"Figure 4-89. The dark_background style\\nGrayscale\\nSometimes you might find yourself preparing figures for a print publication that does\\nnot accept color figures. For this, the grayscale style, shown in Figure 4-90, can be\\nvery useful:\\nIn[16]: with plt.style.context('grayscale'):\\n            hist_and_lines()\\nFigure 4-90. The grayscale style\\nSeaborn style\\nMatplotlib also has stylesheets inspired by the Seaborn library (discussed more fully\\nin “Visualization with Seaborn” on page 311). As we will see, these styles are loaded\\nautomatically when Seaborn is imported into a notebook. I’ve found these settings to\\nbe very nice, and tend to use them as defaults in my own data exploration (see\\nFigure 4-91):\\nIn[17]: import seaborn\\n        hist_and_lines()\\nCustomizing Matplotlib: Configurations and Stylesheets \\n| \\n289\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 307}, page_content=\"Figure 4-91. Seaborn’s plotting style\\nWith all of these built-in options for various plot styles, Matplotlib becomes much\\nmore useful for both interactive visualization and creation of figures for publication.\\nThroughout this book, I will generally use one or more of these style conventions\\nwhen creating plots.\\nThree-Dimensional Plotting in Matplotlib\\nMatplotlib was initially designed with only two-dimensional plotting in mind.\\nAround the time of the 1.0 release, some three-dimensional plotting utilities were\\nbuilt on top of Matplotlib’s two-dimensional display, and the result is a convenient (if\\nsomewhat limited) set of tools for three-dimensional data visualization. We enable\\nthree-dimensional plots by importing the mplot3d toolkit, included with the main\\nMatplotlib installation (Figure 4-92):\\nIn[1]: from mpl_toolkits import mplot3d\\nOnce this submodule is imported, we can create a three-dimensional axes by passing\\nthe keyword projection='3d' to any of the normal axes creation routines:\\nIn[2]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\nIn[3]: fig = plt.figure()\\n       ax = plt.axes(projection='3d')\\n290 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 308}, page_content=\"Figure 4-92. An empty three-dimensional axes\\nWith this 3D axes enabled, we can now plot a variety of three-dimensional plot types.\\nThree-dimensional plotting is one of the functionalities that benefits immensely from\\nviewing figures interactively rather than statically in the notebook; recall that to use\\ninteractive figures, you can use %matplotlib notebook rather than %matplotlib\\ninline when running this code.\\nThree-Dimensional Points and Lines\\nThe most basic three-dimensional plot is a line or scatter plot created from sets of (x,\\ny, z) triples. In analogy with the more common two-dimensional plots discussed ear‐\\nlier, we can create these using the ax.plot3D and ax.scatter3D functions. The call\\nsignature for these is nearly identical to that of their two-dimensional counterparts,\\nso you can refer to “Simple Line Plots” on page 224 and “Simple Scatter Plots” on\\npage 233 for more information on controlling the output. Here we’ll plot a trigono‐\\nmetric spiral, along with some points drawn randomly near the line (Figure 4-93):\\nIn[4]: ax = plt.axes(projection='3d')\\n       # Data for a three-dimensional line\\n       zline = np.linspace(0, 15, 1000)\\n       xline = np.sin(zline)\\n       yline = np.cos(zline)\\n       ax.plot3D(xline, yline, zline, 'gray')\\n       # Data for three-dimensional scattered points\\n       zdata = 15 * np.random.random(100)\\n       xdata = np.sin(zdata) + 0.1 * np.random.randn(100)\\n       ydata = np.cos(zdata) + 0.1 * np.random.randn(100)\\n       ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens');\\nThree-Dimensional Plotting in Matplotlib \\n| \\n291\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 309}, page_content=\"Figure 4-93. Points and lines in three dimensions\\nNotice that by default, the scatter points have their transparency adjusted to give a\\nsense of depth on the page. While the three-dimensional effect is sometimes difficult\\nto see within a static image, an interactive view can lead to some nice intuition about\\nthe layout of the points.\\nThree-Dimensional Contour Plots\\nAnalogous to the contour plots we explored in “Density and Contour Plots” on page\\n241, mplot3d contains tools to create three-dimensional relief plots using the same\\ninputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input\\ndata to be in the form of two-dimensional regular grids, with the Z data evaluated at\\neach point. Here we’ll show a three-dimensional contour diagram of a three-\\ndimensional sinusoidal function (Figure 4-94):\\nIn[5]: def f(x, y):\\n           return np.sin(np.sqrt(x ** 2 + y ** 2))\\n       x = np.linspace(-6, 6, 30)\\n       y = np.linspace(-6, 6, 30)\\n       X, Y = np.meshgrid(x, y)\\n       Z = f(X, Y)\\nIn[6]: fig = plt.figure()\\n       ax = plt.axes(projection='3d')\\n       ax.contour3D(X, Y, Z, 50, cmap='binary')\\n       ax.set_xlabel('x')\\n       ax.set_ylabel('y')\\n       ax.set_zlabel('z');\\n292 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 310}, page_content='Figure 4-94. A three-dimensional contour plot\\nSometimes the default viewing angle is not optimal, in which case we can use the\\nview_init method to set the elevation and azimuthal angles. In this example (the\\nresult of which is shown in Figure 4-95), we’ll use an elevation of 60 degrees (that is,\\n60 degrees above the x-y plane) and an azimuth of 35 degrees (that is, rotated 35\\ndegrees counter-clockwise about the z-axis):\\nIn[7]: ax.view_init(60, 35)\\n       fig\\nFigure 4-95. Adjusting the view angle for a three-dimensional plot\\nAgain, note that we can accomplish this type of rotation interactively by clicking and\\ndragging when using one of Matplotlib’s interactive backends.\\nWireframes and Surface Plots\\nTwo other types of three-dimensional plots that work on gridded data are wireframes\\nand surface plots. These take a grid of values and project it onto the specified three-\\ndimensional surface, and can make the resulting three-dimensional forms quite easy\\nto visualize. Here’s an example using a wireframe (Figure 4-96):\\nThree-Dimensional Plotting in Matplotlib \\n| \\n293'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 311}, page_content=\"In[8]: fig = plt.figure()\\n       ax = plt.axes(projection='3d')\\n       ax.plot_wireframe(X, Y, Z, color='black')\\n       ax.set_title('wireframe');\\nFigure 4-96. A wireframe plot\\nA surface plot is like a wireframe plot, but each face of the wireframe is a filled poly‐\\ngon. Adding a colormap to the filled polygons can aid perception of the topology of\\nthe surface being visualized (Figure 4-97):\\nIn[9]: ax = plt.axes(projection='3d')\\n       ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\\n                       cmap='viridis', edgecolor='none')\\n       ax.set_title('surface');\\nFigure 4-97. A three-dimensional surface plot\\nNote that though the grid of values for a surface plot needs to be two-dimensional, it\\nneed not be rectilinear. Here is an example of creating a partial polar grid, which\\nwhen used with the surface3D plot can give us a slice into the function we’re visualiz‐\\ning (Figure 4-98):\\n294 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 312}, page_content=\"In[10]: r = np.linspace(0, 6, 20)\\n        theta = np.linspace(-0.9 * np.pi, 0.8 * np.pi, 40)\\n        r, theta = np.meshgrid(r, theta)\\n        X = r * np.sin(theta)\\n        Y = r * np.cos(theta)\\n        Z = f(X, Y)\\n        ax = plt.axes(projection='3d')\\n        ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\\n                        cmap='viridis', edgecolor='none');\\nFigure 4-98. A polar surface plot\\nSurface Triangulations\\nFor some applications, the evenly sampled grids required by the preceding routines\\nare overly restrictive and inconvenient. In these situations, the triangulation-based\\nplots can be very useful. What if rather than an even draw from a Cartesian or a polar\\ngrid, we instead have a set of random draws?\\nIn[11]: theta = 2 * np.pi * np.random.random(1000)\\n        r = 6 * np.random.random(1000)\\n        x = np.ravel(r * np.sin(theta))\\n        y = np.ravel(r * np.cos(theta))\\n        z = f(x, y)\\nWe could create a scatter plot of the points to get an idea of the surface we’re sampling\\nfrom (Figure 4-99):\\nIn[12]: ax = plt.axes(projection='3d')\\n        ax.scatter(x, y, z, c=z, cmap='viridis', linewidth=0.5);\\nThree-Dimensional Plotting in Matplotlib \\n| \\n295\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 313}, page_content=\"Figure 4-99. A three-dimensional sampled surface\\nThis leaves a lot to be desired. The function that will help us in this case is\\nax.plot_trisurf, which creates a surface by first finding a set of triangles formed\\nbetween adjacent points (the result is shown in Figure 4-100; remember that x, y, and\\nz here are one-dimensional arrays):\\nIn[13]: ax = plt.axes(projection='3d')\\n        ax.plot_trisurf(x, y, z,\\n                        cmap='viridis', edgecolor='none');\\nFigure 4-100. A triangulated surface plot\\nThe result is certainly not as clean as when it is plotted with a grid, but the flexibility\\nof such a triangulation allows for some really interesting three-dimensional plots. For\\nexample, it is actually possible to plot a three-dimensional Möbius strip using this, as\\nwe’ll see next.\\nExample: Visualizing a Möbius strip\\nA Möbius strip is similar to a strip of paper glued into a loop with a half-twist. Topo‐\\nlogically, it’s quite interesting because despite appearances it has only a single side!\\nHere we will visualize such an object using Matplotlib’s three-dimensional tools. The\\nkey to creating the Möbius strip is to think about its parameterization: it’s a two-\\n296 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 314}, page_content=\"dimensional strip, so we need two intrinsic dimensions. Let’s call them θ, which\\nranges from 0 to 2π around the loop, and w which ranges from –1 to 1 across the\\nwidth of the strip:\\nIn[14]: theta = np.linspace(0, 2 * np.pi, 30)\\n        w = np.linspace(-0.25, 0.25, 8)\\n        w, theta = np.meshgrid(w, theta)\\nNow from this parameterization, we must determine the (x, y, z) positions of the\\nembedded strip.\\nThinking about it, we might realize that there are two rotations happening: one is the\\nposition of the loop about its center (what we’ve called θ), while the other is the twist‐\\ning of the strip about its axis (we’ll call this ϕ). For a Möbius strip, we must have the\\nstrip make half a twist during a full loop, or Δϕ = Δθ/2.\\nIn[15]: phi = 0.5 * theta\\nNow we use our recollection of trigonometry to derive the three-dimensional embed‐\\nding. We’ll define r, the distance of each point from the center, and use this to find the\\nembedded x, y, z  coordinates:\\nIn[16]: # radius in x-y plane\\n        r = 1 + w * np.cos(phi)\\n        x = np.ravel(r * np.cos(theta))\\n        y = np.ravel(r * np.sin(theta))\\n        z = np.ravel(w * np.sin(phi))\\nFinally, to plot the object, we must make sure the triangulation is correct. The best\\nway to do this is to define the triangulation within the underlying parameterization,\\nand then let Matplotlib project this triangulation into the three-dimensional space of\\nthe Möbius strip. This can be accomplished as follows (Figure 4-101):\\nIn[17]: # triangulate in the underlying parameterization\\n        from matplotlib.tri import Triangulation\\n        tri = Triangulation(np.ravel(w), np.ravel(theta))\\n        ax = plt.axes(projection='3d')\\n        ax.plot_trisurf(x, y, z, triangles=tri.triangles,\\n                        cmap='viridis', linewidths=0.2);\\n        ax.set_xlim(-1, 1); ax.set_ylim(-1, 1); ax.set_zlim(-1, 1);\\nThree-Dimensional Plotting in Matplotlib \\n| \\n297\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 315}, page_content=\"Figure 4-101. Visualizing a Möbius strip\\nCombining all of these techniques, it is possible to create and display a wide variety of\\nthree-dimensional objects and patterns in Matplotlib.\\nGeographic Data with Basemap\\nOne common type of visualization in data science is that of geographic data. Matplot‐\\nlib’s main tool for this type of visualization is the Basemap toolkit, which is one of\\nseveral Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly,\\nBasemap feels a bit clunky to use, and often even simple visualizations take much\\nlonger to render than you might hope. More modern solutions, such as leaflet or the\\nGoogle Maps API, may be a better choice for more intensive map visualizations. Still,\\nBasemap is a useful tool for Python users to have in their virtual toolbelts. In this sec‐\\ntion, we’ll show several examples of the type of map visualization that is possible with\\nthis toolkit.\\nInstallation of Basemap is straightforward; if you’re using conda you can type this and\\nthe package will be downloaded:\\n$ conda install basemap\\nWe add just a single new import to our standard boilerplate:\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       from mpl_toolkits.basemap import Basemap\\nOnce you have the Basemap toolkit installed and imported, geographic plots are just\\na few lines away (the graphics in Figure 4-102 also require the PIL package in Python\\n2, or the pillow package in Python 3):\\nIn[2]: plt.figure(figsize=(8, 8))\\n       m = Basemap(projection='ortho', resolution=None, lat_0=50, lon_0=-100)\\n       m.bluemarble(scale=0.5);\\n298 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 316}, page_content=\"Figure 4-102. A “bluemarble” projection of the Earth\\nThe meaning of the arguments to Basemap will be discussed momentarily.\\nThe useful thing is that the globe shown here is not a mere image; it is a fully func‐\\ntioning Matplotlib axes that understands spherical coordinates and allows us to easily\\nover-plot data on the map! For example, we can use a different map projection, zoom\\nin to North America, and plot the location of Seattle. We’ll use an etopo image (which\\nshows topographical features both on land and under the ocean) as the map back‐\\nground (Figure 4-103):\\nIn[3]: fig = plt.figure(figsize=(8, 8))\\n       m = Basemap(projection='lcc', resolution=None,\\n                   width=8E6, height=8E6,\\n                   lat_0=45, lon_0=-100,)\\n       m.etopo(scale=0.5, alpha=0.5)\\n       # Map (long, lat) to (x, y) for plotting\\n       x, y = m(-122.3, 47.6)\\n       plt.plot(x, y, 'ok', markersize=5)\\n       plt.text(x, y, ' Seattle', fontsize=12);\\nGeographic Data with Basemap \\n| \\n299\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 317}, page_content='Figure 4-103. Plotting data and labels on the map\\nThis gives you a brief glimpse into the sort of geographic visualizations that are possi‐\\nble with just a few lines of Python. We’ll now discuss the features of Basemap in more\\ndepth, and provide several examples of visualizing map data. Using these brief exam‐\\nples as building blocks, you should be able to create nearly any map visualization that\\nyou desire.\\nMap Projections\\nThe first thing to decide when you are using maps is which projection to use. You’re\\nprobably familiar with the fact that it is impossible to project a spherical map, such as\\nthat of the Earth, onto a flat surface without somehow distorting it or breaking its\\ncontinuity. These projections have been developed over the course of human history,\\nand there are a lot of choices! Depending on the intended use of the map projection,\\nthere are certain map features (e.g., direction, area, distance, shape, or other consider‐\\nations) that are useful to maintain.\\nThe Basemap package implements several dozen such projections, all referenced by a\\nshort format code. Here we’ll briefly demonstrate some of the more common ones.\\nWe’ll start by defining a convenience routine to draw our world map along with the\\nlongitude and latitude lines:\\n300 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 318}, page_content=\"In[4]: from itertools import chain\\n       def draw_map(m, scale=0.2):\\n           # draw a shaded-relief image\\n           m.shadedrelief(scale=scale)\\n           # lats and longs are returned as a dictionary\\n           lats = m.drawparallels(np.linspace(-90, 90, 13))\\n           lons = m.drawmeridians(np.linspace(-180, 180, 13))\\n           # keys contain the plt.Line2D instances\\n           lat_lines = chain(*(tup[1][0] for tup in lats.items()))\\n           lon_lines = chain(*(tup[1][0] for tup in lons.items()))\\n           all_lines = chain(lat_lines, lon_lines)\\n           # cycle through these lines and set the desired style\\n           for line in all_lines:\\n               line.set(linestyle='-', alpha=0.3, color='w')\\nCylindrical projections\\nThe simplest of map projections are cylindrical projections, in which lines of constant\\nlatitude and longitude are mapped to horizontal and vertical lines, respectively. This\\ntype of mapping represents equatorial regions quite well, but results in extreme dis‐\\ntortions near the poles. The spacing of latitude lines varies between different cylindri‐\\ncal projections, leading to different conservation properties, and different distortion\\nnear the poles. In Figure 4-104, we show an example of the equidistant cylindrical pro‐\\njection, which chooses a latitude scaling that preserves distances along meridians.\\nOther cylindrical projections are the Mercator (projection='merc') and the cylin‐\\ndrical equal-area (projection='cea') projections.\\nIn[5]: fig = plt.figure(figsize=(8, 6), edgecolor='w')\\n       m = Basemap(projection='cyl', resolution=None,\\n                   llcrnrlat=-90, urcrnrlat=90,\\n                   llcrnrlon=-180, urcrnrlon=180, )\\n       draw_map(m)\\nFigure 4-104. Cylindrical equal-area projection\\nGeographic Data with Basemap \\n| \\n301\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 319}, page_content=\"The additional arguments to Basemap for this view specify the latitude (lat) and lon‐\\ngitude (lon) of the lower-left corner (llcrnr) and upper-right corner (urcrnr) for the\\ndesired map, in units of degrees.\\nPseudo-cylindrical projections\\nPseudo-cylindrical projections relax the requirement that meridians (lines of constant\\nlongitude) remain vertical; this can give better properties near the poles of the projec‐\\ntion. The Mollweide projection (projection='moll') is one common example of\\nthis, in which all meridians are elliptical arcs (Figure 4-105). It is constructed so as to\\npreserve area across the map: though there are distortions near the poles, the area of\\nsmall patches reflects the true area. Other pseudo-cylindrical projections are the\\nsinusoidal (projection='sinu') and Robinson (projection='robin') projections.\\nIn[6]: fig = plt.figure(figsize=(8, 6), edgecolor='w')\\n       m = Basemap(projection='moll', resolution=None,\\n                   lat_0=0, lon_0=0)\\n       draw_map(m)\\nFigure 4-105. The Molleweide projection\\nThe extra arguments to Basemap here refer to the central latitude (lat_0) and longi‐\\ntude (lon_0) for the desired map.\\nPerspective projections\\nPerspective projections are constructed using a particular choice of perspective point,\\nsimilar to if you photographed the Earth from a particular point in space (a point\\nwhich, for some projections, technically lies within the Earth!). One common exam‐\\nple is the orthographic projection (projection='ortho'), which shows one side of\\nthe globe as seen from a viewer at a very long distance. Thus, it can show only half the\\nglobe at a time. Other perspective-based projections include the gnomonic projection\\n(projection='gnom') and stereographic projection (projection='stere'). These are\\noften the most useful for showing small portions of the map.\\n302 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 320}, page_content=\"Here is an example of the orthographic projection (Figure 4-106):\\nIn[7]: fig = plt.figure(figsize=(8, 8))\\n       m = Basemap(projection='ortho', resolution=None,\\n                   lat_0=50, lon_0=0)\\n       draw_map(m);\\nFigure 4-106. The orthographic projection\\nConic projections\\nA conic projection projects the map onto a single cone, which is then unrolled. This\\ncan lead to very good local properties, but regions far from the focus point of the\\ncone may become very distorted. One example of this is the Lambert conformal conic\\nprojection (projection='lcc'), which we saw earlier in the map of North America.\\nIt projects the map onto a cone arranged in such a way that two standard parallels\\n(specified in Basemap by lat_1 and lat_2) have well-represented distances, with scale\\ndecreasing between them and increasing outside of them. Other useful conic projec‐\\ntions are the equidistant conic (projection='eqdc') and the Albers equal-area (pro\\njection='aea') projection (Figure 4-107). Conic projections, like perspective\\nprojections, tend to be good choices for representing small to medium patches of the\\nglobe.\\nIn[8]: fig = plt.figure(figsize=(8, 8))\\n       m = Basemap(projection='lcc', resolution=None,\\n                   lon_0=0, lat_0=50, lat_1=45, lat_2=55,\\nGeographic Data with Basemap \\n| \\n303\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 321}, page_content='width=1.6E7, height=1.2E7)\\n       draw_map(m)\\nFigure 4-107. The Albers equal-area projection\\nOther projections\\nIf you’re going to do much with map-based visualizations, I encourage you to read up\\non other available projections, along with their properties, advantages, and disadvan‐\\ntages. Most likely, they are available in the Basemap package. If you dig deep enough\\ninto this topic, you’ll find an incredible subculture of geo-viz geeks who will be ready\\nto argue fervently in support of their favorite projection for any given application!\\nDrawing a Map Background\\nEarlier we saw the bluemarble() and shadedrelief() methods for projecting global\\nimages on the map, as well as the drawparallels() and drawmeridians() methods\\nfor drawing lines of constant latitude and longitude. The Basemap package contains a\\nrange of useful functions for drawing borders of physical features like continents,\\noceans, lakes, and rivers, as well as political boundaries such as countries and US\\nstates and counties. The following are some of the available drawing functions that\\nyou may wish to explore using IPython’s help features:\\n• Physical boundaries and bodies of water\\ndrawcoastlines()\\nDraw continental coast lines\\ndrawlsmask()\\nDraw a mask between the land and sea, for use with projecting images on\\none or the other\\n304 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 322}, page_content='drawmapboundary()\\nDraw the map boundary, including the fill color for oceans\\ndrawrivers()\\nDraw rivers on the map\\nfillcontinents()\\nFill the continents with a given color; optionally fill lakes with another color\\n• Political boundaries\\ndrawcountries()\\nDraw country boundaries\\ndrawstates()\\nDraw US state boundaries\\ndrawcounties()\\nDraw US county boundaries\\n• Map features\\ndrawgreatcircle()\\nDraw a great circle between two points\\ndrawparallels()\\nDraw lines of constant latitude\\ndrawmeridians()\\nDraw lines of constant longitude\\ndrawmapscale()\\nDraw a linear scale on the map\\n• Whole-globe images\\nbluemarble()\\nProject NASA’s blue marble image onto the map\\nshadedrelief()\\nProject a shaded relief image onto the map\\netopo()\\nDraw an etopo relief image onto the map\\nwarpimage()\\nProject a user-provided image onto the map\\nGeographic Data with Basemap \\n| \\n305'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 323}, page_content='For the boundary-based features, you must set the desired resolution when creating a\\nBasemap image. The resolution argument of the Basemap class sets the level of detail\\nin boundaries, either \\'c\\' (crude), \\'l\\' (low), \\'i\\' (intermediate), \\'h\\' (high), \\'f\\' (full),\\nor None if no boundaries will be used. This choice is important: setting high-\\nresolution boundaries on a global map, for example, can be very slow.\\nHere’s an example of drawing land/sea boundaries, and the effect of the resolution\\nparameter. We’ll create both a low- and high-resolution map of Scotland’s beautiful\\nIsle of Skye. It’s located at 57.3°N, 6.2°W, and a map of 90,000×120,000 kilometers\\nshows it well (Figure 4-108):\\nIn[9]: fig, ax = plt.subplots(1, 2, figsize=(12, 8))\\n       for i, res in enumerate([\\'l\\', \\'h\\']):\\n           m = Basemap(projection=\\'gnom\\', lat_0=57.3, lon_0=-6.2,\\n                       width=90000, height=120000, resolution=res, ax=ax[i])\\n           m.fillcontinents(color=\"#FFDDCC\", lake_color=\\'#DDEEFF\\')\\n           m.drawmapboundary(fill_color=\"#DDEEFF\")\\n           m.drawcoastlines()\\n           ax[i].set_title(\"resolution=\\'{0}\\'\".format(res));\\nFigure 4-108. Map boundaries at low and high resolution\\nNotice that the low-resolution coastlines are not suitable for this level of zoom, while\\nhigh-resolution works just fine. The low level would work just fine for a global view,\\nhowever, and would be much faster than loading the high-resolution border data for\\nthe entire globe! It might require some experimentation to find the correct resolution\\n306 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 324}, page_content='parameter for a given view; the best route is to start with a fast, low-resolution plot\\nand increase the resolution as needed.\\nPlotting Data on Maps\\nPerhaps the most useful piece of the Basemap toolkit is the ability to over-plot a vari‐\\nety of data onto a map background. For simple plotting and text, any plt function\\nworks on the map; you can use the Basemap instance to project latitude and longitude\\ncoordinates to (x, y) coordinates for plotting with plt, as we saw earlier in the Seat‐\\ntle example.\\nIn addition to this, there are many map-specific functions available as methods of the\\nBasemap instance. These work very similarly to their standard Matplotlib counter‐\\nparts, but have an additional Boolean argument latlon, which if set to True allows\\nyou to pass raw latitudes and longitudes to the method, rather than projected (x, y)\\ncoordinates.\\nSome of these map-specific methods are:\\ncontour()/contourf()\\nDraw contour lines or filled contours\\nimshow()\\nDraw an image\\npcolor()/pcolormesh()\\nDraw a pseudocolor plot for irregular/regular meshes\\nplot()\\nDraw lines and/or markers\\nscatter()\\nDraw points with markers\\nquiver()\\nDraw vectors\\nbarbs()\\nDraw wind barbs\\ndrawgreatcircle()\\nDraw a great circle\\nWe’ll see examples of a few of these as we continue. For more information on these\\nfunctions, including several example plots, see the online Basemap documentation.\\nGeographic Data with Basemap \\n| \\n307'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 325}, page_content=\"Example: California Cities\\nRecall that in “Customizing Plot Legends” on page 249, we demonstrated the use of\\nsize and color in a scatter plot to convey information about the location, size, and\\npopulation of California cities. Here, we’ll create this plot again, but using Basemap to\\nput the data in context.\\nWe start with loading the data, as we did before:\\nIn[10]: import pandas as pd\\n        cities = pd.read_csv('data/california_cities.csv')\\n        # Extract the data we're interested in\\n        lat = cities['latd'].values\\n        lon = cities['longd'].values\\n        population = cities['population_total'].values\\n        area = cities['area_total_km2'].values\\nNext, we set up the map projection, scatter the data, and then create a colorbar and\\nlegend (Figure 4-109):\\nIn[11]: # 1. Draw the map background\\n        fig = plt.figure(figsize=(8, 8))\\n        m = Basemap(projection='lcc', resolution='h',\\n                    lat_0=37.5, lon_0=-119,\\n                    width=1E6, height=1.2E6)\\n        m.shadedrelief()\\n        m.drawcoastlines(color='gray')\\n        m.drawcountries(color='gray')\\n        m.drawstates(color='gray')\\n        # 2. scatter city data, with color reflecting population\\n        # and size reflecting area\\n        m.scatter(lon, lat, latlon=True,\\n                  c=np.log10(population), s=area,\\n                  cmap='Reds', alpha=0.5)\\n        # 3. create colorbar and legend\\n        plt.colorbar(label=r'$\\\\log_{10}({\\\\rm population})$')\\n        plt.clim(3, 7)\\n        # make legend with dummy points\\n        for a in [100, 300, 500]:\\n            plt.scatter([], [], c='k', alpha=0.5, s=a,\\n                        label=str(a) + ' km$^2$')\\n        plt.legend(scatterpoints=1, frameon=False,\\n                   labelspacing=1, loc='lower left');\\n308 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 326}, page_content='Figure 4-109. Scatter plot over a map background\\nThis shows us roughly where larger populations of people have settled in California:\\nthey are clustered near the coast in the Los Angeles and San Francisco areas,\\nstretched along the highways in the flat central valley, and avoiding almost completely\\nthe mountainous regions along the borders of the state.\\nExample: Surface Temperature Data\\nAs an example of visualizing some more continuous geographic data, let’s consider\\nthe “polar vortex” that hit the eastern half of the United States in January 2014. A\\ngreat source for any sort of climatic data is NASA’s Goddard Institute for Space Stud‐\\nies. Here we’ll use the GIS 250 temperature data, which we can download using shell\\ncommands (these commands may have to be modified on Windows machines). The\\ndata used here was downloaded on 6/12/2016, and the file size is approximately 9\\nMB:\\nIn[12]: # !curl -O http://data.giss.nasa.gov/pub/gistemp/gistemp250.nc.gz\\n        # !gunzip gistemp250.nc.gz\\nThe data comes in NetCDF format, which can be read in Python by the netCDF4\\nlibrary. You can install this library as shown here:\\n$ conda install netcdf4\\nGeographic Data with Basemap \\n| \\n309'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 327}, page_content=\"We read the data as follows:\\nIn[13]: from netCDF4 import Dataset\\n        data = Dataset('gistemp250.nc')\\nThe file contains many global temperature readings on a variety of dates; we need to\\nselect the index of the date we’re interested in—in this case, January 15, 2014:\\nIn[14]: from netCDF4 import date2index\\n        from datetime import datetime\\n        timeindex = date2index(datetime(2014, 1, 15),\\n                               data.variables['time'])\\nNow we can load the latitude and longitude data, as well as the temperature anomaly\\nfor this index:\\nIn[15]: lat = data.variables['lat'][:]\\n        lon = data.variables['lon'][:]\\n        lon, lat = np.meshgrid(lon, lat)\\n        temp_anomaly = data.variables['tempanomaly'][timeindex]\\nFinally, we’ll use the pcolormesh() method to draw a color mesh of the data. We’ll\\nlook at North America, and use a shaded relief map in the background. Note that for\\nthis data we specifically chose a divergent colormap, which has a neutral color at zero\\nand two contrasting colors at negative and positive values (Figure 4-110). We’ll also\\nlightly draw the coastlines over the colors for reference:\\nIn[16]: fig = plt.figure(figsize=(10, 8))\\n        m = Basemap(projection='lcc', resolution='c',\\n                    width=8E6, height=8E6,\\n                    lat_0=45, lon_0=-100,)\\n        m.shadedrelief(scale=0.5)\\n        m.pcolormesh(lon, lat, temp_anomaly,\\n                     latlon=True, cmap='RdBu_r')\\n        plt.clim(-8, 8)\\n        m.drawcoastlines(color='lightgray')\\n        plt.title('January 2014 Temperature Anomaly')\\n        plt.colorbar(label='temperature anomaly (°C)');\\nThe data paints a picture of the localized, extreme temperature anomalies that hap‐\\npened during that month. The eastern half of the United States was much colder than\\nnormal, while the western half and Alaska were much warmer. Regions with no\\nrecorded temperature show the map background.\\n310 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 328}, page_content='Figure 4-110. The temperature anomaly in January 2014\\nVisualization with Seaborn\\nMatplotlib has proven to be an incredibly useful and popular visualization tool, but\\neven avid users will admit it often leaves much to be desired. There are several valid\\ncomplaints about Matplotlib that often come up:\\n• Prior to version 2.0, Matplotlib’s defaults are not exactly the best choices. It was\\nbased off of MATLAB circa 1999, and this often shows.\\n• Matplotlib’s API is relatively low level. Doing sophisticated statistical visualiza‐\\ntion is possible, but often requires a lot of boilerplate code.\\n• Matplotlib predated Pandas by more than a decade, and thus is not designed for\\nuse with Pandas DataFrames. In order to visualize data from a Pandas DataFrame,\\nyou must extract each Series and often concatenate them together into the right\\nformat. It would be nicer to have a plotting library that can intelligently use the\\nDataFrame labels in a plot.\\nAn answer to these problems is Seaborn. Seaborn provides an API on top of Matplot‐\\nlib that offers sane choices for plot style and color defaults, defines simple high-level\\nfunctions for common statistical plot types, and integrates with the functionality pro‐\\nvided by Pandas DataFrames.\\nVisualization with Seaborn \\n| \\n311'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 329}, page_content=\"To be fair, the Matplotlib team is addressing this: it has recently added the plt.style\\ntools (discussed in “Customizing Matplotlib: Configurations and Stylesheets” on page\\n282), and is starting to handle Pandas data more seamlessly. The 2.0 release of the\\nlibrary will include a new default stylesheet that will improve on the current status\\nquo. But for all the reasons just discussed, Seaborn remains an extremely useful\\nadd-on.\\nSeaborn Versus Matplotlib\\nHere is an example of a simple random-walk plot in Matplotlib, using its classic plot\\nformatting and colors. We start with the typical imports:\\nIn[1]: import matplotlib.pyplot as plt\\n       plt.style.use('classic')\\n       %matplotlib inline\\n       import numpy as np\\n       import pandas as pd\\nNow we create some random walk data:\\nIn[2]: # Create some data\\n       rng = np.random.RandomState(0)\\n       x = np.linspace(0, 10, 500)\\n       y = np.cumsum(rng.randn(500, 6), 0)\\nAnd do a simple plot (Figure 4-111):\\nIn[3]: # Plot the data with Matplotlib defaults\\n       plt.plot(x, y)\\n       plt.legend('ABCDEF', ncol=2, loc='upper left');\\nFigure 4-111. Data in Matplotlib’s default style\\nAlthough the result contains all the information we’d like it to convey, it does so in a\\nway that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the\\ncontext of 21st-century data visualization.\\n312 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 330}, page_content=\"Now let’s take a look at how it works with Seaborn. As we will see, Seaborn has many\\nof its own high-level plotting routines, but it can also overwrite Matplotlib’s default\\nparameters and in turn get even simple Matplotlib scripts to produce vastly superior\\noutput. We can set the style by calling Seaborn’s set() method. By convention, Sea‐\\nborn is imported as sns:\\nIn[4]: import seaborn as sns\\n       sns.set()\\nNow let’s rerun the same two lines as before (Figure 4-112):\\nIn[5]: # same plotting code as above!\\n       plt.plot(x, y)\\n       plt.legend('ABCDEF', ncol=2, loc='upper left');\\nFigure 4-112. Data in Seaborn’s default style\\nAh, much better!\\nExploring Seaborn Plots\\nThe main idea of Seaborn is that it provides high-level commands to create a variety\\nof plot types useful for statistical data exploration, and even some statistical model\\nfitting.\\nLet’s take a look at a few of the datasets and plot types available in Seaborn. Note that\\nall of the following could be done using raw Matplotlib commands (this is, in fact,\\nwhat Seaborn does under the hood), but the Seaborn API is much more convenient.\\nVisualization with Seaborn \\n| \\n313\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 331}, page_content=\"Histograms, KDE, and densities\\nOften in statistical data visualization, all you want is to plot histograms and joint dis‐\\ntributions of variables. We have seen that this is relatively straightforward in Matplot‐\\nlib (Figure 4-113):\\nIn[6]: data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)\\n       data = pd.DataFrame(data, columns=['x', 'y'])\\n       for col in 'xy':\\n           plt.hist(data[col], normed=True, alpha=0.5)\\nFigure 4-113. Histograms for visualizing distributions\\nRather than a histogram, we can get a smooth estimate of the distribution using a\\nkernel density estimation, which Seaborn does with sns.kdeplot (Figure 4-114):\\nIn[7]: for col in 'xy':\\n           sns.kdeplot(data[col], shade=True)\\n314 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 332}, page_content=\"Figure 4-114. Kernel density estimates for visualizing distributions\\nHistograms and KDE can be combined using distplot (Figure 4-115):\\nIn[8]: sns.distplot(data['x'])\\n       sns.distplot(data['y']);\\nFigure 4-115. Kernel density and histograms plotted together\\nIf we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional\\nvisualization of the data (Figure 4-116):\\nIn[9]: sns.kdeplot(data);\\nVisualization with Seaborn \\n| \\n315\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 333}, page_content='Figure 4-116. A two-dimensional kernel density plot\\nWe can see the joint distribution and the marginal distributions together using\\nsns.jointplot. For this plot, we’ll set the style to a white background (Figure 4-117):\\nIn[10]: with sns.axes_style(\\'white\\'):\\n            sns.jointplot(\"x\", \"y\", data, kind=\\'kde\\');\\nFigure 4-117. A joint distribution plot with a two-dimensional kernel density estimate\\n316 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 334}, page_content='There are other parameters that can be passed to jointplot—for example, we can\\nuse a hexagonally based histogram instead (Figure 4-118):\\nIn[11]: with sns.axes_style(\\'white\\'):\\n            sns.jointplot(\"x\", \"y\", data, kind=\\'hex\\')\\nFigure 4-118. A joint distribution plot with a hexagonal bin representation\\nPair plots\\nWhen you generalize joint plots to datasets of larger dimensions, you end up with\\npair plots. This is very useful for exploring correlations between multidimensional\\ndata, when you’d like to plot all pairs of values against each other.\\nWe’ll demo this with the well-known Iris dataset, which lists measurements of petals\\nand sepals of three iris species:\\nIn[12]: iris = sns.load_dataset(\"iris\")\\n        iris.head()\\nOut[12]:    sepal_length  sepal_width  petal_length  petal_width species\\n         0           5.1          3.5           1.4          0.2  setosa\\n         1           4.9          3.0           1.4          0.2  setosa\\n         2           4.7          3.2           1.3          0.2  setosa\\n         3           4.6          3.1           1.5          0.2  setosa\\n         4           5.0          3.6           1.4          0.2  setosa\\nVisualization with Seaborn \\n| \\n317'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 335}, page_content=\"Visualizing the multidimensional relationships among the samples is as easy as call‐\\ning sns.pairplot (Figure 4-119):\\nIn[13]: sns.pairplot(iris, hue='species', size=2.5);\\nFigure 4-119. A pair plot showing the relationships between four variables\\nFaceted histograms\\nSometimes the best way to view data is via histograms of subsets. Seaborn’s FacetGrid\\nmakes this extremely simple. We’ll take a look at some data that shows the amount\\nthat restaurant staff receive in tips based on various indicator data (Figure 4-120):\\nIn[14]: tips = sns.load_dataset('tips')\\n        tips.head()\\nOut[14]:    total_bill   tip     sex smoker  day    time  size\\n         0       16.99  1.01  Female     No  Sun  Dinner     2\\n         1       10.34  1.66    Male     No  Sun  Dinner     3\\n         2       21.01  3.50    Male     No  Sun  Dinner     3\\n318 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 336}, page_content='3       23.68  3.31    Male     No  Sun  Dinner     2\\n         4       24.59  3.61  Female     No  Sun  Dinner     4\\nIn[15]: tips[\\'tip_pct\\'] = 100 * tips[\\'tip\\'] / tips[\\'total_bill\\']\\n        grid = sns.FacetGrid(tips, row=\"sex\", col=\"time\", margin_titles=True)\\n        grid.map(plt.hist, \"tip_pct\", bins=np.linspace(0, 40, 15));\\nFigure 4-120. An example of a faceted histogram\\nFactor plots\\nFactor plots can be useful for this kind of visualization as well. This allows you to\\nview the distribution of a parameter within bins defined by any other parameter\\n(Figure 4-121):\\nIn[16]: with sns.axes_style(style=\\'ticks\\'):\\n            g = sns.factorplot(\"day\", \"total_bill\", \"sex\", data=tips, kind=\"box\")\\n            g.set_axis_labels(\"Day\", \"Total Bill\");\\nVisualization with Seaborn \\n| \\n319'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 337}, page_content='Figure 4-121. An example of a factor plot, comparing distributions given various discrete\\nfactors\\nJoint distributions\\nSimilar to the pair plot we saw earlier, we can use sns.jointplot to show the joint\\ndistribution between different datasets, along with the associated marginal distribu‐\\ntions (Figure 4-122):\\nIn[17]: with sns.axes_style(\\'white\\'):\\n            sns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\\'hex\\')\\nFigure 4-122. A joint distribution plot\\n320 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 338}, page_content='The joint plot can even do some automatic kernel density estimation and regression\\n(Figure 4-123):\\nIn[18]: sns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\\'reg\\');\\nFigure 4-123. A joint distribution plot with a regression fit\\nBar plots\\nTime series can be plotted with sns.factorplot. In the following example (visualized\\nin Figure 4-124), we’ll use the Planets data that we first saw in “Aggregation and\\nGrouping” on page 158:\\nIn[19]: planets = sns.load_dataset(\\'planets\\')\\n        planets.head()\\nOut[19]:             method  number  orbital_period   mass  distance  year\\n         0  Radial Velocity       1         269.300   7.10     77.40  2006\\n         1  Radial Velocity       1         874.774   2.21     56.95  2008\\n         2  Radial Velocity       1         763.000   2.60     19.84  2011\\n         3  Radial Velocity       1         326.030  19.40    110.62  2007\\n         4  Radial Velocity       1         516.220  10.50    119.47  2009\\nIn[20]: with sns.axes_style(\\'white\\'):\\n            g = sns.factorplot(\"year\", data=planets, aspect=2,\\n                               kind=\"count\", color=\\'steelblue\\')\\n            g.set_xticklabels(step=5)\\nVisualization with Seaborn \\n| \\n321'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 339}, page_content='Figure 4-124. A histogram as a special case of a factor plot\\nWe can learn more by looking at the method of discovery of each of these planets, as\\nillustrated in Figure 4-125:\\nIn[21]: with sns.axes_style(\\'white\\'):\\n            g = sns.factorplot(\"year\", data=planets, aspect=4.0, kind=\\'count\\',\\n                               hue=\\'method\\', order=range(2001, 2015))\\n            g.set_ylabels(\\'Number of Planets Discovered\\')\\nFigure 4-125. Number of planets discovered by year and type (see the online appendix\\nfor a full-scale figure)\\nFor more information on plotting with Seaborn, see the Seaborn documentation, a\\ntutorial, and the Seaborn gallery.\\nExample: Exploring Marathon Finishing Times\\nHere we’ll look at using Seaborn to help visualize and understand finishing results\\nfrom a marathon. I’ve scraped the data from sources on the Web, aggregated it and\\nremoved any identifying information, and put it on GitHub where it can be downloa‐\\nded (if you are interested in using Python for web scraping, I would recommend Web\\nScraping with Python by Ryan Mitchell). We will start by downloading the data from\\nthe Web, and loading it into Pandas:\\n322 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 340}, page_content=\"In[22]:\\n# !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/\\n# master/marathon-data.csv\\nIn[23]: data = pd.read_csv('marathon-data.csv')\\n        data.head()\\nOut[23]:    age gender     split     final\\n         0   33      M  01:05:38  02:08:51\\n         1   32      M  01:06:26  02:09:28\\n         2   31      M  01:06:49  02:10:42\\n         3   38      M  01:06:16  02:13:45\\n         4   31      M  01:06:32  02:13:59\\nBy default, Pandas loaded the time columns as Python strings (type object); we can\\nsee this by looking at the dtypes attribute of the DataFrame:\\nIn[24]: data.dtypes\\nOut[24]: age        int64\\n         gender    object\\n         split     object\\n         final     object\\n         dtype: object\\nLet’s fix this by providing a converter for the times:\\nIn[25]: def convert_time(s):\\n            h, m, s = map(int, s.split(':'))\\n            return pd.datetools.timedelta(hours=h, minutes=m, seconds=s)\\n        data = pd.read_csv('marathon-data.csv',\\n                           converters={'split':convert_time, 'final':convert_time})\\n        data.head()\\nOut[25]:    age gender    split    final\\n         0   33      M 01:05:38 02:08:51\\n         1   32      M 01:06:26 02:09:28\\n         2   31      M 01:06:49 02:10:42\\n         3   38      M 01:06:16 02:13:45\\n         4   31      M 01:06:32 02:13:59\\nIn[26]: data.dtypes\\nOut[26]: age                 int64\\n         gender             object\\n         split     timedelta64[ns]\\n         final     timedelta64[ns]\\n         dtype: object\\nThat looks much better. For the purpose of our Seaborn plotting utilities, let’s next\\nadd columns that give the times in seconds:\\nIn[27]: data['split_sec'] = data['split'].astype(int) / 1E9\\n        data['final_sec'] = data['final'].astype(int) / 1E9\\n        data.head()\\nVisualization with Seaborn \\n| \\n323\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 341}, page_content='Out[27]:    age gender    split    final  split_sec  final_sec\\n         0   33      M 01:05:38 02:08:51     3938.0     7731.0\\n         1   32      M 01:06:26 02:09:28     3986.0     7768.0\\n         2   31      M 01:06:49 02:10:42     4009.0     7842.0\\n         3   38      M 01:06:16 02:13:45     3976.0     8025.0\\n         4   31      M 01:06:32 02:13:59     3992.0     8039.0\\nTo get an idea of what the data looks like, we can plot a jointplot over the data\\n(Figure 4-126):\\nIn[28]: with sns.axes_style(\\'white\\'):\\n            g = sns.jointplot(\"split_sec\", \"final_sec\", data, kind=\\'hex\\')\\n            g.ax_joint.plot(np.linspace(4000, 16000),\\n                            np.linspace(8000, 32000), \\':k\\')\\nFigure 4-126. The relationship between the split for the first half-marathon and the fin‐\\nishing time for the full marathon\\nThe dotted line shows where someone’s time would lie if they ran the marathon at a\\nperfectly steady pace. The fact that the distribution lies above this indicates (as you\\nmight expect) that most people slow down over the course of the marathon. If you\\nhave run competitively, you’ll know that those who do the opposite—run faster dur‐\\ning the second half of the race—are said to have “negative-split” the race.\\nLet’s create another column in the data, the split fraction, which measures the degree\\nto which each runner negative-splits or positive-splits the race:\\nIn[29]: data[\\'split_frac\\'] = 1 - 2 * data[\\'split_sec\\'] / data[\\'final_sec\\']\\n        data.head()\\n324 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 342}, page_content='Out[29]:    age gender    split    final  split_sec  final_sec  split_frac\\n         0   33      M 01:05:38 02:08:51     3938.0     7731.0   -0.018756\\n         1   32      M 01:06:26 02:09:28     3986.0     7768.0   -0.026262\\n         2   31      M 01:06:49 02:10:42     4009.0     7842.0   -0.022443\\n         3   38      M 01:06:16 02:13:45     3976.0     8025.0    0.009097\\n         4   31      M 01:06:32 02:13:59     3992.0     8039.0    0.006842\\nWhere this split difference is less than zero, the person negative-split the race by that\\nfraction. Let’s do a distribution plot of this split fraction (Figure 4-127):\\nIn[30]: sns.distplot(data[\\'split_frac\\'], kde=False);\\n        plt.axvline(0, color=\"k\", linestyle=\"--\");\\nFigure 4-127. The distribution of split fractions; 0.0 indicates a runner who completed\\nthe first and second halves in identical times\\nIn[31]: sum(data.split_frac < 0)\\nOut[31]: 251\\nOut of nearly 40,000 participants, there were only 250 people who negative-split their\\nmarathon.\\nLet’s see whether there is any correlation between this split fraction and other vari‐\\nables. We’ll do this using a pairgrid, which draws plots of all these correlations\\n(Figure 4-128):\\nIn[32]:\\ng = sns.PairGrid(data, vars=[\\'age\\', \\'split_sec\\', \\'final_sec\\', \\'split_frac\\'],\\n                 hue=\\'gender\\', palette=\\'RdBu_r\\')\\ng.map(plt.scatter, alpha=0.8)\\ng.add_legend();\\nVisualization with Seaborn \\n| \\n325'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 343}, page_content=\"Figure 4-128. The relationship between quantities within the marathon dataset\\nIt looks like the split fraction does not correlate particularly with age, but does corre‐\\nlate with the final time: faster runners tend to have closer to even splits on their mara‐\\nthon time. (We see here that Seaborn is no panacea for Matplotlib’s ills when it comes\\nto plot styles: in particular, the x-axis labels overlap. Because the output is a simple\\nMatplotlib plot, however, the methods in “Customizing Ticks” on page 275 can be\\nused to adjust such things if desired.)\\nThe difference between men and women here is interesting. Let’s look at the histo‐\\ngram of split fractions for these two groups (Figure 4-129):\\nIn[33]: sns.kdeplot(data.split_frac[data.gender=='M'], label='men', shade=True)\\n        sns.kdeplot(data.split_frac[data.gender=='W'], label='women', shade=True)\\n        plt.xlabel('split_frac');\\n326 \\n| \\nChapter 4: Visualization with Matplotlib\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 344}, page_content='Figure 4-129. The distribution of split fractions by gender\\nThe interesting thing here is that there are many more men than women who are\\nrunning close to an even split! This almost looks like some kind of bimodal distribu‐\\ntion among the men and women. Let’s see if we can suss out what’s going on by look‐\\ning at the distributions as a function of age.\\nA nice way to compare distributions is to use a violin plot (Figure 4-130):\\nIn[34]:\\nsns.violinplot(\"gender\", \"split_frac\", data=data,\\n               palette=[\"lightblue\", \"lightpink\"]);\\nFigure 4-130. A violin plot showing the split fraction by gender\\nVisualization with Seaborn \\n| \\n327'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 345}, page_content='This is yet another way to compare the distributions between men and women.\\nLet’s look a little deeper, and compare these violin plots as a function of age. We’ll\\nstart by creating a new column in the array that specifies the decade of age that each\\nperson is in (Figure 4-131):\\nIn[35]: data[\\'age_dec\\'] = data.age.map(lambda age: 10 * (age // 10))\\ndata.head()\\nOut[35]:\\n   age gender    split    final  split_sec  final_sec  split_frac  age_dec\\n0   33      M 01:05:38 02:08:51     3938.0     7731.0   -0.018756       30\\n1   32      M 01:06:26 02:09:28     3986.0     7768.0   -0.026262       30\\n2   31      M 01:06:49 02:10:42     4009.0     7842.0   -0.022443       30\\n3   38      M 01:06:16 02:13:45     3976.0     8025.0    0.009097       30\\n4   31      M 01:06:32 02:13:59     3992.0     8039.0    0.006842       30\\nIn[36]:\\nmen = (data.gender == \\'M\\')\\nwomen = (data.gender == \\'W\\')\\nwith sns.axes_style(style=None):\\n    sns.violinplot(\"age_dec\", \"split_frac\", hue=\"gender\", data=data,\\n                  split=True, inner=\"quartile\",\\n                  palette=[\"lightblue\", \"lightpink\"]);\\nFigure 4-131. A violin plot showing the split fraction by gender and age\\nLooking at this, we can see where the distributions of men and women differ: the split\\ndistributions of men in their 20s to 50s show a pronounced over-density toward\\nlower splits when compared to women of the same age (or of any age, for that\\nmatter).\\n328 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 346}, page_content='Also surprisingly, the 80-year-old women seem to outperform everyone in terms of\\ntheir split time. This is probably due to the fact that we’re estimating the distribution\\nfrom small numbers, as there are only a handful of runners in that range:\\nIn[38]: (data.age > 80).sum()\\nOut[38]: 7\\nBack to the men with negative splits: who are these runners? Does this split fraction\\ncorrelate with finishing quickly? We can plot this very easily. We’ll use regplot,\\nwhich will automatically fit a linear regression to the data (Figure 4-132):\\nIn[37]: g = sns.lmplot(\\'final_sec\\', \\'split_frac\\', col=\\'gender\\', data=data,\\n                       markers=\".\", scatter_kws=dict(color=\\'c\\'))\\n        g.map(plt.axhline, y=0.1, color=\"k\", ls=\":\");\\nFigure 4-132. Split fraction versus finishing time by gender\\nApparently the people with fast splits are the elite runners who are finishing within\\n~15,000 seconds, or about 4 hours. People slower than that are much less likely to\\nhave a fast second split.\\nFurther Resources\\nMatplotlib Resources\\nA single chapter in a book can never hope to cover all the available features and plot\\ntypes available in Matplotlib. As with other packages we’ve seen, liberal use of IPy‐\\nthon’s tab-completion and help functions (see “Help and Documentation in IPython”\\non page 3) can be very helpful when you’re exploring Matplotlib’s API. In addition,\\nMatplotlib’s online documentation can be a helpful reference. See in particular the\\nFurther Resources \\n| \\n329'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 347}, page_content='Matplotlib gallery linked on that page: it shows thumbnails of hundreds of different\\nplot types, each one linked to a page with the Python code snippet used to generate it.\\nIn this way, you can visually inspect and learn about a wide range of different plotting\\nstyles and visualization techniques.\\nFor a book-length treatment of Matplotlib, I would recommend Interactive Applica‐\\ntions Using Matplotlib, written by Matplotlib core developer Ben Root.\\nOther Python Graphics Libraries\\nAlthough Matplotlib is the most prominent Python visualization library, there are\\nother more modern tools that are worth exploring as well. I’ll mention a few of them\\nbriefly here:\\n• Bokeh is a JavaScript visualization library with a Python frontend that creates\\nhighly interactive visualizations capable of handling very large and/or streaming\\ndatasets. The Python frontend outputs a JSON data structure that can be inter‐\\npreted by the Bokeh JS engine.\\n• Plotly is the eponymous open source product of the Plotly company, and is simi‐\\nlar in spirit to Bokeh. Because Plotly is the main product of a startup, it is receiv‐\\ning a high level of development effort. Use of the library is entirely free.\\n• Vispy is an actively developed project focused on dynamic visualizations of very\\nlarge datasets. Because it is built to target OpenGL and make use of efficient\\ngraphics processors in your computer, it is able to render some quite large and\\nstunning visualizations.\\n• Vega and Vega-Lite are declarative graphics representations, and are the product\\nof years of research into the fundamental language of data visualization. The ref‐\\nerence rendering implementation is JavaScript, but the API is language agnostic.\\nThere is a Python API under development in the Altair package. Though it’s not\\nmature yet, I’m quite excited for the possibilities of this project to provide a com‐\\nmon reference point for visualization in Python and other languages.\\nThe visualization space in the Python community is very dynamic, and I fully expect\\nthis list to be out of date as soon as it is published. Keep an eye out for what’s coming\\nin the future!\\n330 \\n| \\nChapter 4: Visualization with Matplotlib'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 348}, page_content='CHAPTER 5\\nMachine Learning\\nIn many ways, machine learning is the primary means by which data science mani‐\\nfests itself to the broader world. Machine learning is where these computational and\\nalgorithmic skills of data science meet the statistical thinking of data science, and the\\nresult is a collection of approaches to inference and data exploration that are not\\nabout effective theory so much as effective computation.\\nThe term “machine learning” is sometimes thrown around as if it is some kind of\\nmagic pill: apply machine learning to your data, and all your problems will be solved!\\nAs you might expect, the reality is rarely this simple. While these methods can be\\nincredibly powerful, to be effective they must be approached with a firm grasp of the\\nstrengths and weaknesses of each method, as well as a grasp of general concepts such\\nas bias and variance, overfitting and underfitting, and more.\\nThis chapter will dive into practical aspects of machine learning, primarily using\\nPython’s Scikit-Learn package. This is not meant to be a comprehensive introduction\\nto the field of machine learning; that is a large subject and necessitates a more techni‐\\ncal approach than we take here. Nor is it meant to be a comprehensive manual for the\\nuse of the Scikit-Learn package (for this, see “Further Machine Learning Resources”\\non page 514). Rather, the goals of this chapter are:\\n• To introduce the fundamental vocabulary and concepts of machine learning.\\n• To introduce the Scikit-Learn API and show some examples of its use.\\n• To take a deeper dive into the details of several of the most important machine\\nlearning approaches, and develop an intuition into how they work and when and\\nwhere they are applicable.\\nMuch of this material is drawn from the Scikit-Learn tutorials and workshops I have\\ngiven on several occasions at PyCon, SciPy, PyData, and other conferences. Any\\n331'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 349}, page_content='clarity in the following pages is likely due to the many workshop participants and co-\\ninstructors who have given me valuable feedback on this material over the years!\\nFinally, if you are seeking a more comprehensive or technical treatment of any of\\nthese subjects, I’ve listed several resources and references in “Further Machine Learn‐\\ning Resources” on page 514.\\nWhat Is Machine Learning?\\nBefore we take a look at the details of various machine learning methods, let’s start by\\nlooking at what machine learning is, and what it isn’t. Machine learning is often cate‐\\ngorized as a subfield of artificial intelligence, but I find that categorization can often\\nbe misleading at first brush. The study of machine learning certainly arose from\\nresearch in this context, but in the data science application of machine learning meth‐\\nods, it’s more helpful to think of machine learning as a means of building models of\\ndata.\\nFundamentally, machine learning involves building mathematical models to help\\nunderstand data. “Learning” enters the fray when we give these models tunable\\nparameters that can be adapted to observed data; in this way the program can be con‐\\nsidered to be “learning” from the data. Once these models have been fit to previously\\nseen data, they can be used to predict and understand aspects of newly observed data.\\nI’ll leave to the reader the more philosophical digression regarding the extent to\\nwhich this type of mathematical, model-based “learning” is similar to the “learning”\\nexhibited by the human brain.\\nUnderstanding the problem setting in machine learning is essential to using these\\ntools effectively, and so we will start with some broad categorizations of the types of\\napproaches we’ll discuss here.\\nCategories of Machine Learning\\nAt the most fundamental level, machine learning can be categorized into two main\\ntypes: supervised learning and unsupervised learning.\\nSupervised learning involves somehow modeling the relationship between measured\\nfeatures of data and some label associated with the data; once this model is deter‐\\nmined, it can be used to apply labels to new, unknown data. This is further subdivi‐\\nded into classification tasks and regression tasks: in classification, the labels are\\ndiscrete categories, while in regression, the labels are continuous quantities. We will\\nsee examples of both types of supervised learning in the following section.\\nUnsupervised learning involves modeling the features of a dataset without reference to\\nany label, and is often described as “letting the dataset speak for itself.” These models\\ninclude tasks such as clustering and dimensionality reduction. Clustering algorithms\\n332 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 350}, page_content='identify distinct groups of data, while dimensionality reduction algorithms search for\\nmore succinct representations of the data. We will see examples of both types of\\nunsupervised learning in the following section.\\nIn addition, there are so-called semi-supervised learning methods, which fall some‐\\nwhere between supervised learning and unsupervised learning. Semi-supervised\\nlearning methods are often useful when only incomplete labels are available.\\nQualitative Examples of Machine Learning Applications\\nTo make these ideas more concrete, let’s take a look at a few very simple examples of a\\nmachine learning task. These examples are meant to give an intuitive, non-\\nquantitative overview of the types of machine learning tasks we will be looking at in\\nthis chapter. In later sections, we will go into more depth regarding the particular\\nmodels and how they are used. For a preview of these more technical aspects, you can\\nfind the Python source that generates the figures in the online appendix.\\nClassification: Predicting discrete labels\\nWe will first take a look at a simple classification task, in which you are given a set of\\nlabeled points and want to use these to classify some unlabeled points.\\nImagine that we have the data shown in Figure 5-1 (the code used to generate this\\nfigure, and all figures in this section, is available in the online appendix).\\nFigure 5-1. A simple data set for classification\\nWhat Is Machine Learning? \\n| \\n333'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 351}, page_content='Here we have two-dimensional data; that is, we have two features for each point, rep‐\\nresented by the (x,y) positions of the points on the plane. In addition, we have one of\\ntwo class labels for each point, here represented by the colors of the points. From\\nthese features and labels, we would like to create a model that will let us decide\\nwhether a new point should be labeled “blue” or “red.”\\nThere are a number of possible models for such a classification task, but here we will\\nuse an extremely simple one. We will make the assumption that the two groups can\\nbe separated by drawing a straight line through the plane between them, such that\\npoints on each side of the line fall in the same group. Here the model is a quantitative\\nversion of the statement “a straight line separates the classes,” while the model param‐\\neters are the particular numbers describing the location and orientation of that line\\nfor our data. The optimal values for these model parameters are learned from the\\ndata (this is the “learning” in machine learning), which is often called training the\\nmodel.\\nFigure 5-2 is a visual representation of what the trained model looks like for this data.\\nFigure 5-2. A simple classification model\\nNow that this model has been trained, it can be generalized to new, unlabeled data. In\\nother words, we can take a new set of data, draw this model line through it, and\\nassign labels to the new points based on this model. This stage is usually called predic‐\\ntion. See Figure 5-3.\\n334 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 352}, page_content='Figure 5-3. Applying a classification model to new data\\nThis is the basic idea of a classification task in machine learning, where “classifica‐\\ntion” indicates that the data has discrete class labels. At first glance this may look\\nfairly trivial: it would be relatively easy to simply look at this data and draw such a\\ndiscriminatory line to accomplish this classification. A benefit of the machine learn‐\\ning approach, however, is that it can generalize to much larger datasets in many more\\ndimensions.\\nFor example, this is similar to the task of automated spam detection for email; in this\\ncase, we might use the following features and labels:\\n• feature 1, feature 2, etc. \\n normalized counts of important words or phrases\\n(“Viagra,” “Nigerian prince,” etc.)\\n• label \\n “spam” or “not spam”\\nFor the training set, these labels might be determined by individual inspection of a\\nsmall representative sample of emails; for the remaining emails, the label would be\\ndetermined using the model. For a suitably trained classification algorithm with\\nenough well-constructed features (typically thousands or millions of words or\\nphrases), this type of approach can be very effective. We will see an example of such\\ntext-based classification in “In Depth: Naive Bayes Classification” on page 382.\\nSome important classification algorithms that we will discuss in more detail are Gaus‐\\nsian naive Bayes (see “In Depth: Naive Bayes Classification” on page 382), support\\nvector machines (see “In-Depth: Support Vector Machines” on page 405), and ran‐\\ndom forest classification (see “In-Depth: Decision Trees and Random Forests” on\\npage 421).\\nRegression: Predicting continuous labels\\nIn contrast with the discrete labels of a classification algorithm, we will next look at a\\nsimple regression task in which the labels are continuous quantities.\\nWhat Is Machine Learning? \\n| \\n335'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 353}, page_content='Consider the data shown in Figure 5-4, which consists of a set of points, each with a\\ncontinuous label.\\nFigure 5-4. A simple dataset for regression\\nAs with the classification example, we have two-dimensional data; that is, there are\\ntwo features describing each data point. The color of each point represents the con‐\\ntinuous label for that point.\\nThere are a number of possible regression models we might use for this type of data,\\nbut here we will use a simple linear regression to predict the points. This simple linear\\nregression model assumes that if we treat the label as a third spatial dimension, we\\ncan fit a plane to the data. This is a higher-level generalization of the well-known\\nproblem of fitting a line to data with two coordinates.\\nWe can visualize this setup as shown in Figure 5-5.\\n336 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 354}, page_content='Figure 5-5. A three-dimensional view of the regression data\\nNotice that the feature 1–feature 2 plane here is the same as in the two-dimensional\\nplot from before; in this case, however, we have represented the labels by both color\\nand three-dimensional axis position. From this view, it seems reasonable that fitting a\\nplane through this three-dimensional data would allow us to predict the expected\\nlabel for any set of input parameters. Returning to the two-dimensional projection,\\nwhen we fit such a plane we get the result shown in Figure 5-6.\\nFigure 5-6. A representation of the regression model\\nWhat Is Machine Learning? \\n| \\n337'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 355}, page_content='This plane of fit gives us what we need to predict labels for new points. Visually, we\\nfind the results shown in Figure 5-7.\\nFigure 5-7. Applying the regression model to new data\\nAs with the classification example, this may seem rather trivial in a low number of\\ndimensions. But the power of these methods is that they can be straightforwardly\\napplied and evaluated in the case of data with many, many features.\\nFor example, this is similar to the task of computing the distance to galaxies observed\\nthrough a telescope—in this case, we might use the following features and labels:\\n• feature 1, feature 2, etc. \\n brightness of each galaxy at one of several wavelengths\\nor colors\\n• label \\n distance or redshift of the galaxy\\nThe distances for a small number of these galaxies might be determined through an\\nindependent set of (typically more expensive) observations. We could then estimate\\ndistances to remaining galaxies using a suitable regression model, without the need to\\nemploy the more expensive observation across the entire set. In astronomy circles,\\nthis is known as the “photometric redshift” problem.\\nSome important regression algorithms that we will discuss are linear regression (see\\n“In Depth: Linear Regression” on page 390), support vector machines (see “In-Depth:\\nSupport Vector Machines” on page 405), and random forest regression (see “In-\\nDepth: Decision Trees and Random Forests” on page 421).\\nClustering: Inferring labels on unlabeled data\\nThe classification and regression illustrations we just looked at are examples of super‐\\nvised learning algorithms, in which we are trying to build a model that will predict\\nlabels for new data. Unsupervised learning involves models that describe data without\\nreference to any known labels.\\n338 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 356}, page_content='One common case of unsupervised learning is “clustering,” in which data is automati‐\\ncally assigned to some number of discrete groups. For example, we might have some\\ntwo-dimensional data like that shown in Figure 5-8.\\nFigure 5-8. Example data for clustering\\nBy eye, it is clear that each of these points is part of a distinct group. Given this input,\\na clustering model will use the intrinsic structure of the data to determine which\\npoints are related. Using the very fast and intuitive k-means algorithm (see “In Depth:\\nk-Means Clustering” on page 462), we find the clusters shown in Figure 5-9.\\nk-means fits a model consisting of k cluster centers; the optimal centers are assumed\\nto be those that minimize the distance of each point from its assigned center. Again,\\nthis might seem like a trivial exercise in two dimensions, but as our data becomes\\nlarger and more complex, such clustering algorithms can be employed to extract use‐\\nful information from the dataset.\\nWe will discuss the k-means algorithm in more depth in “In Depth: k-Means Cluster‐\\ning” on page 462. Other important clustering algorithms include Gaussian mixture\\nmodels (see “In Depth: Gaussian Mixture Models” on page 476) and spectral cluster‐\\ning (see Scikit-Learn’s clustering documentation).\\nWhat Is Machine Learning? \\n| \\n339'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 357}, page_content='Figure 5-9. Data labeled with a k-means clustering model\\nDimensionality reduction: Inferring structure of unlabeled data\\nDimensionality reduction is another example of an unsupervised algorithm, in which\\nlabels or other information are inferred from the structure of the dataset itself.\\nDimensionality reduction is a bit more abstract than the examples we looked at\\nbefore, but generally it seeks to pull out some low-dimensional representation of data\\nthat in some way preserves relevant qualities of the full dataset. Different dimension‐\\nality reduction routines measure these relevant qualities in different ways, as we will\\nsee in “In-Depth: Manifold Learning” on page 445.\\nAs an example of this, consider the data shown in Figure 5-10.\\nVisually, it is clear that there is some structure in this data: it is drawn from a one-\\ndimensional line that is arranged in a spiral within this two-dimensional space. In a\\nsense, you could say that this data is “intrinsically” only one dimensional, though this\\none-dimensional data is embedded in higher-dimensional space. A suitable dimen‐\\nsionality reduction model in this case would be sensitive to this nonlinear embedded\\nstructure, and be able to pull out this lower-dimensionality representation.\\n340 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 358}, page_content='Figure 5-10. Example data for dimensionality reduction\\nFigure 5-11 presents a visualization of the results of the Isomap algorithm, a manifold\\nlearning algorithm that does exactly this.\\nFigure 5-11. Data with a label learned via dimensionality reduction\\nNotice that the colors (which represent the extracted one-dimensional latent variable)\\nchange uniformly along the spiral, which indicates that the algorithm did in fact\\ndetect the structure we saw by eye. As with the previous examples, the power of\\nWhat Is Machine Learning? \\n| \\n341'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 359}, page_content='dimensionality reduction algorithms becomes clearer in higher-dimensional cases.\\nFor example, we might wish to visualize important relationships within a dataset that\\nhas 100 or 1,000 features. Visualizing 1,000-dimensional data is a challenge, and one\\nway we can make this more manageable is to use a dimensionality reduction techni‐\\nque to reduce the data to two or three dimensions.\\nSome important dimensionality reduction algorithms that we will discuss are princi‐\\npal component analysis (see “In Depth: Principal Component Analysis” on page 433)\\nand various manifold learning algorithms, including Isomap and locally linear\\nembedding (see “In-Depth: Manifold Learning” on page 445).\\nSummary\\nHere we have seen a few simple examples of some of the basic types of machine learn‐\\ning approaches. Needless to say, there are a number of important practical details that\\nwe have glossed over, but I hope this section was enough to give you a basic idea of\\nwhat types of problems machine learning approaches can solve.\\nIn short, we saw the following:\\nSupervised learning\\nModels that can predict labels based on labeled training data\\nClassification\\nModels that predict labels as two or more discrete categories\\nRegression\\nModels that predict continuous labels\\nUnsupervised learning\\nModels that identify structure in unlabeled data\\nClustering\\nModels that detect and identify distinct groups in the data\\nDimensionality reduction\\nModels that detect and identify lower-dimensional structure in higher-\\ndimensional data\\nIn the following sections we will go into much greater depth within these categories,\\nand see some more interesting examples of where these concepts can be useful.\\nAll of the figures in the preceding discussion are generated based on actual machine\\nlearning computations; the code behind them can be found in the online appendix.\\n342 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 360}, page_content=\"Introducing Scikit-Learn\\nThere are several Python libraries that provide solid implementations of a range of\\nmachine learning algorithms. One of the best known is Scikit-Learn, a package that\\nprovides efficient versions of a large number of common algorithms. Scikit-Learn is\\ncharacterized by a clean, uniform, and streamlined API, as well as by very useful and\\ncomplete online documentation. A benefit of this uniformity is that once you under‐\\nstand the basic use and syntax of Scikit-Learn for one type of model, switching to a\\nnew model or algorithm is very straightforward.\\nThis section provides an overview of the Scikit-Learn API; a solid understanding of\\nthese API elements will form the foundation for understanding the deeper practical\\ndiscussion of machine learning algorithms and approaches in the following chapters.\\nWe will start by covering data representation in Scikit-Learn, followed by covering the\\nEstimator API, and finally go through a more interesting example of using these tools\\nfor exploring a set of images of handwritten digits.\\nData Representation in Scikit-Learn\\nMachine learning is about creating models from data: for that reason, we’ll start by\\ndiscussing how data can be represented in order to be understood by the computer.\\nThe best way to think about data within Scikit-Learn is in terms of tables of data.\\nData as table\\nA basic table is a two-dimensional grid of data, in which the rows represent individ‐\\nual elements of the dataset, and the columns represent quantities related to each of\\nthese elements. For example, consider the Iris dataset, famously analyzed by Ronald\\nFisher in 1936. We can download this dataset in the form of a Pandas DataFrame\\nusing the Seaborn library:\\nIn[1]: import seaborn as sns\\n       iris = sns.load_dataset('iris')\\n       iris.head()\\nOut[1]:    sepal_length  sepal_width  petal_length  petal_width species\\n        0           5.1          3.5           1.4          0.2  setosa\\n        1           4.9          3.0           1.4          0.2  setosa\\n        2           4.7          3.2           1.3          0.2  setosa\\n        3           4.6          3.1           1.5          0.2  setosa\\n        4           5.0          3.6           1.4          0.2  setosa\\nHere each row of the data refers to a single observed flower, and the number of rows\\nis the total number of flowers in the dataset. In general, we will refer to the rows of\\nthe matrix as samples, and the number of rows as n_samples.\\nIntroducing Scikit-Learn \\n| \\n343\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 361}, page_content='Likewise, each column of the data refers to a particular quantitative piece of informa‐\\ntion that describes each sample. In general, we will refer to the columns of the matrix\\nas features, and the number of columns as n_features.\\nFeatures matrix\\nThis table layout makes clear that the information can be thought of as a two-\\ndimensional numerical array or matrix, which we will call the features matrix. By con‐\\nvention, this features matrix is often stored in a variable named X. The features matrix\\nis assumed to be two-dimensional, with shape [n_samples, n_features], and is\\nmost often contained in a NumPy array or a Pandas DataFrame, though some Scikit-\\nLearn models also accept SciPy sparse matrices.\\nThe samples (i.e., rows) always refer to the individual objects described by the dataset.\\nFor example, the sample might be a flower, a person, a document, an image, a sound\\nfile, a video, an astronomical object, or anything else you can describe with a set of\\nquantitative measurements.\\nThe features (i.e., columns) always refer to the distinct observations that describe\\neach sample in a quantitative manner. Features are generally real-valued, but may be\\nBoolean or discrete-valued in some cases.\\nTarget array\\nIn addition to the feature matrix X, we also generally work with a label or target array,\\nwhich by convention we will usually call y. The target array is usually one dimen‐\\nsional, with length n_samples, and is generally contained in a NumPy array or Pan‐\\ndas Series. The target array may have continuous numerical values, or discrete\\nclasses/labels. While some Scikit-Learn estimators do handle multiple target values in\\nthe form of a two-dimensional [n_samples, n_targets] target array, we will pri‐\\nmarily be working with the common case of a one-dimensional target array.\\nOften one point of confusion is how the target array differs from the other features\\ncolumns. The distinguishing feature of the target array is that it is usually the quantity\\nwe want to predict from the data: in statistical terms, it is the dependent variable. For\\nexample, in the preceding data we may wish to construct a model that can predict the\\nspecies of flower based on the other measurements; in this case, the species column\\nwould be considered the feature.\\n344 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 362}, page_content=\"With this target array in mind, we can use Seaborn (discussed earlier in “Visualiza‐\\ntion with Seaborn” on page 311) to conveniently visualize the data (see Figure 5-12):\\nIn[2]: %matplotlib inline\\n       import seaborn as sns; sns.set()\\n       sns.pairplot(iris, hue='species', size=1.5);\\nFigure 5-12. A visualization of the Iris dataset\\nFor use in Scikit-Learn, we will extract the features matrix and target array from the\\nDataFrame, which we can do using some of the Pandas DataFrame operations dis‐\\ncussed in Chapter 3:\\nIn[3]: X_iris = iris.drop('species', axis=1)\\n       X_iris.shape\\nOut[3]: (150, 4)\\nIn[4]: y_iris = iris['species']\\n       y_iris.shape\\nOut[4]: (150,)\\nTo summarize, the expected layout of features and target values is visualized in\\nFigure 5-13.\\nIntroducing Scikit-Learn \\n| \\n345\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 363}, page_content='Figure 5-13. Scikit-Learn’s data layout\\nWith this data properly formatted, we can move on to consider the estimator API of\\nScikit-Learn.\\nScikit-Learn’s Estimator API\\nThe Scikit-Learn API is designed with the following guiding principles in mind, as\\noutlined in the Scikit-Learn API paper:\\nConsistency\\nAll objects share a common interface drawn from a limited set of methods, with\\nconsistent documentation.\\nInspection\\nAll specified parameter values are exposed as public attributes.\\nLimited object hierarchy\\nOnly algorithms are represented by Python classes; datasets are represented in\\nstandard formats (NumPy arrays, Pandas DataFrames, SciPy sparse matrices) and\\nparameter names use standard Python strings.\\nComposition\\nMany machine learning tasks can be expressed as sequences of more fundamen‐\\ntal algorithms, and Scikit-Learn makes use of this wherever possible.\\nSensible defaults\\nWhen models require user-specified parameters, the library defines an appropri‐\\nate default value.\\n346 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 364}, page_content='In practice, these principles make Scikit-Learn very easy to use, once the basic princi‐\\nples are understood. Every machine learning algorithm in Scikit-Learn is imple‐\\nmented via the Estimator API, which provides a consistent interface for a wide range\\nof machine learning applications.\\nBasics of the API\\nMost commonly, the steps in using the Scikit-Learn estimator API are as follows (we\\nwill step through a handful of detailed examples in the sections that follow):\\n1. Choose a class of model by importing the appropriate estimator class from Scikit-\\nLearn.\\n2. Choose model hyperparameters by instantiating this class with desired values.\\n3. Arrange data into a features matrix and target vector following the discussion\\nfrom before.\\n4. Fit the model to your data by calling the fit() method of the model instance.\\n5. Apply the model to new data:\\n• For supervised learning, often we predict labels for unknown data using the\\npredict() method.\\n• For unsupervised learning, we often transform or infer properties of the data\\nusing the transform() or predict() method.\\nWe will now step through several simple examples of applying supervised and unsu‐\\npervised learning methods.\\nSupervised learning example: Simple linear regression\\nAs an example of this process, let’s consider a simple linear regression—that is, the\\ncommon case of fitting a line to x, y  data. We will use the following simple data for\\nour regression example (Figure 5-14):\\nIn[5]: import matplotlib.pyplot as plt\\n       import numpy as np\\n       rng = np.random.RandomState(42)\\n       x = 10 * rng.rand(50)\\n       y = 2 * x - 1 + rng.randn(50)\\n       plt.scatter(x, y);\\nIntroducing Scikit-Learn \\n| \\n347'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 365}, page_content='Figure 5-14. Data for linear regression\\nWith this data in place, we can use the recipe outlined earlier. Let’s walk through the\\nprocess:\\n1. Choose a class of model.\\nIn Scikit-Learn, every class of model is represented by a Python class. So, for\\nexample, if we would like to compute a simple linear regression model, we can\\nimport the linear regression class:\\nIn[6]: from sklearn.linear_model import LinearRegression\\nNote that other, more general linear regression models exist as well; you can read\\nmore about them in the sklearn.linear_model module documentation.\\n2. Choose model hyperparameters.\\nAn important point is that a class of model is not the same as an instance of a\\nmodel.\\nOnce we have decided on our model class, there are still some options open to us.\\nDepending on the model class we are working with, we might need to answer\\none or more questions like the following:\\n• Would we like to fit for the offset (i.e., intercept)?\\n• Would we like the model to be normalized?\\n• Would we like to preprocess our features to add model flexibility?\\n• What degree of regularization would we like to use in our model?\\n• How many model components would we like to use?\\n348 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 366}, page_content='These are examples of the important choices that must be made once the model\\nclass is selected. These choices are often represented as hyperparameters, or\\nparameters that must be set before the model is fit to data. In Scikit-Learn, we\\nchoose hyperparameters by passing values at model instantiation. We will\\nexplore how you can quantitatively motivate the choice of hyperparameters in\\n“Hyperparameters and Model Validation” on page 359.\\nFor our linear regression example, we can instantiate the LinearRegression\\nclass and specify that we would like to fit the intercept using the fit_inter\\ncept hyperparameter:\\nIn[7]: model = LinearRegression(fit_intercept=True)\\n       model\\nOut[7]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,\\n                         normalize=False)\\nKeep in mind that when the model is instantiated, the only action is the storing\\nof these hyperparameter values. In particular, we have not yet applied the\\nmodel to any data: the Scikit-Learn API makes very clear the distinction\\nbetween choice of model and application of model to data.\\n3. Arrange data into a features matrix and target vector.\\nPreviously we detailed the Scikit-Learn data representation, which requires a\\ntwo-dimensional features matrix and a one-dimensional target array. Here our\\ntarget variable y is already in the correct form (a length-n_samples array), but we\\nneed to massage the data x to make it a matrix of size [n_samples, n_features].\\nIn this case, this amounts to a simple reshaping of the one-dimensional array:\\nIn[8]: X = x[:, np.newaxis]\\n       X.shape\\nOut[8]: (50, 1)\\n4. Fit the model to your data.\\nNow it is time to apply our model to data. This can be done with the fit()\\nmethod of the model:\\nIn[9]: model.fit(X, y)\\nOut[9]:\\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,\\n                 normalize=False)\\nThis fit() command causes a number of model-dependent internal computa‐\\ntions to take place, and the results of these computations are stored in model-\\nspecific attributes that the user can explore. In Scikit-Learn, by convention all\\nmodel parameters that were learned during the fit() process have trailing\\nunderscores; for example, in this linear model, we have the following:\\nIntroducing Scikit-Learn \\n| \\n349'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 367}, page_content='In[10]: model.coef_\\nOut[10]: array([ 1.9776566])\\nIn[11]: model.intercept_\\nOut[11]: -0.90331072553111635\\nThese two parameters represent the slope and intercept of the simple linear fit to\\nthe data. Comparing to the data definition, we see that they are very close to the\\ninput slope of 2 and intercept of –1.\\nOne question that frequently comes up regards the uncertainty in such internal\\nmodel parameters. In general, Scikit-Learn does not provide tools to draw con‐\\nclusions from internal model parameters themselves: interpreting model parame‐\\nters is much more a statistical modeling question than a machine learning\\nquestion. Machine learning rather focuses on what the model predicts. If you\\nwould like to dive into the meaning of fit parameters within the model, other\\ntools are available, including the StatsModels Python package.\\n5. Predict labels for unknown data.\\nOnce the model is trained, the main task of supervised machine learning is to\\nevaluate it based on what it says about new data that was not part of the training\\nset. In Scikit-Learn, we can do this using the predict() method. For the sake of\\nthis example, our “new data” will be a grid of x values, and we will ask what y\\nvalues the model predicts:\\nIn[12]: xfit = np.linspace(-1, 11)\\nAs before, we need to coerce these x values into a [n_samples, n_features]\\nfeatures matrix, after which we can feed it to the model:\\nIn[13]: Xfit = xfit[:, np.newaxis]\\n        yfit = model.predict(Xfit)\\nFinally, let’s visualize the results by plotting first the raw data, and then this\\nmodel fit (Figure 5-15):\\nIn[14]: plt.scatter(x, y)\\n        plt.plot(xfit, yfit);\\nTypically one evaluates the efficacy of the model by comparing its results to some\\nknown baseline, as we will see in the next example.\\n350 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 368}, page_content='Figure 5-15. A simple linear regression fit to the data\\nSupervised learning example: Iris classification\\nLet’s take a look at another example of this process, using the Iris dataset we discussed\\nearlier. Our question will be this: given a model trained on a portion of the Iris data,\\nhow well can we predict the remaining labels?\\nFor this task, we will use an extremely simple generative model known as Gaussian\\nnaive Bayes, which proceeds by assuming each class is drawn from an axis-aligned\\nGaussian distribution (see “In Depth: Naive Bayes Classification” on page 382 for\\nmore details). Because it is so fast and has no hyperparameters to choose, Gaussian\\nnaive Bayes is often a good model to use as a baseline classification, before you\\nexplore whether improvements can be found through more sophisticated models.\\nWe would like to evaluate the model on data it has not seen before, and so we will\\nsplit the data into a training set and a testing set. This could be done by hand, but it is\\nmore convenient to use the train_test_split utility function:\\nIn[15]: from sklearn.cross_validation import train_test_split\\n        Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\\n                                                        random_state=1)\\nWith the data arranged, we can follow our recipe to predict the labels:\\nIn[16]: from sklearn.naive_bayes import GaussianNB # 1. choose model class\\n        model = GaussianNB()                       # 2. instantiate model\\n        model.fit(Xtrain, ytrain)                  # 3. fit model to data\\n        y_model = model.predict(Xtest)             # 4. predict on new data\\nFinally, we can use the accuracy_score utility to see the fraction of predicted labels\\nthat match their true value:\\nIntroducing Scikit-Learn \\n| \\n351'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 369}, page_content='In[17]: from sklearn.metrics import accuracy_score\\n        accuracy_score(ytest, y_model)\\nOut[17]: 0.97368421052631582\\nWith an accuracy topping 97%, we see that even this very naive classification algo‐\\nrithm is effective for this particular dataset!\\nUnsupervised learning example: Iris dimensionality\\nAs an example of an unsupervised learning problem, let’s take a look at reducing the\\ndimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data\\nis four dimensional: there are four features recorded for each sample.\\nThe task of dimensionality reduction is to ask whether there is a suitable lower-\\ndimensional representation that retains the essential features of the data. Often\\ndimensionality reduction is used as an aid to visualizing data; after all, it is much eas‐\\nier to plot data in two dimensions than in four dimensions or higher!\\nHere we will use principal component analysis (PCA; see “In Depth: Principal Com‐\\nponent Analysis” on page 433), which is a fast linear dimensionality reduction techni‐\\nque. We will ask the model to return two components—that is, a two-dimensional\\nrepresentation of the data.\\nFollowing the sequence of steps outlined earlier, we have:\\nIn[18]:\\nfrom sklearn.decomposition import PCA  # 1. Choose the model class\\nmodel = PCA(n_components=2)      # 2. Instantiate the model with hyperparameters\\nmodel.fit(X_iris)                # 3. Fit to data. Notice y is not specified!\\nX_2D = model.transform(X_iris)   # 4. Transform the data to two dimensions\\nNow let’s plot the results. A quick way to do this is to insert the results into the origi‐\\nnal Iris DataFrame, and use Seaborn’s lmplot to show the results (Figure 5-16):\\nIn[19]: iris[\\'PCA1\\'] = X_2D[:, 0]\\n        iris[\\'PCA2\\'] = X_2D[:, 1]\\n        sns.lmplot(\"PCA1\", \"PCA2\", hue=\\'species\\', data=iris, fit_reg=False);\\nWe see that in the two-dimensional representation, the species are fairly well separa‐\\nted, even though the PCA algorithm had no knowledge of the species labels! This\\nindicates to us that a relatively straightforward classification will probably be effective\\non the dataset, as we saw before.\\n352 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 370}, page_content='Figure 5-16. The Iris data projected to two dimensions\\nUnsupervised learning: Iris clustering\\nLet’s next look at applying clustering to the Iris data. A clustering algorithm attempts\\nto find distinct groups of data without reference to any labels. Here we will use a\\npowerful clustering method called a Gaussian mixture model (GMM), discussed in\\nmore detail in “In Depth: Gaussian Mixture Models” on page 476. A GMM attempts\\nto model the data as a collection of Gaussian blobs.\\nWe can fit the Gaussian mixture model as follows:\\nIn[20]:\\nfrom sklearn.mixture import GMM      # 1. Choose the model class\\nmodel = GMM(n_components=3,\\n            covariance_type=\\'full\\')  # 2. Instantiate the model w/ hyperparameters\\nmodel.fit(X_iris)                    # 3. Fit to data. Notice y is not specified!\\ny_gmm = model.predict(X_iris)        # 4. Determine cluster labels\\nAs before, we will add the cluster label to the Iris DataFrame and use Seaborn to plot\\nthe results (Figure 5-17):\\nIn[21]:\\niris[\\'cluster\\'] = y_gmm\\nsns.lmplot(\"PCA1\", \"PCA2\", data=iris, hue=\\'species\\',\\n           col=\\'cluster\\', fit_reg=False);\\nBy splitting the data by cluster number, we see exactly how well the GMM algorithm\\nhas recovered the underlying label: the setosa species is separated perfectly within\\ncluster 0, while there remains a small amount of mixing between versicolor and vir‐\\nginica. This means that even without an expert to tell us the species labels of the indi‐\\nvidual flowers, the measurements of these flowers are distinct enough that we could\\nautomatically identify the presence of these different groups of species with a simple\\nIntroducing Scikit-Learn \\n| \\n353'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 371}, page_content=\"clustering algorithm! This sort of algorithm might further give experts in the field\\nclues as to the relationship between the samples they are observing.\\nFigure 5-17. k-means clusters within the Iris data\\nApplication: Exploring Handwritten Digits\\nTo demonstrate these principles on a more interesting problem, let’s consider one\\npiece of the optical character recognition problem: the identification of handwritten\\ndigits. In the wild, this problem involves both locating and identifying characters in\\nan image. Here we’ll take a shortcut and use Scikit-Learn’s set of preformatted digits,\\nwhich is built into the library.\\nLoading and visualizing the digits data\\nWe’ll use Scikit-Learn’s data access interface and take a look at this data:\\nIn[22]: from sklearn.datasets import load_digits\\n        digits = load_digits()\\n        digits.images.shape\\nOut[22]: (1797, 8, 8)\\nThe images data is a three-dimensional array: 1,797 samples, each consisting of an\\n8×8 grid of pixels. Let’s visualize the first hundred of these (Figure 5-18):\\nIn[23]: import matplotlib.pyplot as plt\\n        fig, axes = plt.subplots(10, 10, figsize=(8, 8),\\n                                 subplot_kw={'xticks':[], 'yticks':[]},\\n                                 gridspec_kw=dict(hspace=0.1, wspace=0.1))\\n        for i, ax in enumerate(axes.flat):\\n            ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\\n            ax.text(0.05, 0.05, str(digits.target[i]),\\n                    transform=ax.transAxes, color='green')\\n354 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 372}, page_content='Figure 5-18. The handwritten digits data; each sample is represented by one 8×8 grid of\\npixels\\nIn order to work with this data within Scikit-Learn, we need a two-dimensional,\\n[n_samples, n_features] representation. We can accomplish this by treating each\\npixel in the image as a feature—that is, by flattening out the pixel arrays so that we\\nhave a length-64 array of pixel values representing each digit. Additionally, we need\\nthe target array, which gives the previously determined label for each digit. These two\\nquantities are built into the digits dataset under the data and target attributes,\\nrespectively:\\nIn[24]: X = digits.data\\n        X.shape\\nOut[24]: (1797, 64)\\nIn[25]: y = digits.target\\n        y.shape\\nOut[25]: (1797,)\\nWe see here that there are 1,797 samples and 64 features.\\nUnsupervised learning: Dimensionality reduction\\nWe’d like to visualize our points within the 64-dimensional parameter space, but it’s\\ndifficult to effectively visualize points in such a high-dimensional space. Instead we’ll\\nreduce the dimensions to 2, using an unsupervised method. Here, we’ll make use of a\\nIntroducing Scikit-Learn \\n| \\n355'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 373}, page_content=\"manifold learning algorithm called Isomap (see “In-Depth: Manifold Learning” on\\npage 445), and transform the data to two dimensions:\\nIn[26]: from sklearn.manifold import Isomap\\n        iso = Isomap(n_components=2)\\n        iso.fit(digits.data)\\n        data_projected = iso.transform(digits.data)\\n        data_projected.shape\\nOut[26]: (1797, 2)\\nWe see that the projected data is now two-dimensional. Let’s plot this data to see if we\\ncan learn anything from its structure (Figure 5-19):\\nIn[27]: plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\\n                    edgecolor='none', alpha=0.5,\\n                    cmap=plt.cm.get_cmap('spectral', 10))\\n        plt.colorbar(label='digit label', ticks=range(10))\\n        plt.clim(-0.5, 9.5);\\nFigure 5-19. An Isomap embedding of the digits data\\nThis plot gives us some good intuition into how well various numbers are separated\\nin the larger 64-dimensional space. For example, zeros (in black) and ones (in purple)\\nhave very little overlap in parameter space. Intuitively, this makes sense: a zero is\\nempty in the middle of the image, while a one will generally have ink in the middle.\\nOn the other hand, there seems to be a more or less continuous spectrum between\\nones and fours: we can understand this by realizing that some people draw ones with\\n“hats” on them, which cause them to look similar to fours.\\nOverall, however, the different groups appear to be fairly well separated in the param‐\\neter space: this tells us that even a very straightforward supervised classification algo‐\\nrithm should perform suitably on this data. Let’s give it a try.\\n356 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 374}, page_content=\"Classification on digits\\nLet’s apply a classification algorithm to the digits. As with the Iris data previously, we\\nwill split the data into a training and test set, and fit a Gaussian naive Bayes model:\\nIn[28]: Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\\nIn[29]: from sklearn.naive_bayes import GaussianNB\\n        model = GaussianNB()\\n        model.fit(Xtrain, ytrain)\\n        y_model = model.predict(Xtest)\\nNow that we have predicted our model, we can gauge its accuracy by comparing the\\ntrue values of the test set to the predictions:\\nIn[30]: from sklearn.metrics import accuracy_score\\n        accuracy_score(ytest, y_model)\\nOut[30]: 0.83333333333333337\\nWith even this extremely simple model, we find about 80% accuracy for classification\\nof the digits! However, this single number doesn’t tell us where we’ve gone wrong—\\none nice way to do this is to use the confusion matrix, which we can compute with\\nScikit-Learn and plot with Seaborn (Figure 5-20):\\nIn[31]: from sklearn.metrics import confusion_matrix\\n        mat = confusion_matrix(ytest, y_model)\\n        sns.heatmap(mat, square=True, annot=True, cbar=False)\\n        plt.xlabel('predicted value')\\n        plt.ylabel('true value');\\nFigure 5-20. A confusion matrix showing the frequency of misclassifications by our\\nclassifier\\nIntroducing Scikit-Learn \\n| \\n357\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 375}, page_content=\"This shows us where the mislabeled points tend to be: for example, a large number of\\ntwos here are misclassified as either ones or eights. Another way to gain intuition into\\nthe characteristics of the model is to plot the inputs again, with their predicted labels.\\nWe’ll use green for correct labels, and red for incorrect labels (Figure 5-21):\\nIn[32]: fig, axes = plt.subplots(10, 10, figsize=(8, 8),\\n                                 subplot_kw={'xticks':[], 'yticks':[]},\\n                                 gridspec_kw=dict(hspace=0.1, wspace=0.1))\\n        for i, ax in enumerate(axes.flat):\\n            ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\\n            ax.text(0.05, 0.05, str(y_model[i]),\\n                    transform=ax.transAxes,\\n                    color='green' if (ytest[i] == y_model[i]) else 'red')\\nFigure 5-21. Data showing correct (green) and incorrect (red) labels; for a color version\\nof this plot, see the online appendix\\nExamining this subset of the data, we can gain insight regarding where the algorithm\\nmight not be performing optimally. To go beyond our 80% classification rate, we\\nmight move to a more sophisticated algorithm, such as support vector machines (see\\n“In-Depth: Support Vector Machines” on page 405) or random forests (see “In-\\nDepth: Decision Trees and Random Forests” on page 421), or another classification\\napproach.\\n358 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 376}, page_content='Summary\\nIn this section we have covered the essential features of the Scikit-Learn data repre‐\\nsentation, and the estimator API. Regardless of the type of estimator, the same\\nimport/instantiate/fit/predict pattern holds. Armed with this information about the\\nestimator API, you can explore the Scikit-Learn documentation and begin trying out\\nvarious models on your data.\\nIn the next section, we will explore perhaps the most important topic in machine\\nlearning: how to select and validate your model.\\nHyperparameters and Model Validation\\nIn the previous section, we saw the basic recipe for applying a supervised machine\\nlearning model:\\n1. Choose a class of model\\n2. Choose model hyperparameters\\n3. Fit the model to the training data\\n4. Use the model to predict labels for new data\\nThe first two pieces of this—the choice of model and choice of hyperparameters—are\\nperhaps the most important part of using these tools and techniques effectively. In\\norder to make an informed choice, we need a way to validate that our model and our\\nhyperparameters are a good fit to the data. While this may sound simple, there are\\nsome pitfalls that you must avoid to do this effectively.\\nThinking About Model Validation\\nIn principle, model validation is very simple: after choosing a model and its hyper‐\\nparameters, we can estimate how effective it is by applying it to some of the training\\ndata and comparing the prediction to the known value.\\nThe following sections first show a naive approach to model validation and why it\\nfails, before exploring the use of holdout sets and cross-validation for more robust\\nmodel evaluation.\\nModel validation the wrong way\\nLet’s demonstrate the naive approach to validation using the Iris data, which we saw\\nin the previous section. We will start by loading the data:\\nIn[1]: from sklearn.datasets import load_iris\\n       iris = load_iris()\\nHyperparameters and Model Validation \\n| \\n359'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 377}, page_content='X = iris.data\\n       y = iris.target\\nNext we choose a model and hyperparameters. Here we’ll use a k-neighbors classifier\\nwith n_neighbors=1. This is a very simple and intuitive model that says “the label of\\nan unknown point is the same as the label of its closest training point”:\\nIn[2]: from sklearn.neighbors import KNeighborsClassifier\\n       model = KNeighborsClassifier(n_neighbors=1)\\nThen we train the model, and use it to predict labels for data we already know:\\nIn[3]: model.fit(X, y)\\n       y_model = model.predict(X)\\nFinally, we compute the fraction of correctly labeled points:\\nIn[4]: from sklearn.metrics import accuracy_score\\n       accuracy_score(y, y_model)\\nOut[4]: 1.0\\nWe see an accuracy score of 1.0, which indicates that 100% of points were correctly\\nlabeled by our model! But is this truly measuring the expected accuracy? Have we\\nreally come upon a model that we expect to be correct 100% of the time?\\nAs you may have gathered, the answer is no. In fact, this approach contains a funda‐\\nmental flaw: it trains and evaluates the model on the same data. Furthermore, the\\nnearest neighbor model is an instance-based estimator that simply stores the training\\ndata, and predicts labels by comparing new data to these stored points; except in con‐\\ntrived cases, it will get 100% accuracy every time!\\nModel validation the right way: Holdout sets\\nSo what can be done? We can get a better sense of a model’s performance using what’s\\nknown as a holdout set; that is, we hold back some subset of the data from the training\\nof the model, and then use this holdout set to check the model performance. We can\\ndo this splitting using the train_test_split utility in Scikit-Learn:\\nIn[5]: from sklearn.cross_validation import train_test_split\\n       # split the data with 50% in each set\\n       X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\\n                                         train_size=0.5)\\n       # fit the model on one set of data\\n       model.fit(X1, y1)\\n       # evaluate the model on the second set of data\\n       y2_model = model.predict(X2)\\n       accuracy_score(y2, y2_model)\\nOut[5]: 0.90666666666666662\\n360 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 378}, page_content='We see here a more reasonable result: the nearest-neighbor classifier is about 90%\\naccurate on this holdout set. The holdout set is similar to unknown data, because the\\nmodel has not “seen” it before.\\nModel validation via cross-validation\\nOne disadvantage of using a holdout set for model validation is that we have lost a\\nportion of our data to the model training. In the previous case, half the dataset does\\nnot contribute to the training of the model! This is not optimal, and can cause prob‐\\nlems—especially if the initial set of training data is small.\\nOne way to address this is to use cross-validation—that is, to do a sequence of fits\\nwhere each subset of the data is used both as a training set and as a validation set.\\nVisually, it might look something like Figure 5-22.\\nFigure 5-22. Visualization of two-fold cross-validation\\nHere we do two validation trials, alternately using each half of the data as a holdout\\nset. Using the split data from before, we could implement it like this:\\nIn[6]: y2_model = model.fit(X1, y1).predict(X2)\\n       y1_model = model.fit(X2, y2).predict(X1)\\n       accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)\\nOut[6]: (0.95999999999999996, 0.90666666666666662)\\nWhat comes out are two accuracy scores, which we could combine (by, say, taking the\\nmean) to get a better measure of the global model performance. This particular form\\nof cross-validation is a two-fold cross-validation—one in which we have split the data\\ninto two sets and used each in turn as a validation set.\\nWe could expand on this idea to use even more trials, and more folds in the data—for\\nexample, Figure 5-23 is a visual depiction of five-fold cross-validation.\\nHyperparameters and Model Validation \\n| \\n361'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 379}, page_content='Figure 5-23. Visualization of five-fold cross-validation\\nHere we split the data into five groups, and use each of them in turn to evaluate the\\nmodel fit on the other 4/5 of the data. This would be rather tedious to do by hand,\\nand so we can use Scikit-Learn’s cross_val_score convenience routine to do it\\nsuccinctly:\\nIn[7]: from sklearn.cross_validation import cross_val_score\\n       cross_val_score(model, X, y, cv=5)\\nOut[7]: array([ 0.96666667,  0.96666667,  0.93333333,  0.93333333,  1.        ])\\nRepeating the validation across different subsets of the data gives us an even better\\nidea of the performance of the algorithm.\\nScikit-Learn implements a number of cross-validation schemes that are useful in par‐\\nticular situations; these are implemented via iterators in the cross_validation mod‐\\nule. For example, we might wish to go to the extreme case in which our number of\\nfolds is equal to the number of data points; that is, we train on all points but one in\\neach trial. This type of cross-validation is known as leave-one-out cross-validation,\\nand can be used as follows:\\nIn[8]: from sklearn.cross_validation import LeaveOneOut\\n       scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\\n       scores\\nOut[8]: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\\n362 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 380}, page_content='1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\\n                1.,  1.,  1.,  1.,  1.,  1.,  1.])\\nBecause we have 150 samples, the leave-one-out cross-validation yields scores for 150\\ntrials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\\nTaking the mean of these gives an estimate of the error rate:\\nIn[9]: scores.mean()\\nOut[9]: 0.95999999999999996\\nOther cross-validation schemes can be used similarly. For a description of what is\\navailable in Scikit-Learn, use IPython to explore the sklearn.cross_validation sub‐\\nmodule, or take a look at Scikit-Learn’s online cross-validation documentation.\\nSelecting the Best Model\\nNow that we’ve seen the basics of validation and cross-validation, we will go into a\\nlittle more depth regarding model selection and selection of hyperparameters. These\\nissues are some of the most important aspects of the practice of machine learning,\\nand I find that this information is often glossed over in introductory machine learn‐\\ning tutorials.\\nOf core importance is the following question: if our estimator is underperforming, how\\nshould we move forward? There are several possible answers:\\n• Use a more complicated/more flexible model\\n• Use a less complicated/less flexible model\\n• Gather more training samples\\n• Gather more data to add features to each sample\\nThe answer to this question is often counterintuitive. In particular, sometimes using a\\nmore complicated model will give worse results, and adding more training samples\\nmay not improve your results! The ability to determine what steps will improve your\\nmodel is what separates the successful machine learning practitioners from the\\nunsuccessful.\\nHyperparameters and Model Validation \\n| \\n363'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 381}, page_content='The bias–variance trade-off\\nFundamentally, the question of “the best model” is about finding a sweet spot in the\\ntrade-off between bias and variance. Consider Figure 5-24, which presents two\\nregression fits to the same dataset.\\nFigure 5-24. A high-bias and high-variance regression model\\nIt is clear that neither of these models is a particularly good fit to the data, but they\\nfail in different ways.\\nThe model on the left attempts to find a straight-line fit through the data. Because the\\ndata are intrinsically more complicated than a straight line, the straight-line model\\nwill never be able to describe this dataset well. Such a model is said to underfit the\\ndata; that is, it does not have enough model flexibility to suitably account for all the\\nfeatures in the data. Another way of saying this is that the model has high bias.\\nThe model on the right attempts to fit a high-order polynomial through the data.\\nHere the model fit has enough flexibility to nearly perfectly account for the fine fea‐\\ntures in the data, but even though it very accurately describes the training data, its\\nprecise form seems to be more reflective of the particular noise properties of the data\\nrather than the intrinsic properties of whatever process generated that data. Such a\\nmodel is said to overfit the data; that is, it has so much model flexibility that the\\nmodel ends up accounting for random errors as well as the underlying data distribu‐\\ntion. Another way of saying this is that the model has high variance.\\nTo look at this in another light, consider what happens if we use these two models to\\npredict the y-value for some new data. In diagrams in Figure 5-25, the red/lighter\\npoints indicate data that is omitted from the training set.\\n364 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 382}, page_content='Figure 5-25. Training and validation scores in high-bias and high-variance models\\nThe score here is the R2 score, or coefficient of determination, which measures how\\nwell a model performs relative to a simple mean of the target values. R2 = 1 indicates\\na perfect match, R2 = 0 indicates the model does no better than simply taking the\\nmean of the data, and negative values mean even worse models. From the scores asso‐\\nciated with these two models, we can make an observation that holds more generally:\\n• For high-bias models, the performance of the model on the validation set is simi‐\\nlar to the performance on the training set.\\n• For high-variance models, the performance of the model on the validation set is\\nfar worse than the performance on the training set.\\nIf we imagine that we have some ability to tune the model complexity, we would\\nexpect the training score and validation score to behave as illustrated in Figure 5-26.\\nThe diagram shown in Figure 5-26 is often called a validation curve, and we see the\\nfollowing essential features:\\n• The training score is everywhere higher than the validation score. This is gener‐\\nally the case: the model will be a better fit to data it has seen than to data it has\\nnot seen.\\n• For very low model complexity (a high-bias model), the training data is underfit,\\nwhich means that the model is a poor predictor both for the training data and for\\nany previously unseen data.\\n• For very high model complexity (a high-variance model), the training data is\\noverfit, which means that the model predicts the training data very well, but fails\\nfor any previously unseen data.\\n• For some intermediate value, the validation curve has a maximum. This level of\\ncomplexity indicates a suitable trade-off between bias and variance.\\nHyperparameters and Model Validation \\n| \\n365'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 383}, page_content='Figure 5-26. A schematic of the relationship between model complexity, training score,\\nand validation score\\nThe means of tuning the model complexity varies from model to model; when we\\ndiscuss individual models in depth in later sections, we will see how each model\\nallows for such tuning.\\nValidation curves in Scikit-Learn\\nLet’s look at an example of using cross-validation to compute the validation curve for\\na class of models. Here we will use a polynomial regression model: this is a generalized\\nlinear model in which the degree of the polynomial is a tunable parameter. For\\nexample, a degree-1 polynomial fits a straight line to the data; for model parameters a\\nand b:\\ny = ax + b\\nA degree-3 polynomial fits a cubic curve to the data; for model parameters a, b, c, d:\\ny = ax3 + bx2 + cx + d\\nWe can generalize this to any number of polynomial features. In Scikit-Learn, we can\\nimplement this with a simple linear regression combined with the polynomial pre‐\\nprocessor. We will use a pipeline to string these operations together (we will discuss\\npolynomial features and pipelines more fully in “Feature Engineering” on page 375):\\n366 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 384}, page_content=\"In[10]: from sklearn.preprocessing import PolynomialFeatures\\n        from sklearn.linear_model import LinearRegression\\n        from sklearn.pipeline import make_pipeline\\n        def PolynomialRegression(degree=2, **kwargs):\\n            return make_pipeline(PolynomialFeatures(degree),\\n                                 LinearRegression(**kwargs))\\nNow let’s create some data to which we will fit our model:\\nIn[11]: import numpy as np\\n        def make_data(N, err=1.0, rseed=1):\\n            # randomly sample the data\\n            rng = np.random.RandomState(rseed)\\n            X = rng.rand(N, 1) ** 2\\n            y = 10 - 1. / (X.ravel() + 0.1)\\n            if err > 0:\\n                y += err * rng.randn(N)\\n            return X, y\\n        X, y = make_data(40)\\nWe can now visualize our data, along with polynomial fits of several degrees\\n(Figure 5-27):\\nIn[12]: %matplotlib inline\\n        import matplotlib.pyplot as plt\\n        import seaborn; seaborn.set()  # plot formatting\\n        X_test = np.linspace(-0.1, 1.1, 500)[:, None]\\n        plt.scatter(X.ravel(), y, color='black')\\n        axis = plt.axis()\\n        for degree in [1, 3, 5]:\\n            y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\\n            plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\\n        plt.xlim(-0.1, 1.0)\\n        plt.ylim(-2, 12)\\n        plt.legend(loc='best');\\nThe knob controlling model complexity in this case is the degree of the polynomial,\\nwhich can be any non-negative integer. A useful question to answer is this: what\\ndegree of polynomial provides a suitable trade-off between bias (underfitting) and\\nvariance (overfitting)?\\nHyperparameters and Model Validation \\n| \\n367\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 385}, page_content=\"Figure 5-27. Three different polynomial models fit to a dataset\\nWe can make progress in this by visualizing the validation curve for this particular\\ndata and model; we can do this straightforwardly using the validation_curve conve‐\\nnience routine provided by Scikit-Learn. Given a model, data, parameter name, and a\\nrange to explore, this function will automatically compute both the training score and\\nvalidation score across the range (Figure 5-28):\\nIn[13]:\\nfrom sklearn.learning_curve import validation_curve\\ndegree = np.arange(0, 21)\\ntrain_score, val_score = validation_curve(PolynomialRegression(), X, y,\\n                                          'polynomialfeatures__degree',\\n                                          degree, cv=7)\\nplt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\\nplt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\\nplt.legend(loc='best')\\nplt.ylim(0, 1)\\nplt.xlabel('degree')\\nplt.ylabel('score');\\nThis shows precisely the qualitative behavior we expect: the training score is every‐\\nwhere higher than the validation score; the training score is monotonically improving\\nwith increased model complexity; and the validation score reaches a maximum\\nbefore dropping off as the model becomes overfit.\\n368 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 386}, page_content='Figure 5-28. The validation curves for the data in Figure 5-27 (cf. Figure 5-26)\\nFrom the validation curve, we can read off that the optimal trade-off between bias\\nand variance is found for a third-order polynomial; we can compute and display this\\nfit over the original data as follows (Figure 5-29):\\nIn[14]: plt.scatter(X.ravel(), y)\\n        lim = plt.axis()\\n        y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\\n        plt.plot(X_test.ravel(), y_test);\\n        plt.axis(lim);\\nFigure 5-29. The cross-validated optimal model for the data in Figure 5-27\\nHyperparameters and Model Validation \\n| \\n369'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 387}, page_content=\"Notice that finding this optimal model did not actually require us to compute the\\ntraining score, but examining the relationship between the training score and valida‐\\ntion score can give us useful insight into the performance of the model.\\nLearning Curves\\nOne important aspect of model complexity is that the optimal model will generally\\ndepend on the size of your training data. For example, let’s generate a new dataset\\nwith a factor of five more points (Figure 5-30):\\nIn[15]: X2, y2 = make_data(200)\\n        plt.scatter(X2.ravel(), y2);\\nFigure 5-30. Data to demonstrate learning curves\\nWe will duplicate the preceding code to plot the validation curve for this larger data‐\\nset; for reference let’s over-plot the previous results as well (Figure 5-31):\\nIn[16]:\\ndegree = np.arange(21)\\ntrain_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\\n                                            'polynomialfeatures__degree',\\n                                            degree, cv=7)\\nplt.plot(degree, np.median(train_score2, 1), color='blue',\\n         label='training score')\\nplt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\\nplt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3,\\n         linestyle='dashed')\\nplt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3,\\n         linestyle='dashed')\\nplt.legend(loc='lower center')\\nplt.ylim(0, 1)\\n370 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 388}, page_content=\"plt.xlabel('degree')\\nplt.ylabel('score');\\nFigure 5-31. Learning curves for the polynomial model fit to data in Figure 5-30\\nThe solid lines show the new results, while the fainter dashed lines show the results of\\nthe previous smaller dataset. It is clear from the validation curve that the larger data‐\\nset can support a much more complicated model: the peak here is probably around a\\ndegree of 6, but even a degree-20 model is not seriously overfitting the data—the vali‐\\ndation and training scores remain very close.\\nThus we see that the behavior of the validation curve has not one, but two, important\\ninputs: the model complexity and the number of training points. It is often useful to\\nexplore the behavior of the model as a function of the number of training points,\\nwhich we can do by using increasingly larger subsets of the data to fit our model. A\\nplot of the training/validation score with respect to the size of the training set is\\nknown as a learning curve.\\nThe general behavior we would expect from a learning curve is this:\\n• A model of a given complexity will overfit a small dataset: this means the training\\nscore will be relatively high, while the validation score will be relatively low.\\n• A model of a given complexity will underfit a large dataset: this means that the\\ntraining score will decrease, but the validation score will increase.\\n• A model will never, except by chance, give a better score to the validation set than\\nthe training set: this means the curves should keep getting closer together but\\nnever cross.\\nHyperparameters and Model Validation \\n| \\n371\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 389}, page_content=\"With these features in mind, we would expect a learning curve to look qualitatively\\nlike that shown in Figure 5-32.\\nFigure 5-32. Schematic showing the typical interpretation of learning curves\\nThe notable feature of the learning curve is the convergence to a particular score as\\nthe number of training samples grows. In particular, once you have enough points\\nthat a particular model has converged, adding more training data will not help you!\\nThe only way to increase model performance in this case is to use another (often\\nmore complex) model.\\nLearning curves in Scikit-Learn\\nScikit-Learn offers a convenient utility for computing such learning curves from your\\nmodels; here we will compute a learning curve for our original dataset with a second-\\norder polynomial model and a ninth-order polynomial (Figure 5-33):\\nIn[17]:\\nfrom sklearn.learning_curve import learning_curve\\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\\nfor i, degree in enumerate([2, 9]):\\n    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\\n                                         X, y, cv=7,\\n                                         train_sizes=np.linspace(0.3, 1, 25))\\n    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\\n    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\\n    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], color='gray',\\n                 linestyle='dashed')\\n372 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 390}, page_content=\"ax[i].set_ylim(0, 1)\\n    ax[i].set_xlim(N[0], N[-1])\\n    ax[i].set_xlabel('training size')\\n    ax[i].set_ylabel('score')\\n    ax[i].set_title('degree = {0}'.format(degree), size=14)\\n    ax[i].legend(loc='best')\\nFigure 5-33. Learning curves for a low-complexity model (left) and a high-complexity\\nmodel (right)\\nThis is a valuable diagnostic, because it gives us a visual depiction of how our model\\nresponds to increasing training data. In particular, when your learning curve has\\nalready converged (i.e., when the training and validation curves are already close to\\neach other), adding more training data will not significantly improve the fit! This situa‐\\ntion is seen in the left panel, with the learning curve for the degree-2 model.\\nThe only way to increase the converged score is to use a different (usually more com‐\\nplicated) model. We see this in the right panel: by moving to a much more compli‐\\ncated model, we increase the score of convergence (indicated by the dashed line), but\\nat the expense of higher model variance (indicated by the difference between the\\ntraining and validation scores). If we were to add even more data points, the learning\\ncurve for the more complicated model would eventually converge.\\nPlotting a learning curve for your particular choice of model and dataset can help you\\nto make this type of decision about how to move forward in improving your analysis.\\nValidation in Practice: Grid Search\\nThe preceding discussion is meant to give you some intuition into the trade-off\\nbetween bias and variance, and its dependence on model complexity and training set\\nsize. In practice, models generally have more than one knob to turn, and thus plots of\\nvalidation and learning curves change from lines to multidimensional surfaces. In\\nthese cases, such visualizations are difficult and we would rather simply find the par‐\\nticular model that maximizes the validation score.\\nHyperparameters and Model Validation \\n| \\n373\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 391}, page_content=\"Scikit-Learn provides automated tools to do this in the grid_search module. Here is\\nan example of using grid search to find the optimal polynomial model. We will\\nexplore a three-dimensional grid of model features—namely, the polynomial degree,\\nthe flag telling us whether to fit the intercept, and the flag telling us whether to nor‐\\nmalize the problem. We can set this up using Scikit-Learn’s GridSearchCV meta-\\nestimator:\\nIn[18]: from sklearn.grid_search import GridSearchCV\\n        param_grid = {'polynomialfeatures__degree': np.arange(21),\\n                      'linearregression__fit_intercept': [True, False],\\n                      'linearregression__normalize': [True, False]}\\n        grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\\nNotice that like a normal estimator, this has not yet been applied to any data. Calling\\nthe fit() method will fit the model at each grid point, keeping track of the scores\\nalong the way:\\nIn[19]: grid.fit(X, y);\\nNow that this is fit, we can ask for the best parameters as follows:\\nIn[20]: grid.best_params_\\nOut[20]: {'linearregression__fit_intercept': False,\\n          'linearregression__normalize': True,\\n          'polynomialfeatures__degree': 4}\\nFinally, if we wish, we can use the best model and show the fit to our data using code\\nfrom before (Figure 5-34):\\nIn[21]: model = grid.best_estimator_\\n        plt.scatter(X.ravel(), y)\\n        lim = plt.axis()\\n        y_test = model.fit(X, y).predict(X_test)\\n        plt.plot(X_test.ravel(), y_test, hold=True);\\n        plt.axis(lim);\\nThe grid search provides many more options, including the ability to specify a cus‐\\ntom scoring function, to parallelize the computations, to do randomized searches,\\nand more. For information, see the examples in “In-Depth: Kernel Density Estima‐\\ntion” on page 491 and “Application: A Face Detection Pipeline” on page 506, or refer\\nto Scikit-Learn’s grid search documentation.\\n374 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 392}, page_content='Figure 5-34. The best-fit model determined via an automatic grid-search\\nSummary\\nIn this section, we have begun to explore the concept of model validation and hyper‐\\nparameter optimization, focusing on intuitive aspects of the bias–variance trade-off\\nand how it comes into play when fitting models to data. In particular, we found that\\nthe use of a validation set or cross-validation approach is vital when tuning parame‐\\nters in order to avoid overfitting for more complex/flexible models.\\nIn later sections, we will discuss the details of particularly useful models, and\\nthroughout will talk about what tuning is available for these models and how these\\nfree parameters affect model complexity. Keep the lessons of this section in mind as\\nyou read on and learn about these machine learning approaches!\\nFeature Engineering\\nThe previous sections outline the fundamental ideas of machine learning, but all of\\nthe examples assume that you have numerical data in a tidy, [n_samples, n_fea\\ntures] format. In the real world, data rarely comes in such a form. With this in mind,\\none of the more important steps in using machine learning in practice is feature engi‐\\nneering—that is, taking whatever information you have about your problem and\\nturning it into numbers that you can use to build your feature matrix.\\nIn this section, we will cover a few common examples of feature engineering tasks:\\nfeatures for representing categorical data, features for representing text, and features\\nfor representing images. Additionally, we will discuss derived features for increasing\\nmodel complexity and imputation of missing data. Often this process is known as vec‐\\ntorization, as it involves converting arbitrary data into well-behaved vectors.\\nFeature Engineering \\n| \\n375'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 393}, page_content=\"Categorical Features\\nOne common type of non-numerical data is categorical data. For example, imagine\\nyou are exploring some data on housing prices, and along with numerical features\\nlike “price” and “rooms,” you also have “neighborhood” information. For example,\\nyour data might look something like this:\\nIn[1]: data = [\\n           {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\\n           {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\\n           {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\\n           {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\\n       ]\\nYou might be tempted to encode this data with a straightforward numerical mapping:\\nIn[2]: {'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};\\nIt turns out that this is not generally a useful approach in Scikit-Learn: the package’s\\nmodels make the fundamental assumption that numerical features reflect algebraic\\nquantities. Thus such a mapping would imply, for example, that Queen Anne < Fre‐\\nmont < Wallingford, or even that Wallingford - Queen Anne = Fremont, which (niche\\ndemographic jokes aside) does not make much sense.\\nIn this case, one proven technique is to use one-hot encoding, which effectively creates\\nextra columns indicating the presence or absence of a category with a value of 1 or 0,\\nrespectively. When your data comes as a list of dictionaries, Scikit-Learn’s DictVector\\nizer will do this for you:\\nIn[3]: from sklearn.feature_extraction import DictVectorizer\\n       vec = DictVectorizer(sparse=False, dtype=int)\\n       vec.fit_transform(data)\\nOut[3]: array([[     0,      1,      0, 850000,      4],\\n               [     1,      0,      0, 700000,      3],\\n               [     0,      0,      1, 650000,      3],\\n               [     1,      0,      0, 600000,      2]], dtype=int64)\\nNotice that the neighborhood column has been expanded into three separate columns,\\nrepresenting the three neighborhood labels, and that each row has a 1 in the column\\nassociated with its neighborhood. With these categorical features thus encoded, you\\ncan proceed as normal with fitting a Scikit-Learn model.\\nTo see the meaning of each column, you can inspect the feature names:\\nIn[4]: vec.get_feature_names()\\nOut[4]: ['neighborhood=Fremont',\\n         'neighborhood=Queen Anne',\\n         'neighborhood=Wallingford',\\n         'price',\\n         'rooms']\\n376 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 394}, page_content=\"There is one clear disadvantage of this approach: if your category has many possible\\nvalues, this can greatly increase the size of your dataset. However, because the enco‐\\nded data contains mostly zeros, a sparse output can be a very efficient solution:\\nIn[5]: vec = DictVectorizer(sparse=True, dtype=int)\\n       vec.fit_transform(data)\\nOut[5]: <4x5 sparse matrix of type '<class 'numpy.int64'>'\\n            with 12 stored elements in Compressed Sparse Row format>\\nMany (though not yet all) of the Scikit-Learn estimators accept such sparse inputs\\nwhen fitting and evaluating models. sklearn.preprocessing.OneHotEncoder and\\nsklearn.feature_extraction.FeatureHasher are two additional tools that Scikit-\\nLearn includes to support this type of encoding.\\nText Features\\nAnother common need in feature engineering is to convert text to a set of representa‐\\ntive numerical values. For example, most automatic mining of social media data relies\\non some form of encoding the text as numbers. One of the simplest methods of\\nencoding data is by word counts: you take each snippet of text, count the occurrences\\nof each word within it, and put the results in a table.\\nFor example, consider the following set of three phrases:\\nIn[6]: sample = ['problem of evil',\\n                 'evil queen',\\n                 'horizon problem']\\nFor a vectorization of this data based on word count, we could construct a column\\nrepresenting the word “problem,” the word “evil,” the word “horizon,” and so on.\\nWhile doing this by hand would be possible, we can avoid the tedium by using Scikit-\\nLearn’s CountVectorizer:\\nIn[7]: from sklearn.feature_extraction.text import CountVectorizer\\n       vec = CountVectorizer()\\n       X = vec.fit_transform(sample)\\n       X\\nOut[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>'\\n            with 7 stored elements in Compressed Sparse Row format>\\nThe result is a sparse matrix recording the number of times each word appears; it is\\neasier to inspect if we convert this to a DataFrame with labeled columns:\\nIn[8]: import pandas as pd\\n       pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\\nFeature Engineering \\n| \\n377\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 395}, page_content='Out[8]:    evil  horizon  of  problem  queen\\n        0     1        0   1        1      0\\n        1     1        0   0        0      1\\n        2     0        1   0        1      0\\nThere are some issues with this approach, however: the raw word counts lead to fea‐\\ntures that put too much weight on words that appear very frequently, and this can be\\nsuboptimal in some classification algorithms. One approach to fix this is known as \\nterm frequency–inverse document frequency (TF–IDF), which weights the word counts\\nby a measure of how often they appear in the documents. The syntax for computing\\nthese features is similar to the previous example:\\nIn[9]: from sklearn.feature_extraction.text import TfidfVectorizer\\n       vec = TfidfVectorizer()\\n       X = vec.fit_transform(sample)\\n       pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\\nOut[9]:        evil   horizon        of   problem     queen\\n        0  0.517856  0.000000  0.680919  0.517856  0.000000\\n        1  0.605349  0.000000  0.000000  0.000000  0.795961\\n        2  0.000000  0.795961  0.000000  0.605349  0.000000\\nFor an example of using TF–IDF in a classification problem, see “In Depth: Naive\\nBayes Classification” on page 382.\\nImage Features\\nAnother common need is to suitably encode images for machine learning analysis.\\nThe simplest approach is what we used for the digits data in “Introducing Scikit-\\nLearn” on page 343: simply using the pixel values themselves. But depending on the\\napplication, such approaches may not be optimal.\\nA comprehensive summary of feature extraction techniques for images is well beyond\\nthe scope of this section, but you can find excellent implementations of many of the\\nstandard approaches in the Scikit-Image project. For one example of using Scikit-\\nLearn and Scikit-Image together, see “Application: A Face Detection Pipeline” on page\\n506.\\nDerived Features\\nAnother useful type of feature is one that is mathematically derived from some input\\nfeatures. We saw an example of this in “Hyperparameters and Model Validation” on\\npage 359 when we constructed polynomial features from our input data. We saw that\\nwe could convert a linear regression into a polynomial regression not by changing the\\nmodel, but by transforming the input! This is sometimes known as basis function\\nregression, and is explored further in “In Depth: Linear Regression” on page 390.\\n378 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 396}, page_content='For example, this data clearly cannot be well described by a straight line\\n(Figure 5-35):\\nIn[10]: %matplotlib inline\\n        import numpy as np\\n        import matplotlib.pyplot as plt\\n        x = np.array([1, 2, 3, 4, 5])\\n        y = np.array([4, 2, 1, 3, 7])\\n        plt.scatter(x, y);\\nFigure 5-35. Data that is not well described by a straight line\\nStill, we can fit a line to the data using LinearRegression and get the optimal result\\n(Figure 5-36):\\nIn[11]: from sklearn.linear_model import LinearRegression\\n        X = x[:, np.newaxis]\\n        model = LinearRegression().fit(X, y)\\n        yfit = model.predict(X)\\n        plt.scatter(x, y)\\n        plt.plot(x, yfit);\\nFigure 5-36. A poor straight-line fit\\nFeature Engineering \\n| \\n379'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 397}, page_content='It’s clear that we need a more sophisticated model to describe the relationship\\nbetween x and y. We can do this by transforming the data, adding extra columns of\\nfeatures to drive more flexibility in the model. For example, we can add polynomial\\nfeatures to the data this way:\\nIn[12]: from sklearn.preprocessing import PolynomialFeatures\\n        poly = PolynomialFeatures(degree=3, include_bias=False)\\n        X2 = poly.fit_transform(X)\\n        print(X2)\\n[[   1.    1.    1.]\\n [   2.    4.    8.]\\n [   3.    9.   27.]\\n [   4.   16.   64.]\\n [   5.   25.  125.]]\\nThe derived feature matrix has one column representing x, and a second column rep‐\\nresenting x2, and a third column representing x3. Computing a linear regression on\\nthis expanded input gives a much closer fit to our data (Figure 5-37):\\nIn[13]: model = LinearRegression().fit(X2, y)\\n        yfit = model.predict(X2)\\n        plt.scatter(x, y)\\n        plt.plot(x, yfit);\\nFigure 5-37. A linear fit to polynomial features derived from the data\\nThis idea of improving a model not by changing the model, but by transforming the\\ninputs, is fundamental to many of the more powerful machine learning methods. We\\nexplore this idea further in “In Depth: Linear Regression” on page 390 in the context\\nof basis function regression. More generally, this is one motivational path to the pow‐\\nerful set of techniques known as kernel methods, which we will explore in “In-Depth:\\nSupport Vector Machines” on page 405.\\n380 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 398}, page_content=\"Imputation of Missing Data\\nAnother common need in feature engineering is handling missing data. We discussed\\nthe handling of missing data in DataFrames in “Handling Missing Data” on page 119,\\nand saw that often the NaN value is used to mark missing values. For example, we\\nmight have a dataset that looks like this:\\nIn[14]: from numpy import nan\\n        X = np.array([[ nan, 0,   3  ],\\n                      [ 3,   7,   9  ],\\n                      [ 3,   5,   2  ],\\n                      [ 4,   nan, 6  ],\\n                      [ 8,   8,   1  ]])\\n        y = np.array([14, 16, -1,  8, -5])\\nWhen applying a typical machine learning model to such data, we will need to first\\nreplace such missing data with some appropriate fill value. This is known as imputa‐\\ntion of missing values, and strategies range from simple (e.g., replacing missing values\\nwith the mean of the column) to sophisticated (e.g., using matrix completion or a\\nrobust model to handle such data).\\nThe sophisticated approaches tend to be very application-specific, and we won’t dive\\ninto them here. For a baseline imputation approach, using the mean, median, or most\\nfrequent value, Scikit-Learn provides the Imputer class:\\nIn[15]: from sklearn.preprocessing import Imputer\\n        imp = Imputer(strategy='mean')\\n        X2 = imp.fit_transform(X)\\n        X2\\nOut[15]: array([[ 4.5,  0. ,  3. ],\\n                [ 3. ,  7. ,  9. ],\\n                [ 3. ,  5. ,  2. ],\\n                [ 4. ,  5. ,  6. ],\\n                [ 8. ,  8. ,  1. ]])\\nWe see that in the resulting data, the two missing values have been replaced with the\\nmean of the remaining values in the column. This imputed data can then be fed\\ndirectly into, for example, a LinearRegression estimator:\\nIn[16]: model = LinearRegression().fit(X2, y)\\n        model.predict(X2)\\nOut[16]:\\narray([ 13.14869292,  14.3784627 ,  -1.15539732,  10.96606197,  -5.33782027])\\nFeature Pipelines\\nWith any of the preceding examples, it can quickly become tedious to do the transfor‐\\nmations by hand, especially if you wish to string together multiple steps. For example,\\nwe might want a processing pipeline that looks something like this:\\nFeature Engineering \\n| \\n381\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 399}, page_content=\"1. Impute missing values using the mean\\n2. Transform features to quadratic\\n3. Fit a linear regression\\nTo streamline this type of processing pipeline, Scikit-Learn provides a pipeline object,\\nwhich can be used as follows:\\nIn[17]: from sklearn.pipeline import make_pipeline\\n        model = make_pipeline(Imputer(strategy='mean'),\\n                              PolynomialFeatures(degree=2),\\n                              LinearRegression())\\nThis pipeline looks and acts like a standard Scikit-Learn object, and will apply all the\\nspecified steps to any input data.\\nIn[18]: model.fit(X, y)  # X with missing values, from above\\n        print(y)\\n        print(model.predict(X))\\n[14 16 -1  8 -5]\\n[ 14.  16.  -1.   8.  -5.]\\nAll the steps of the model are applied automatically. Notice that for the simplicity of\\nthis demonstration, we’ve applied the model to the data it was trained on; this is why\\nit was able to perfectly predict the result (refer back to “Hyperparameters and Model\\nValidation” on page 359 for further discussion of this).\\nFor some examples of Scikit-Learn pipelines in action, see the following section on\\nnaive Bayes classification as well as “In Depth: Linear Regression” on page 390 and\\n“In-Depth: Support Vector Machines” on page 405.\\nIn Depth: Naive Bayes Classification\\nThe previous four sections have given a general overview of the concepts of machine\\nlearning. In this section and the ones that follow, we will be taking a closer look at\\nseveral specific algorithms for supervised and unsupervised learning, starting here\\nwith naive Bayes classification.\\nNaive Bayes models are a group of extremely fast and simple classification algorithms\\nthat are often suitable for very high-dimensional datasets. Because they are so fast\\nand have so few tunable parameters, they end up being very useful as a quick-and-\\ndirty baseline for a classification problem. This section will focus on an intuitive\\nexplanation of how naive Bayes classifiers work, followed by a couple examples of\\nthem in action on some datasets.\\n382 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 400}, page_content='Bayesian Classification\\nNaive Bayes classifiers are built on Bayesian classification methods. These rely on \\nBayes’s theorem, which is an equation describing the relationship of conditional\\nprobabilities of statistical quantities. In Bayesian classification, we’re interested in\\nfinding the probability of a label given some observed features, which we can write as\\nP L\\nfeatures . Bayes’s theorem tells us how to express this in terms of quantities we\\ncan compute more directly:\\nP L\\nfeatures = P features\\nL P L\\nP features\\nIf we are trying to decide between two labels—let’s call them L1 and L2—then one way\\nto make this decision is to compute the ratio of the posterior probabilities for each\\nlabel:\\nP L1\\nfeatures\\nP L1\\nfeatures =\\nP features\\nL1\\nP features\\nL2\\nP L1\\nP L2\\nAll we need now is some model by which we can compute P features\\nLi  for each\\nlabel. Such a model is called a generative model because it specifies the hypothetical\\nrandom process that generates the data. Specifying this generative model for each\\nlabel is the main piece of the training of such a Bayesian classifier. The general ver‐\\nsion of such a training step is a very difficult task, but we can make it simpler through\\nthe use of some simplifying assumptions about the form of this model.\\nThis is where the “naive” in “naive Bayes” comes in: if we make very naive assump‐\\ntions about the generative model for each label, we can find a rough approximation of\\nthe generative model for each class, and then proceed with the Bayesian classification.\\nDifferent types of naive Bayes classifiers rest on different naive assumptions about the\\ndata, and we will examine a few of these in the following sections. We begin with the\\nstandard imports:\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\nGaussian Naive Bayes\\nPerhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In\\nthis classifier, the assumption is that data from each label is drawn from a simple Gaus‐\\nsian distribution. Imagine that you have the following data (Figure 5-38):\\nIn Depth: Naive Bayes Classification \\n| \\n383'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 401}, page_content=\"In[2]: from sklearn.datasets import make_blobs\\n       X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\\n       plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');\\nFigure 5-38. Data for Gaussian naive Bayes classification\\nOne extremely fast way to create a simple model is to assume that the data is\\ndescribed by a Gaussian distribution with no covariance between dimensions. We can\\nfit this model by simply finding the mean and standard deviation of the points within\\neach label, which is all you need to define such a distribution. The result of this naive\\nGaussian assumption is shown in Figure 5-39.\\nFigure 5-39. Visualization of the Gaussian naive Bayes model\\n384 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 402}, page_content=\"The ellipses here represent the Gaussian generative model for each label, with larger\\nprobability toward the center of the ellipses. With this generative model in place for\\neach class, we have a simple recipe to compute the likelihood P features\\nL1  for any\\ndata point, and thus we can quickly compute the posterior ratio and determine which\\nlabel is the most probable for a given point.\\nThis procedure is implemented in Scikit-Learn’s sklearn.naive_bayes.GaussianNB\\nestimator:\\nIn[3]: from sklearn.naive_bayes import GaussianNB\\n       model = GaussianNB()\\n       model.fit(X, y);\\nNow let’s generate some new data and predict the label:\\nIn[4]: rng = np.random.RandomState(0)\\n       Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\\n       ynew = model.predict(Xnew)\\nNow we can plot this new data to get an idea of where the decision boundary is\\n(Figure 5-40):\\nIn[5]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\\n       lim = plt.axis()\\n       plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\\n       plt.axis(lim);\\nFigure 5-40. Visualization of the Gaussian naive Bayes classification\\nWe see a slightly curved boundary in the classifications—in general, the boundary in\\nGaussian naive Bayes is quadratic.\\nA nice piece of this Bayesian formalism is that it naturally allows for probabilistic\\nclassification, which we can compute using the predict_proba method:\\nIn Depth: Naive Bayes Classification \\n| \\n385\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 403}, page_content='In[6]: yprob = model.predict_proba(Xnew)\\n       yprob[-8:].round(2)\\nOut[6]: array([[ 0.89,  0.11],\\n               [ 1.  ,  0.  ],\\n               [ 1.  ,  0.  ],\\n               [ 1.  ,  0.  ],\\n               [ 1.  ,  0.  ],\\n               [ 1.  ,  0.  ],\\n               [ 0.  ,  1.  ],\\n               [ 0.15,  0.85]])\\nThe columns give the posterior probabilities of the first and second label, respectively.\\nIf you are looking for estimates of uncertainty in your classification, Bayesian\\napproaches like this can be a useful approach.\\nOf course, the final classification will only be as good as the model assumptions that\\nlead to it, which is why Gaussian naive Bayes often does not produce very good\\nresults. Still, in many cases—especially as the number of features becomes large—this\\nassumption is not detrimental enough to prevent Gaussian naive Bayes from being a\\nuseful method.\\nMultinomial Naive Bayes\\nThe Gaussian assumption just described is by no means the only simple assumption\\nthat could be used to specify the generative distribution for each label. Another useful\\nexample is multinomial naive Bayes, where the features are assumed to be generated\\nfrom a simple multinomial distribution. The multinomial distribution describes the\\nprobability of observing counts among a number of categories, and thus multinomial\\nnaive Bayes is most appropriate for features that represent counts or count rates.\\nThe idea is precisely the same as before, except that instead of modeling the data dis‐\\ntribution with the best-fit Gaussian, we model the data distribution with a best-fit\\nmultinomial distribution.\\nExample: Classifying text\\nOne place where multinomial naive Bayes is often used is in text classification, where\\nthe features are related to word counts or frequencies within the documents to be\\nclassified. We discussed the extraction of such features from text in “Feature Engi‐\\nneering” on page 375; here we will use the sparse word count features from the 20\\nNewsgroups corpus to show how we might classify these short documents into\\ncategories.\\nLet’s download the data and take a look at the target names:\\nIn[7]: from sklearn.datasets import fetch_20newsgroups\\n386 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 404}, page_content=\"data = fetch_20newsgroups()\\n       data.target_names\\nOut[7]: ['alt.atheism',\\n         'comp.graphics',\\n         'comp.os.ms-windows.misc',\\n         'comp.sys.ibm.pc.hardware',\\n         'comp.sys.mac.hardware',\\n         'comp.windows.x',\\n         'misc.forsale',\\n         'rec.autos',\\n         'rec.motorcycles',\\n         'rec.sport.baseball',\\n         'rec.sport.hockey',\\n         'sci.crypt',\\n         'sci.electronics',\\n         'sci.med',\\n         'sci.space',\\n         'soc.religion.christian',\\n         'talk.politics.guns',\\n         'talk.politics.mideast',\\n         'talk.politics.misc',\\n         'talk.religion.misc']\\nFor simplicity, we will select just a few of these categories, and download the training\\nand testing set:\\nIn[8]:\\ncategories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space',\\n              'comp.graphics']\\ntrain = fetch_20newsgroups(subset='train', categories=categories)\\ntest = fetch_20newsgroups(subset='test', categories=categories)\\nHere is a representative entry from the data:\\nIn[9]: print(train.data[5])\\nFrom: dmcgee@uluhe.soest.hawaii.edu (Don McGee)\\nSubject: Federal Hearing\\nOriginator: dmcgee@uluhe\\nOrganization: School of Ocean and Earth Science and Technology\\nDistribution: usa\\nLines: 10\\nFact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the\\nuse of the bible reading and prayer in public schools 15 years ago is now\\ngoing to appear before the FCC with a petition to stop the reading of the\\nGospel on the airways of America.  And she is also campaigning to remove\\nChristmas programs, songs, etc from the public schools.  If it is true\\nthen mail to Federal Communications Commission 1919 H Street Washington DC\\n20054 expressing your opposition to her request.  Reference Petition number\\n2493.\\nIn Depth: Naive Bayes Classification \\n| \\n387\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 405}, page_content=\"In order to use this data for machine learning, we need to be able to convert the con‐\\ntent of each string into a vector of numbers. For this we will use the TF–IDF vector‐\\nizer (discussed in “Feature Engineering” on page 375), and create a pipeline that\\nattaches it to a multinomial naive Bayes classifier:\\nIn[10]: from sklearn.feature_extraction.text import TfidfVectorizer\\n        from sklearn.naive_bayes import MultinomialNB\\n        from sklearn.pipeline import make_pipeline\\n        model = make_pipeline(TfidfVectorizer(), MultinomialNB())\\nWith this pipeline, we can apply the model to the training data, and predict labels for\\nthe test data:\\nIn[11]: model.fit(train.data, train.target)\\n        labels = model.predict(test.data)\\nNow that we have predicted the labels for the test data, we can evaluate them to learn\\nabout the performance of the estimator. For example, here is the confusion matrix\\nbetween the true and predicted labels for the test data (Figure 5-41):\\nIn[12]:\\nfrom sklearn.metrics import confusion_matrix\\nmat = confusion_matrix(test.target, labels)\\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\\n            xticklabels=train.target_names, yticklabels=train.target_names)\\nplt.xlabel('true label')\\nplt.ylabel('predicted label');\\nFigure 5-41. Confusion matrix for the multinomial naive Bayes text classifier\\n388 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 406}, page_content=\"Evidently, even this very simple classifier can successfully separate space talk from\\ncomputer talk, but it gets confused between talk about religion and talk about Chris‐\\ntianity. This is perhaps an expected area of confusion!\\nThe very cool thing here is that we now have the tools to determine the category for\\nany string, using the predict() method of this pipeline. Here’s a quick utility func‐\\ntion that will return the prediction for a single string:\\nIn[13]: def predict_category(s, train=train, model=model):\\n            pred = model.predict([s])\\n            return train.target_names[pred[0]]\\nLet’s try it out:\\nIn[14]: predict_category('sending a payload to the ISS')\\nOut[14]: 'sci.space'\\nIn[15]: predict_category('discussing islam vs atheism')\\nOut[15]: 'soc.religion.christian'\\nIn[16]: predict_category('determining the screen resolution')\\nOut[16]: 'comp.graphics'\\nRemember that this is nothing more sophisticated than a simple probability model\\nfor the (weighted) frequency of each word in the string; nevertheless, the result is\\nstriking. Even a very naive algorithm, when used carefully and trained on a large set\\nof high-dimensional data, can be surprisingly effective.\\nWhen to Use Naive Bayes\\nBecause naive Bayesian classifiers make such stringent assumptions about data, they\\nwill generally not perform as well as a more complicated model. That said, they have\\nseveral advantages:\\n• They are extremely fast for both training and prediction\\n• They provide straightforward probabilistic prediction\\n• They are often very easily interpretable\\n• They have very few (if any) tunable parameters\\nThese advantages mean a naive Bayesian classifier is often a good choice as an initial\\nbaseline classification. If it performs suitably, then congratulations: you have a very\\nfast, very interpretable classifier for your problem. If it does not perform well, then\\nyou can begin exploring more sophisticated models, with some baseline knowledge of\\nhow well they should perform.\\nNaive Bayes classifiers tend to perform especially well in one of the following\\nsituations:\\nIn Depth: Naive Bayes Classification \\n| \\n389\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 407}, page_content='• When the naive assumptions actually match the data (very rare in practice)\\n• For very well-separated categories, when model complexity is less important\\n• For very high-dimensional data, when model complexity is less important\\nThe last two points seem distinct, but they actually are related: as the dimension of a\\ndataset grows, it is much less likely for any two points to be found close together\\n(after all, they must be close in every single dimension to be close overall). This means\\nthat clusters in high dimensions tend to be more separated, on average, than clusters\\nin low dimensions, assuming the new dimensions actually add information. For this\\nreason, simplistic classifiers like naive Bayes tend to work as well or better than more\\ncomplicated classifiers as the dimensionality grows: once you have enough data, even\\na simple model can be very powerful.\\nIn Depth: Linear Regression\\nJust as naive Bayes (discussed earlier in “In Depth: Naive Bayes Classification” on\\npage 382) is a good starting point for classification tasks, linear regression models are\\na good starting point for regression tasks. Such models are popular because they can\\nbe fit very quickly, and are very interpretable. You are probably familiar with the sim‐\\nplest form of a linear regression model (i.e., fitting a straight line to data), but such\\nmodels can be extended to model more complicated data behavior.\\nIn this section we will start with a quick intuitive walk-through of the mathematics\\nbehind this well-known problem, before moving on to see how linear models can be\\ngeneralized to account for more complicated patterns in data. We begin with the stan‐\\ndard imports:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\n       import numpy as np\\nSimple Linear Regression\\nWe will start with the most familiar linear regression, a straight-line fit to data. A\\nstraight-line fit is a model of the form y = ax + b where a is commonly known as the\\nslope, and b is commonly known as the intercept.\\nConsider the following data, which is scattered about a line with a slope of 2 and an\\nintercept of –5 (Figure 5-42):\\nIn[2]: rng = np.random.RandomState(1)\\n       x = 10 * rng.rand(50)\\n       y = 2 * x - 5 + rng.randn(50)\\n       plt.scatter(x, y);\\n390 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 408}, page_content='Figure 5-42. Data for linear regression\\nWe can use Scikit-Learn’s LinearRegression estimator to fit this data and construct\\nthe best-fit line (Figure 5-43):\\nIn[3]: from sklearn.linear_model import LinearRegression\\n       model = LinearRegression(fit_intercept=True)\\n       model.fit(x[:, np.newaxis], y)\\n       xfit = np.linspace(0, 10, 1000)\\n       yfit = model.predict(xfit[:, np.newaxis])\\n       plt.scatter(x, y)\\n       plt.plot(xfit, yfit);\\nFigure 5-43. A linear regression model\\nIn Depth: Linear Regression \\n| \\n391'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 409}, page_content='The slope and intercept of the data are contained in the model’s fit parameters, which\\nin Scikit-Learn are always marked by a trailing underscore. Here the relevant parame‐\\nters are coef_ and intercept_:\\nIn[4]: print(\"Model slope:    \", model.coef_[0])\\n       print(\"Model intercept:\", model.intercept_)\\nModel slope:     2.02720881036\\nModel intercept: -4.99857708555\\nWe see that the results are very close to the inputs, as we might hope.\\nThe LinearRegression estimator is much more capable than this, however—in addi‐\\ntion to simple straight-line fits, it can also handle multidimensional linear models of\\nthe form:\\ny = a0 + a1x1 + a2x2 + ⋯\\nwhere there are multiple x values. Geometrically, this is akin to fitting a plane to\\npoints in three dimensions, or fitting a hyper-plane to points in higher dimensions.\\nThe multidimensional nature of such regressions makes them more difficult to visu‐\\nalize, but we can see one of these fits in action by building some example data, using\\nNumPy’s matrix multiplication operator:\\nIn[5]: rng = np.random.RandomState(1)\\n       X = 10 * rng.rand(100, 3)\\n       y = 0.5 + np.dot(X, [1.5, -2., 1.])\\n       model.fit(X, y)\\n       print(model.intercept_)\\n       print(model.coef_)\\n0.5\\n[ 1.5 -2.   1. ]\\nHere the y data is constructed from three random x values, and the linear regression\\nrecovers the coefficients used to construct the data.\\nIn this way, we can use the single LinearRegression estimator to fit lines, planes, or\\nhyperplanes to our data. It still appears that this approach would be limited to strictly\\nlinear relationships between variables, but it turns out we can relax this as well.\\nBasis Function Regression\\nOne trick you can use to adapt linear regression to nonlinear relationships between\\nvariables is to transform the data according to basis functions. We have seen one ver‐\\nsion of this before, in the PolynomialRegression pipeline used in “Hyperparameters\\n392 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 410}, page_content='and Model Validation” on page 359 and “Feature Engineering” on page 375. The idea\\nis to take our multidimensional linear model:\\ny = a0 + a1x1 + a2x2 + a3x3 + ⋯\\nand build the x1, x2, x3, and so on from our single-dimensional input x. That is, we let\\nxn = f n x , where f n  is some function that transforms our data.\\nFor example, if f n x = xn, our model becomes a polynomial regression:\\ny = a0 + a1x + a2x2 + a3x3 + ⋯\\nNotice that this is still a linear model—the linearity refers to the fact that the coeffi‐\\ncients an never multiply or divide each other. What we have effectively done is taken\\nour one-dimensional x values and projected them into a higher dimension, so that a\\nlinear fit can fit more complicated relationships between x and y.\\nPolynomial basis functions\\nThis polynomial projection is useful enough that it is built into Scikit-Learn, using\\nthe PolynomialFeatures transformer:\\nIn[6]: from sklearn.preprocessing import PolynomialFeatures\\n       x = np.array([2, 3, 4])\\n       poly = PolynomialFeatures(3, include_bias=False)\\n       poly.fit_transform(x[:, None])\\nOut[6]: array([[  2.,   4.,   8.],\\n               [  3.,   9.,  27.],\\n               [  4.,  16.,  64.]])\\nWe see here that the transformer has converted our one-dimensional array into a\\nthree-dimensional array by taking the exponent of each value. This new, higher-\\ndimensional data representation can then be plugged into a linear regression.\\nAs we saw in “Feature Engineering” on page 375, the cleanest way to accomplish this\\nis to use a pipeline. Let’s make a 7th-degree polynomial model in this way:\\nIn[7]: from sklearn.pipeline import make_pipeline\\n       poly_model = make_pipeline(PolynomialFeatures(7),\\n                                  LinearRegression())\\nWith this transform in place, we can use the linear model to fit much more compli‐\\ncated relationships between x and y. For example, here is a sine wave with noise\\n(Figure 5-44):\\nIn Depth: Linear Regression \\n| \\n393'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 411}, page_content='In[8]: rng = np.random.RandomState(1)\\n       x = 10 * rng.rand(50)\\n       y = np.sin(x) + 0.1 * rng.randn(50)\\n       poly_model.fit(x[:, np.newaxis], y)\\n       yfit = poly_model.predict(xfit[:, np.newaxis])\\n       plt.scatter(x, y)\\n       plt.plot(xfit, yfit);\\nFigure 5-44. A linear polynomial fit to nonlinear training data\\nOur linear model, through the use of 7th-order polynomial basis functions, can pro‐\\nvide an excellent fit to this nonlinear data!\\nGaussian basis functions\\nOf course, other basis functions are possible. For example, one useful pattern is to fit\\na model that is not a sum of polynomial bases, but a sum of Gaussian bases. The\\nresult might look something like Figure 5-45.\\n394 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 412}, page_content='Figure 5-45. A Gaussian basis function fit to nonlinear data\\nThe shaded regions in the plot shown in Figure 5-45 are the scaled basis functions,\\nand when added together they reproduce the smooth curve through the data. These\\nGaussian basis functions are not built into Scikit-Learn, but we can write a custom\\ntransformer that will create them, as shown here and illustrated in Figure 5-46\\n(Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn’s\\nsource is a good way to see how they can be created):\\nIn[9]:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\\n    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\\n    def __init__(self, N, width_factor=2.0):\\n        self.N = N\\n        self.width_factor = width_factor\\n    @staticmethod\\n    def _gauss_basis(x, y, width, axis=None):\\n        arg = (x - y) / width\\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\\n    def fit(self, X, y=None):\\n        # create N centers spread along the data range\\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\\n        return self\\n    def transform(self, X):\\nIn Depth: Linear Regression \\n| \\n395'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 413}, page_content='return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\\n                                 self.width_, axis=1)\\ngauss_model = make_pipeline(GaussianFeatures(20),\\n                            LinearRegression())\\ngauss_model.fit(x[:, np.newaxis], y)\\nyfit = gauss_model.predict(xfit[:, np.newaxis])\\nplt.scatter(x, y)\\nplt.plot(xfit, yfit)\\nplt.xlim(0, 10);\\nFigure 5-46. A Gaussian basis function fit computed with a custom transformer\\nWe put this example here just to make clear that there is nothing magic about poly‐\\nnomial basis functions: if you have some sort of intuition into the generating process\\nof your data that makes you think one basis or another might be appropriate, you can\\nuse them as well.\\nRegularization\\nThe introduction of basis functions into our linear regression makes the model much\\nmore flexible, but it also can very quickly lead to overfitting (refer back to “Hyper‐\\nparameters and Model Validation” on page 359 for a discussion of this). For example,\\nif we choose too many Gaussian basis functions, we end up with results that don’t\\nlook so good (Figure 5-47):\\nIn[10]: model = make_pipeline(GaussianFeatures(30),\\n                              LinearRegression())\\n        model.fit(x[:, np.newaxis], y)\\n        plt.scatter(x, y)\\n        plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\\n396 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 414}, page_content=\"plt.xlim(0, 10)\\n        plt.ylim(-1.5, 1.5);\\nFigure 5-47. An overly complex basis function model that overfits the data\\nWith the data projected to the 30-dimensional basis, the model has far too much flex‐\\nibility and goes to extreme values between locations where it is constrained by data.\\nWe can see the reason for this if we plot the coefficients of the Gaussian bases with\\nrespect to their locations (Figure 5-48):\\nIn[11]: def basis_plot(model, title=None):\\n            fig, ax = plt.subplots(2, sharex=True)\\n            model.fit(x[:, np.newaxis], y)\\n            ax[0].scatter(x, y)\\n            ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\\n            ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\\n            if title:\\n                ax[0].set_title(title)\\n            ax[1].plot(model.steps[0][1].centers_,\\n                       model.steps[1][1].coef_)\\n            ax[1].set(xlabel='basis location',\\n                      ylabel='coefficient',\\n                      xlim=(0, 10))\\n        model = make_pipeline(GaussianFeatures(30), LinearRegression())\\n        basis_plot(model)\\nIn Depth: Linear Regression \\n| \\n397\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 415}, page_content=\"Figure 5-48. The coefficients of the Gaussian bases in the overly complex model\\nThe lower panel in Figure 5-48 shows the amplitude of the basis function at each\\nlocation. This is typical overfitting behavior when basis functions overlap: the coeffi‐\\ncients of adjacent basis functions blow up and cancel each other out. We know that\\nsuch behavior is problematic, and it would be nice if we could limit such spikes\\nexplicitly in the model by penalizing large values of the model parameters. Such a\\npenalty is known as regularization, and comes in several forms.\\nRidge regression (L2 regularization)\\nPerhaps the most common form of regularization is known as ridge regression or L2\\nregularization, sometimes also called Tikhonov regularization. This proceeds by penal‐\\nizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty\\non the model fit would be:\\nP = α∑n = 1\\nN\\nθn\\n2\\nwhere α is a free parameter that controls the strength of the penalty. This type of\\npenalized model is built into Scikit-Learn with the Ridge estimator (Figure 5-49):\\nIn[12]: from sklearn.linear_model import Ridge\\n        model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\\n        basis_plot(model, title='Ridge Regression')\\n398 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 416}, page_content=\"Figure 5-49. Ridge (L2) regularization applied to the overly complex model (compare to\\nFigure 5-48)\\nThe α parameter is essentially a knob controlling the complexity of the resulting\\nmodel. In the limit α\\n0, we recover the standard linear regression result; in the\\nlimit α\\n∞, all model responses will be suppressed. One advantage of ridge regres‐\\nsion in particular is that it can be computed very efficiently—at hardly more compu‐\\ntational cost than the original linear regression model.\\nLasso regularization (L1)\\nAnother very common type of regularization is known as lasso, and involves penaliz‐\\ning the sum of absolute values (1-norms) of regression coefficients:\\nP = α∑n = 1\\nN\\nθn\\nThough this is conceptually very similar to ridge regression, the results can differ sur‐\\nprisingly: for example, due to geometric reasons lasso regression tends to favor sparse\\nmodels where possible; that is, it preferentially sets model coefficients to exactly zero.\\nWe can see this behavior in duplicating the plot shown in Figure 5-49, but using L1-\\nnormalized coefficients (Figure 5-50):\\nIn[13]: from sklearn.linear_model import Lasso\\n        model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\\n        basis_plot(model, title='Lasso Regression')\\nIn Depth: Linear Regression \\n| \\n399\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 417}, page_content='Figure 5-50. Lasso (L1) regularization applied to the overly complex model (compare to\\nFigure 5-48)\\nWith the lasso regression penalty, the majority of the coefficients are exactly zero,\\nwith the functional behavior being modeled by a small subset of the available basis\\nfunctions. As with ridge regularization, the α parameter tunes the strength of the\\npenalty, and should be determined via, for example, cross-validation (refer back to\\n“Hyperparameters and Model Validation” on page 359 for a discussion of this).\\nExample: Predicting Bicycle Traffic\\nAs an example, let’s take a look at whether we can predict the number of bicycle trips\\nacross Seattle’s Fremont Bridge based on weather, season, and other factors. We have\\nseen this data already in “Working with Time Series” on page 188.\\nIn this section, we will join the bike data with another dataset, and try to determine\\nthe extent to which weather and seasonal factors—temperature, precipitation, and\\ndaylight hours—affect the volume of bicycle traffic through this corridor. Fortunately,\\nthe NOAA makes available their daily weather station data (I used station ID\\nUSW00024233) and we can easily use Pandas to join the two data sources. We will\\nperform a simple linear regression to relate weather and other information to bicycle\\ncounts, in order to estimate how a change in any one of these parameters affects the\\nnumber of riders on a given day.\\nIn particular, this is an example of how the tools of Scikit-Learn can be used in a stat‐\\nistical modeling framework, in which the parameters of the model are assumed to\\nhave interpretable meaning. As discussed previously, this is not a standard approach\\nwithin machine learning, but such interpretation is possible for some models.\\n400 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 418}, page_content='Let’s start by loading the two datasets, indexing by date:\\nIn[14]:\\nimport pandas as pd\\ncounts = pd.read_csv(\\'fremont_hourly.csv\\', index_col=\\'Date\\', parse_dates=True)\\nweather = pd.read_csv(\\'599021.csv\\', index_col=\\'DATE\\', parse_dates=True)\\nNext we will compute the total daily bicycle traffic, and put this in its own DataFrame:\\nIn[15]: daily = counts.resample(\\'d\\', how=\\'sum\\')\\n        daily[\\'Total\\'] = daily.sum(axis=1)\\n        daily = daily[[\\'Total\\']] # remove other columns\\nWe saw previously that the patterns of use generally vary from day to day; let’s\\naccount for this in our data by adding binary columns that indicate the day of the\\nweek:\\nIn[16]: days = [\\'Mon\\', \\'Tue\\', \\'Wed\\', \\'Thu\\', \\'Fri\\', \\'Sat\\', \\'Sun\\']\\n        for i in range(7):\\n            daily[days[i]] = (daily.index.dayofweek == i).astype(float)\\nSimilarly, we might expect riders to behave differently on holidays; let’s add an indica‐\\ntor of this as well:\\nIn[17]: from pandas.tseries.holiday import USFederalHolidayCalendar\\n        cal = USFederalHolidayCalendar()\\n        holidays = cal.holidays(\\'2012\\', \\'2016\\')\\n        daily = daily.join(pd.Series(1, index=holidays, name=\\'holiday\\'))\\n        daily[\\'holiday\\'].fillna(0, inplace=True)\\nWe also might suspect that the hours of daylight would affect how many people ride;\\nlet’s use the standard astronomical calculation to add this information (Figure 5-51):\\nIn[18]: def hours_of_daylight(date, axis=23.44, latitude=47.61):\\n            \"\"\"Compute the hours of daylight for the given date\"\"\"\\n            days = (date - pd.datetime(2000, 12, 21)).days\\n            m = (1. - np.tan(np.radians(latitude))\\n                 * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\\n            return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\\n        daily[\\'daylight_hrs\\'] = list(map(hours_of_daylight, daily.index))\\n        daily[[\\'daylight_hrs\\']].plot();\\nIn Depth: Linear Regression \\n| \\n401'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 419}, page_content=\"Figure 5-51. Visualization of hours of daylight in Seattle\\nWe can also add the average temperature and total precipitation to the data. In addi‐\\ntion to the inches of precipitation, let’s add a flag that indicates whether a day is dry\\n(has zero precipitation):\\nIn[19]: # temperatures are in 1/10 deg C; convert to C\\n        weather['TMIN'] /= 10\\n        weather['TMAX'] /= 10\\n        weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\\n        # precip is in 1/10 mm; convert to inches\\n        weather['PRCP'] /= 254\\n        weather['dry day'] = (weather['PRCP'] == 0).astype(int)\\n        daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])\\nFinally, let’s add a counter that increases from day 1, and measures how many years\\nhave passed. This will let us measure any observed annual increase or decrease in\\ndaily crossings:\\nIn[20]: daily['annual'] = (daily.index - daily.index[0]).days / 365.\\nNow our data is in order, and we can take a look at it:\\nIn[21]: daily.head()\\nOut[21]:\\n            Total  Mon  Tue  Wed  Thu  Fri  Sat  Sun  holiday  daylight_hrs  \\\\\\\\\\nDate\\n2012-10-03   3521    0    0    1    0    0    0    0        0     11.277359\\n2012-10-04   3475    0    0    0    1    0    0    0        0     11.219142\\n2012-10-05   3148    0    0    0    0    1    0    0        0     11.161038\\n2012-10-06   2006    0    0    0    0    0    1    0        0     11.103056\\n402 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 420}, page_content=\"2012-10-07   2142    0    0    0    0    0    0    1        0     11.045208\\n            PRCP  Temp (C)  dry day    annual\\nDate\\n2012-10-03     0     13.35        1  0.000000\\n2012-10-04     0     13.60        1  0.002740\\n2012-10-05     0     15.30        1  0.005479\\n2012-10-06     0     15.85        1  0.008219\\n2012-10-07     0     15.85        1  0.010959\\nWith this in place, we can choose the columns to use, and fit a linear regression\\nmodel to our data. We will set fit_intercept = False, because the daily flags essen‐\\ntially operate as their own day-specific intercepts:\\nIn[22]:\\ncolumn_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\\n                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\\nX = daily[column_names]\\ny = daily['Total']\\nmodel = LinearRegression(fit_intercept=False)\\nmodel.fit(X, y)\\ndaily['predicted'] = model.predict(X)\\nFinally, we can compare the total and predicted bicycle traffic visually (Figure 5-52):\\nIn[23]: daily[['Total', 'predicted']].plot(alpha=0.5);\\nFigure 5-52. Our model’s prediction of bicycle traffic\\nIt is evident that we have missed some key features, especially during the summer\\ntime. Either our features are not complete (i.e., people decide whether to ride to work\\nbased on more than just these) or there are some nonlinear relationships that we have\\nIn Depth: Linear Regression \\n| \\n403\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 421}, page_content=\"failed to take into account (e.g., perhaps people ride less at both high and low temper‐\\natures). Nevertheless, our rough approximation is enough to give us some insights,\\nand we can take a look at the coefficients of the linear model to estimate how much\\neach feature contributes to the daily bicycle count:\\nIn[24]: params = pd.Series(model.coef_, index=X.columns)\\n        params\\nOut[24]: Mon              503.797330\\n         Tue              612.088879\\n         Wed              591.611292\\n         Thu              481.250377\\n         Fri              176.838999\\n         Sat            -1104.321406\\n         Sun            -1134.610322\\n         holiday        -1187.212688\\n         daylight_hrs     128.873251\\n         PRCP            -665.185105\\n         dry day          546.185613\\n         Temp (C)          65.194390\\n         annual            27.865349\\n         dtype: float64\\nThese numbers are difficult to interpret without some measure of their uncertainty.\\nWe can compute these uncertainties quickly using bootstrap resamplings of the data:\\nIn[25]: from sklearn.utils import resample\\n        np.random.seed(1)\\n        err = np.std([model.fit(*resample(X, y)).coef_\\n                      for i in range(1000)], 0)\\nWith these errors estimated, let’s again look at the results:\\nIn[26]: print(pd.DataFrame({'effect': params.round(0),\\n                            'error': err.round(0)}))\\n              effect  error\\nMon              504     85\\nTue              612     82\\nWed              592     82\\nThu              481     85\\nFri              177     81\\nSat            -1104     79\\nSun            -1135     82\\nholiday        -1187    164\\ndaylight_hrs     129      9\\nPRCP            -665     62\\ndry day          546     33\\nTemp (C)          65      4\\nannual            28     18\\nWe first see that there is a relatively stable trend in the weekly baseline: there are\\nmany more riders on weekdays than on weekends and holidays. We see that for each\\n404 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 422}, page_content='additional hour of daylight, 129 ± 9 more people choose to ride; a temperature\\nincrease of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry\\nday means an average of 546 ± 33 more riders; and each inch of precipitation means\\n665 ± 62 more people leave their bike at home. Once all these effects are accounted\\nfor, we see a modest increase of 28 ± 18 new daily riders each year.\\nOur model is almost certainly missing some relevant information. For example, non‐\\nlinear effects (such as effects of precipitation and cold temperature) and nonlinear\\ntrends within each variable (such as disinclination to ride at very cold and very hot\\ntemperatures) cannot be accounted for in this model. Additionally, we have thrown\\naway some of the finer-grained information (such as the difference between a rainy\\nmorning and a rainy afternoon), and we have ignored correlations between days\\n(such as the possible effect of a rainy Tuesday on Wednesday’s numbers, or the effect\\nof an unexpected sunny day after a streak of rainy days). These are all potentially\\ninteresting effects, and you now have the tools to begin exploring them if you wish!\\nIn-Depth: Support Vector Machines\\nSupport vector machines (SVMs) are a particularly powerful and flexible class of\\nsupervised algorithms for both classification and regression. In this section, we will\\ndevelop the intuition behind support vector machines and their use in classification\\nproblems. We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       from scipy import stats\\n       # use Seaborn plotting defaults\\n       import seaborn as sns; sns.set()\\nMotivating Support Vector Machines\\nAs part of our discussion of Bayesian classification (see “In Depth: Naive Bayes Clas‐\\nsification” on page 382), we learned a simple model describing the distribution of\\neach underlying class, and used these generative models to probabilistically deter‐\\nmine labels for new points. That was an example of generative classification; here we\\nwill consider instead discriminative classification: rather than modeling each class, we\\nsimply find a line or curve (in two dimensions) or manifold (in multiple dimensions)\\nthat divides the classes from each other.\\nAs an example of this, consider the simple case of a classification task, in which the\\ntwo classes of points are well separated (Figure 5-53):\\nIn-Depth: Support Vector Machines \\n| \\n405'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 423}, page_content=\"In[2]: from sklearn.datasets.samples_generator import make_blobs\\n       X, y = make_blobs(n_samples=50, centers=2,\\n                         random_state=0, cluster_std=0.60)\\n       plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\\nFigure 5-53. Simple data for classification\\nA linear discriminative classifier would attempt to draw a straight line separating the\\ntwo sets of data, and thereby create a model for classification. For two-dimensional\\ndata like that shown here, this is a task we could do by hand. But immediately we see\\na problem: there is more than one possible dividing line that can perfectly discrimi‐\\nnate between the two classes!\\nWe can draw them as follows (Figure 5-54):\\nIn[3]: xfit = np.linspace(-1, 3.5)\\n       plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n       plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\\n       for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\\n           plt.plot(xfit, m * xfit + b, '-k')\\n       plt.xlim(-1, 3.5);\\n406 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 424}, page_content=\"Figure 5-54. Three perfect linear discriminative classifiers for our data\\nThese are three very different separators that, nevertheless, perfectly discriminate\\nbetween these samples. Depending on which you choose, a new data point (e.g., the\\none marked by the “X” in Figure 5-54) will be assigned a different label! Evidently our\\nsimple intuition of “drawing a line between classes” is not enough, and we need to\\nthink a bit deeper.\\nSupport Vector Machines: Maximizing the Margin\\nSupport vector machines offer one way to improve on this. The intuition is this:\\nrather than simply drawing a zero-width line between the classes, we can draw\\naround each line a margin of some width, up to the nearest point. Here is an example\\nof how this might look (Figure 5-55):\\nIn[4]:\\nxfit = np.linspace(-1, 3.5)\\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\nfor m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\\n    yfit = m * xfit + b\\n    plt.plot(xfit, yfit, '-k')\\n    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA',\\n                     alpha=0.4)\\nplt.xlim(-1, 3.5);\\nIn-Depth: Support Vector Machines \\n| \\n407\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 425}, page_content='Figure 5-55. Visualization of “margins” within discriminative classifiers\\nIn support vector machines, the line that maximizes this margin is the one we will\\nchoose as the optimal model. Support vector machines are an example of such a max‐\\nimum margin estimator.\\nFitting a support vector machine\\nLet’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector\\nclassifier to train an SVM model on this data. For the time being, we will use a linear\\nkernel and set the C parameter to a very large number (we’ll discuss the meaning of\\nthese in more depth momentarily):\\nIn[5]: from sklearn.svm import SVC # \"Support vector classifier\"\\n       model = SVC(kernel=\\'linear\\', C=1E10)\\n       model.fit(X, y)\\nOut[5]: SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n          decision_function_shape=None, degree=3, gamma=\\'auto\\', kernel=\\'linear\\',\\n          max_iter=-1, probability=False, random_state=None, shrinking=True,\\n          tol=0.001, verbose=False)\\nTo better visualize what’s happening here, let’s create a quick convenience function\\nthat will plot SVM decision boundaries for us (Figure 5-56):\\nIn[6]: def plot_svc_decision_function(model, ax=None, plot_support=True):\\n           \"\"\"Plot the decision function for a two-dimensional SVC\"\"\"\\n           if ax is None:\\n               ax = plt.gca()\\n           xlim = ax.get_xlim()\\n           ylim = ax.get_ylim()\\n           # create grid to evaluate model\\n           x = np.linspace(xlim[0], xlim[1], 30)\\n408 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 426}, page_content=\"y = np.linspace(ylim[0], ylim[1], 30)\\n           Y, X = np.meshgrid(y, x)\\n           xy = np.vstack([X.ravel(), Y.ravel()]).T\\n           P = model.decision_function(xy).reshape(X.shape)\\n           # plot decision boundary and margins\\n           ax.contour(X, Y, P, colors='k',\\n                      levels=[-1, 0, 1], alpha=0.5,\\n                      linestyles=['--', '-', '--'])\\n           # plot support vectors\\n           if plot_support:\\n               ax.scatter(model.support_vectors_[:, 0],\\n                          model.support_vectors_[:, 1],\\n                          s=300, linewidth=1, facecolors='none');\\n           ax.set_xlim(xlim)\\n           ax.set_ylim(ylim)\\nIn[7]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n       plot_svc_decision_function(model);\\nFigure 5-56. A support vector machine classifier fit to the data, with margins (dashed\\nlines) and support vectors (circles) shown\\nThis is the dividing line that maximizes the margin between the two sets of points.\\nNotice that a few of the training points just touch the margin; they are indicated by\\nthe black circles in Figure 5-56. These points are the pivotal elements of this fit, and\\nare known as the support vectors, and give the algorithm its name. In Scikit-Learn, the\\nidentity of these points is stored in the support_vectors_ attribute of the classifier:\\nIn[8]: model.support_vectors_\\nOut[8]: array([[ 0.44359863,  3.11530945],\\n               [ 2.33812285,  3.43116792],\\n               [ 2.06156753,  1.96918596]])\\nIn-Depth: Support Vector Machines \\n| \\n409\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 427}, page_content=\"A key to this classifier’s success is that for the fit, only the position of the support vec‐\\ntors matters; any points further from the margin that are on the correct side do not\\nmodify the fit! Technically, this is because these points do not contribute to the loss\\nfunction used to fit the model, so their position and number do not matter so long as\\nthey do not cross the margin.\\nWe can see this, for example, if we plot the model learned from the first 60 points and\\nfirst 120 points of this dataset (Figure 5-57):\\nIn[9]: def plot_svm(N=10, ax=None):\\n           X, y = make_blobs(n_samples=200, centers=2,\\n                             random_state=0, cluster_std=0.60)\\n           X = X[:N]\\n           y = y[:N]\\n           model = SVC(kernel='linear', C=1E10)\\n           model.fit(X, y)\\n           ax = ax or plt.gca()\\n           ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n           ax.set_xlim(-1, 4)\\n           ax.set_ylim(-1, 6)\\n           plot_svc_decision_function(model, ax)\\n       fig, ax = plt.subplots(1, 2, figsize=(16, 6))\\n       fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\\n       for axi, N in zip(ax, [60, 120]):\\n           plot_svm(N, axi)\\n           axi.set_title('N = {0}'.format(N))\\nFigure 5-57. The influence of new training points on the SVM model\\nIn the left panel, we see the model and the support vectors for 60 training points. In\\nthe right panel, we have doubled the number of training points, but the model has\\nnot changed: the three support vectors from the left panel are still the support vectors\\nfrom the right panel. This insensitivity to the exact behavior of distant points is one of\\nthe strengths of the SVM model.\\n410 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 428}, page_content=\"If you are running this notebook live, you can use IPython’s interactive widgets to\\nview this feature of the SVM model interactively (Figure 5-58):\\nIn[10]: from ipywidgets import interact, fixed\\n        interact(plot_svm, N=[10, 200], ax=fixed(None));\\nFigure 5-58. The first frame of the interactive SVM visualization (see the online appen‐\\ndix for the full version)\\nBeyond linear boundaries: Kernel SVM\\nWhere SVM becomes extremely powerful is when it is combined with kernels. We\\nhave seen a version of kernels before, in the basis function regressions of “In Depth:\\nLinear Regression” on page 390. There we projected our data into higher-dimensional\\nspace defined by polynomials and Gaussian basis functions, and thereby were able to\\nfit for nonlinear relationships with a linear classifier.\\nIn SVM models, we can use a version of the same idea. To motivate the need for ker‐\\nnels, let’s look at some data that is not linearly separable (Figure 5-59):\\nIn[11]: from sklearn.datasets.samples_generator import make_circles\\n        X, y = make_circles(100, factor=.1, noise=.1)\\n        clf = SVC(kernel='linear').fit(X, y)\\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n        plot_svc_decision_function(clf, plot_support=False);\\nIn-Depth: Support Vector Machines \\n| \\n411\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 429}, page_content=\"Figure 5-59. A linear classifier performs poorly for nonlinear boundaries\\nIt is clear that no linear discrimination will ever be able to separate this data. But we\\ncan draw a lesson from the basis function regressions in “In Depth: Linear Regres‐\\nsion” on page 390, and think about how we might project the data into a higher\\ndimension such that a linear separator would be sufficient. For example, one simple\\nprojection we could use would be to compute a radial basis function centered on the\\nmiddle clump:\\nIn[12]: r = np.exp(-(X ** 2).sum(1))\\nWe can visualize this extra data dimension using a three-dimensional plot—if you are\\nrunning this notebook live, you will be able to use the sliders to rotate the plot\\n(Figure 5-60):\\nIn[13]: from mpl_toolkits import mplot3d\\n        def plot_3D(elev=30, azim=30, X=X, y=y):\\n            ax = plt.subplot(projection='3d')\\n            ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\\n            ax.view_init(elev=elev, azim=azim)\\n            ax.set_xlabel('x')\\n            ax.set_ylabel('y')\\n            ax.set_zlabel('r')\\n        interact(plot_3D, elev=[-90, 90], azip=(-180, 180),\\n                 X=fixed(X), y=fixed(y));\\n412 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 430}, page_content=\"Figure 5-60. A third dimension added to the data allows for linear separation\\nWe can see that with this additional dimension, the data becomes trivially linearly\\nseparable, by drawing a separating plane at, say, r=0.7.\\nHere we had to choose and carefully tune our projection; if we had not centered our\\nradial basis function in the right location, we would not have seen such clean, linearly\\nseparable results. In general, the need to make such a choice is a problem: we would\\nlike to somehow automatically find the best basis functions to use.\\nOne strategy to this end is to compute a basis function centered at every point in the\\ndataset, and let the SVM algorithm sift through the results. This type of basis function\\ntransformation is known as a kernel transformation, as it is based on a similarity rela‐\\ntionship (or kernel) between each pair of points.\\nA potential problem with this strategy—projecting N points into N dimensions—is\\nthat it might become very computationally intensive as N grows large. However,\\nbecause of a neat little procedure known as the kernel trick, a fit on kernel-\\ntransformed data can be done implicitly—that is, without ever building the full N-\\ndimensional representation of the kernel projection! This kernel trick is built into the\\nSVM, and is one of the reasons the method is so powerful.\\nIn Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to\\nan RBF (radial basis function) kernel, using the kernel model hyperparameter\\n(Figure 5-61):\\nIn[14]: clf = SVC(kernel='rbf', C=1E6)\\n        clf.fit(X, y)\\nOut[14]: SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,\\n           decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\\n           max_iter=-1, probability=False, random_state=None, shrinking=True,\\n           tol=0.001, verbose=False)\\nIn-Depth: Support Vector Machines \\n| \\n413\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 431}, page_content=\"In[15]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n        plot_svc_decision_function(clf)\\n        plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\\n                    s=300, lw=1, facecolors='none');\\nFigure 5-61. Kernel SVM fit to the data\\nUsing this kernelized support vector machine, we learn a suitable nonlinear decision\\nboundary. This kernel transformation strategy is used often in machine learning to\\nturn fast linear methods into fast nonlinear methods, especially for models in which\\nthe kernel trick can be used.\\nTuning the SVM: Softening margins\\nOur discussion so far has centered on very clean datasets, in which a perfect decision\\nboundary exists. But what if your data has some amount of overlap? For example, you\\nmay have data like this (Figure 5-62):\\nIn[16]: X, y = make_blobs(n_samples=100, centers=2,\\n                          random_state=0, cluster_std=1.2)\\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\\n414 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 432}, page_content=\"Figure 5-62. Data with some level of overlap\\nTo handle this case, the SVM implementation has a bit of a fudge-factor that “softens”\\nthe margin; that is, it allows some of the points to creep into the margin if that allows\\na better fit. The hardness of the margin is controlled by a tuning parameter, most\\noften known as C. For very large C, the margin is hard, and points cannot lie in it. For\\nsmaller C, the margin is softer, and can grow to encompass some points.\\nThe plot shown in Figure 5-63 gives a visual picture of how a changing C parameter\\naffects the final fit, via the softening of the margin:\\nIn[17]: X, y = make_blobs(n_samples=100, centers=2,\\n                          random_state=0, cluster_std=0.8)\\n        fig, ax = plt.subplots(1, 2, figsize=(16, 6))\\n        fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\\n        for axi, C in zip(ax, [10.0, 0.1]):\\n            model = SVC(kernel='linear', C=C).fit(X, y)\\n            axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\\n            plot_svc_decision_function(model, axi)\\n            axi.scatter(model.support_vectors_[:, 0],\\n                        model.support_vectors_[:, 1],\\n                        s=300, lw=1, facecolors='none');\\n            axi.set_title('C = {0:.1f}'.format(C), size=14)\\nIn-Depth: Support Vector Machines \\n| \\n415\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 433}, page_content=\"Figure 5-63. The effect of the C parameter on the support vector fit\\nThe optimal value of the C parameter will depend on your dataset, and should be\\ntuned via cross-validation or a similar procedure (refer back to “Hyperparameters\\nand Model Validation” on page 359 for further information).\\nExample: Face Recognition\\nAs an example of support vector machines in action, let’s take a look at the facial rec‐\\nognition problem. We will use the Labeled Faces in the Wild dataset, which consists\\nof several thousand collated photos of various public figures. A fetcher for the dataset\\nis built into Scikit-Learn:\\nIn[18]: from sklearn.datasets import fetch_lfw_people\\n        faces = fetch_lfw_people(min_faces_per_person=60)\\n        print(faces.target_names)\\n        print(faces.images.shape)\\n['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\\n 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\\n(1348, 62, 47)\\nLet’s plot a few of these faces to see what we’re working with (Figure 5-64):\\nIn[19]: fig, ax = plt.subplots(3, 5)\\n        for i, axi in enumerate(ax.flat):\\n            axi.imshow(faces.images[i], cmap='bone')\\n            axi.set(xticks=[], yticks=[],\\n                    xlabel=faces.target_names[faces.target[i]])\\n416 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 434}, page_content=\"Figure 5-64. Examples from the Labeled Faces in the Wild dataset\\nEach image contains [62×47] or nearly 3,000 pixels. We could proceed by simply\\nusing each pixel value as a feature, but often it is more effective to use some sort of\\npreprocessor to extract more meaningful features; here we will use a principal com‐\\nponent analysis (see “In Depth: Principal Component Analysis” on page 433) to\\nextract 150 fundamental components to feed into our support vector machine classi‐\\nfier. We can do this most straightforwardly by packaging the preprocessor and the\\nclassifier into a single pipeline:\\nIn[20]: from sklearn.svm import SVC\\n        from sklearn.decomposition import RandomizedPCA\\n        from sklearn.pipeline import make_pipeline\\n        pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\\n        svc = SVC(kernel='rbf', class_weight='balanced')\\n        model = make_pipeline(pca, svc)\\nFor the sake of testing our classifier output, we will split the data into a training and\\ntesting set:\\nIn[21]: from sklearn.cross_validation import train_test_split\\n        Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\\n                                                        random_state=42)\\nFinally, we can use a grid search cross-validation to explore combinations of parame‐\\nters. Here we will adjust C (which controls the margin hardness) and gamma (which\\ncontrols the size of the radial basis function kernel), and determine the best model:\\nIn[22]: from sklearn.grid_search import GridSearchCV\\n        param_grid = {'svc__C': [1, 5, 10, 50],\\n                      'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\\n        grid = GridSearchCV(model, param_grid)\\nIn-Depth: Support Vector Machines \\n| \\n417\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 435}, page_content=\"%time grid.fit(Xtrain, ytrain)\\n        print(grid.best_params_)\\nCPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s\\nWall time: 26 s\\n{'svc__gamma': 0.001, 'svc__C': 10}\\nThe optimal values fall toward the middle of our grid; if they fell at the edges, we\\nwould want to expand the grid to make sure we have found the true optimum.\\nNow with this cross-validated model, we can predict the labels for the test data, which\\nthe model has not yet seen:\\nIn[23]: model = grid.best_estimator_\\n        yfit = model.predict(Xtest)\\nLet’s take a look at a few of the test images along with their predicted values\\n(Figure 5-65):\\nIn[24]: fig, ax = plt.subplots(4, 6)\\n        for i, axi in enumerate(ax.flat):\\n            axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\\n            axi.set(xticks=[], yticks=[])\\n            axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\\n                           color='black' if yfit[i] == ytest[i] else 'red')\\n        fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);\\nFigure 5-65. Labels predicted by our model\\nOut of this small sample, our optimal estimator mislabeled only a single face (Bush’s\\nface in the bottom row was mislabeled as Blair). We can get a better sense of our esti‐\\nmator’s performance using the classification report, which lists recovery statistics\\nlabel by label:\\n418 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 436}, page_content=\"In[25]: from sklearn.metrics import classification_report\\n        print(classification_report(ytest, yfit,\\n                                    target_names=faces.target_names))\\n                   precision    recall  f1-score   support\\n     Ariel Sharon       0.65      0.73      0.69        15\\n     Colin Powell       0.81      0.87      0.84        68\\n  Donald Rumsfeld       0.75      0.87      0.81        31\\n    George W Bush       0.93      0.83      0.88       126\\nGerhard Schroeder       0.86      0.78      0.82        23\\n      Hugo Chavez       0.93      0.70      0.80        20\\nJunichiro Koizumi       0.80      1.00      0.89        12\\n       Tony Blair       0.83      0.93      0.88        42\\n      avg / total       0.85      0.85      0.85       337\\nWe might also display the confusion matrix between these classes (Figure 5-66):\\nIn[26]: from sklearn.metrics import confusion_matrix\\n        mat = confusion_matrix(ytest, yfit)\\n        sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\\n                    xticklabels=faces.target_names,\\n                    yticklabels=faces.target_names)\\n        plt.xlabel('true label')\\n        plt.ylabel('predicted label');\\nFigure 5-66. Confusion matrix for the faces data\\nThis helps us get a sense of which labels are likely to be confused by the estimator.\\nIn-Depth: Support Vector Machines \\n| \\n419\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 437}, page_content='For a real-world facial recognition task, in which the photos do not come precropped\\ninto nice grids, the only difference in the facial classification scheme is the feature\\nselection: you would need to use a more sophisticated algorithm to find the faces, and\\nextract features that are independent of the pixellation. For this kind of application,\\none good option is to make use of OpenCV, which among other things, includes pre‐\\ntrained implementations of state-of-the-art feature extraction tools for images in gen‐\\neral and faces in particular.\\nSupport Vector Machine Summary\\nWe have seen here a brief intuitive introduction to the principals behind support vec‐\\ntor machines. These methods are a powerful classification method for a number of\\nreasons:\\n• Their dependence on relatively few support vectors means that they are very\\ncompact models, and take up very little memory.\\n• Once the model is trained, the prediction phase is very fast.\\n• Because they are affected only by points near the margin, they work well with\\nhigh-dimensional data—even data with more dimensions than samples, which is\\na challenging regime for other algorithms.\\n• Their integration with kernel methods makes them very versatile, able to adapt to\\nmany types of data.\\nHowever, SVMs have several disadvantages as well:\\n• The scaling with the number of samples N is �N3  at worst, or �N2  for effi‐\\ncient implementations. For large numbers of training samples, this computa‐\\ntional cost can be prohibitive.\\n• The results are strongly dependent on a suitable choice for the softening parame‐\\nter C. This must be carefully chosen via cross-validation, which can be expensive\\nas datasets grow in size.\\n• The results do not have a direct probabilistic interpretation. This can be estima‐\\nted via an internal cross-validation (see the probability parameter of SVC), but\\nthis extra estimation is costly.\\nWith those traits in mind, I generally only turn to SVMs once other simpler, faster,\\nand less tuning-intensive methods have been shown to be insufficient for my needs.\\nNevertheless, if you have the CPU cycles to commit to training and cross-validating\\nan SVM on your data, the method can lead to excellent results.\\n420 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 438}, page_content='In-Depth: Decision Trees and Random Forests\\nPreviously we have looked in depth at a simple generative classifier (naive Bayes; see\\n“In Depth: Naive Bayes Classification” on page 382) and a powerful discriminative\\nclassifier (support vector machines; see “In-Depth: Support Vector Machines” on\\npage 405). Here we’ll take a look at motivating another powerful algorithm—a non‐\\nparametric algorithm called random forests. Random forests are an example of an \\nensemble method, a method that relies on aggregating the results of an ensemble of\\nsimpler estimators. The somewhat surprising result with such ensemble methods is\\nthat the sum can be greater than the parts; that is, a majority vote among a number of\\nestimators can end up being better than any of the individual estimators doing the\\nvoting! We will see examples of this in the following sections. We begin with the stan‐\\ndard imports:\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\nMotivating Random Forests: Decision Trees\\nRandom forests are an example of an ensemble learner built on decision trees. For this\\nreason we’ll start by discussing decision trees themselves.\\nDecision trees are extremely intuitive ways to classify or label objects: you simply ask\\na series of questions designed to zero in on the classification. For example, if you\\nwanted to build a decision tree to classify an animal you come across while on a hike,\\nyou might construct the one shown in Figure 5-67.\\nFigure 5-67. An example of a binary decision tree\\nIn-Depth: Decision Trees and Random Forests \\n| \\n421'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 439}, page_content=\"The binary splitting makes this extremely efficient: in a well-constructed tree, each\\nquestion will cut the number of options by approximately half, very quickly narrow‐\\ning the options even among a large number of classes. The trick, of course, comes in\\ndeciding which questions to ask at each step. In machine learning implementations of\\ndecision trees, the questions generally take the form of axis-aligned splits in the data;\\nthat is, each node in the tree splits the data into two groups using a cutoff value\\nwithin one of the features. Let’s now take a look at an example.\\nCreating a decision tree\\nConsider the following two-dimensional data, which has one of four class labels\\n(Figure 5-68):\\nIn[2]: from sklearn.datasets import make_blobs\\n       X, y = make_blobs(n_samples=300, centers=4,\\n                         random_state=0, cluster_std=1.0)\\n       plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\\nFigure 5-68. Data for the decision tree classifier\\nA simple decision tree built on this data will iteratively split the data along one or the\\nother axis according to some quantitative criterion, and at each level assign the label\\nof the new region according to a majority vote of points within it. Figure 5-69\\npresents a visualization of the first four levels of a decision tree classifier for this data.\\n422 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 440}, page_content=\"Figure 5-69. Visualization of how the decision tree splits the data\\nNotice that after the first split, every point in the upper branch remains unchanged,\\nso there is no need to further subdivide this branch. Except for nodes that contain all\\nof one color, at each level every region is again split along one of the two features.\\nThis process of fitting a decision tree to our data can be done in Scikit-Learn with the\\nDecisionTreeClassifier estimator:\\nIn[3]: from sklearn.tree import DecisionTreeClassifier\\n       tree = DecisionTreeClassifier().fit(X, y)\\nLet’s write a quick utility function to help us visualize the output of the classifier:\\nIn[4]: def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\\n           ax = ax or plt.gca()\\n           # Plot the training points\\n           ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\\n                      clim=(y.min(), y.max()), zorder=3)\\n           ax.axis('tight')\\n           ax.axis('off')\\n           xlim = ax.get_xlim()\\n           ylim = ax.get_ylim()\\n           # fit the estimator\\n           model.fit(X, y)\\n           xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\\n                                np.linspace(*ylim, num=200))\\n           Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\\n           # Create a color plot with the results\\n           n_classes = len(np.unique(y))\\n           contours = ax.contourf(xx, yy, Z, alpha=0.3,\\n                                  levels=np.arange(n_classes + 1) - 0.5,\\n                                  cmap=cmap, clim=(y.min(), y.max()),\\n                                  zorder=1)\\n           ax.set(xlim=xlim, ylim=ylim)\\nNow we can examine what the decision tree classification looks like (Figure 5-70):\\nIn[5]: visualize_classifier(DecisionTreeClassifier(), X, y)\\nIn-Depth: Decision Trees and Random Forests \\n| \\n423\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 441}, page_content='Figure 5-70. Visualization of a decision tree classification\\nIf you’re running this notebook live, you can use the helpers script included in the\\nonline appendix to bring up an interactive visualization of the decision tree building\\nprocess (Figure 5-71):\\nIn[6]: # helpers_05_08 is found in the online appendix\\n       # (https://github.com/jakevdp/PythonDataScienceHandbook)\\n       import helpers_05_08\\n       helpers_05_08.plot_tree_interactive(X, y);\\nFigure 5-71. First frame of the interactive decision tree widget; for the full version, see\\nthe online appendix\\nNotice that as the depth increases, we tend to get very strangely shaped classification\\nregions; for example, at a depth of five, there is a tall and skinny purple region\\n424 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 442}, page_content='between the yellow and blue regions. It’s clear that this is less a result of the true,\\nintrinsic data distribution, and more a result of the particular sampling or noise prop‐\\nerties of the data. That is, this decision tree, even at only five levels deep, is clearly\\noverfitting our data.\\nDecision trees and overfitting\\nSuch overfitting turns out to be a general property of decision trees; it is very easy to\\ngo too deep in the tree, and thus to fit details of the particular data rather than the\\noverall properties of the distributions they are drawn from. Another way to see this\\noverfitting is to look at models trained on different subsets of the data—for example,\\nin Figure 5-72 we train two different trees, each on half of the original data.\\nFigure 5-72. An example of two randomized decision trees\\nIt is clear that in some places, the two trees produce consistent results (e.g., in the\\nfour corners), while in other places, the two trees give very different classifications\\n(e.g., in the regions between any two clusters). The key observation is that the incon‐\\nsistencies tend to happen where the classification is less certain, and thus by using\\ninformation from both of these trees, we might come up with a better result!\\nIf you are running this notebook live, the following function will allow you to interac‐\\ntively display the fits of trees trained on a random subset of the data (Figure 5-73):\\nIn[7]: # helpers_05_08 is found in the online appendix\\n       # (https://github.com/jakevdp/PythonDataScienceHandbook)\\n       import helpers_05_08\\n       helpers_05_08.randomized_tree_interactive(X, y)\\nIn-Depth: Decision Trees and Random Forests \\n| \\n425'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 443}, page_content='Figure 5-73. First frame of the interactive randomized decision tree widget; for the full\\nversion, see the online appendix\\nJust as using information from two trees improves our results, we might expect that\\nusing information from many trees would improve our results even further.\\nEnsembles of Estimators: Random Forests\\nThis notion—that multiple overfitting estimators can be combined to reduce the\\neffect of this overfitting—is what underlies an ensemble method called bagging. Bag‐\\nging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of\\nwhich overfits the data, and averages the results to find a better classification. An\\nensemble of randomized decision trees is known as a random forest.\\nWe can do this type of bagging classification manually using Scikit-Learn’s Bagging\\nClassifier meta-estimator as shown here (Figure 5-74):\\nIn[8]: from sklearn.tree import DecisionTreeClassifier\\n       from sklearn.ensemble import BaggingClassifier\\n       tree = DecisionTreeClassifier()\\n       bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\\n                               random_state=1)\\n       bag.fit(X, y)\\n       visualize_classifier(bag, X, y)\\n426 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 444}, page_content='Figure 5-74. Decision boundaries for an ensemble of random decision trees\\nIn this example, we have randomized the data by fitting each estimator with a ran‐\\ndom subset of 80% of the training points. In practice, decision trees are more effec‐\\ntively randomized when some stochasticity is injected in how the splits are chosen;\\nthis way, all the data contributes to the fit each time, but the results of the fit still have\\nthe desired randomness. For example, when determining which feature to split on,\\nthe randomized tree might select from among the top several features. You can read\\nmore technical details about these randomization strategies in the Scikit-Learn docu‐\\nmentation and references within.\\nIn Scikit-Learn, such an optimized ensemble of randomized decision trees is imple‐\\nmented in the RandomForestClassifier estimator, which takes care of all the ran‐\\ndomization automatically. All you need to do is select a number of estimators, and it\\nwill very quickly (in parallel, if desired) fit the ensemble of trees (Figure 5-75):\\nIn[9]: from sklearn.ensemble import RandomForestClassifier\\n       model = RandomForestClassifier(n_estimators=100, random_state=0)\\n       visualize_classifier(model, X, y);\\nIn-Depth: Decision Trees and Random Forests \\n| \\n427'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 445}, page_content=\"Figure 5-75. Decision boundaries for a random forest, which is an optimized ensemble of\\ndecision trees\\nWe see that by averaging over 100 randomly perturbed models, we end up with an\\noverall model that is much closer to our intuition about how the parameter space\\nshould be split.\\nRandom Forest Regression\\nIn the previous section we considered random forests within the context of classifica‐\\ntion. Random forests can also be made to work in the case of regression (that is, con‐\\ntinuous rather than categorical variables). The estimator to use for this is the\\nRandomForestRegressor, and the syntax is very similar to what we saw earlier.\\nConsider the following data, drawn from the combination of a fast and slow oscilla‐\\ntion (Figure 5-76):\\nIn[10]: rng = np.random.RandomState(42)\\n        x = 10 * rng.rand(200)\\n        def model(x, sigma=0.3):\\n            fast_oscillation = np.sin(5 * x)\\n            slow_oscillation = np.sin(0.5 * x)\\n            noise = sigma * rng.randn(len(x))\\n            return slow_oscillation + fast_oscillation + noise\\n        y = model(x)\\n        plt.errorbar(x, y, 0.3, fmt='o');\\n428 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 446}, page_content=\"Figure 5-76. Data for random forest regression\\nUsing the random forest regressor, we can find the best-fit curve as follows\\n(Figure 5-77):\\nIn[11]: from sklearn.ensemble import RandomForestRegressor\\n        forest = RandomForestRegressor(200)\\n        forest.fit(x[:, None], y)\\n        xfit = np.linspace(0, 10, 1000)\\n        yfit = forest.predict(xfit[:, None])\\n        ytrue = model(xfit, sigma=0)\\n        plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\\n        plt.plot(xfit, yfit, '-r');\\n        plt.plot(xfit, ytrue, '-k', alpha=0.5);\\nHere the true model is shown by the smooth curve, while the random forest model is\\nshown by the jagged curve. As you can see, the nonparametric random forest model\\nis flexible enough to fit the multiperiod data, without us needing to specify a multi‐\\nperiod model!\\nIn-Depth: Decision Trees and Random Forests \\n| \\n429\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 447}, page_content=\"Figure 5-77. Random forest model fit to the data\\nExample: Random Forest for Classifying Digits\\nEarlier we took a quick look at the handwritten digits data (see “Introducing Scikit-\\nLearn” on page 343). Let’s use that again here to see how the random forest classifier\\ncan be used in this context.\\nIn[12]: from sklearn.datasets import load_digits\\n        digits = load_digits()\\n        digits.keys()\\nOut[12]: dict_keys(['target', 'data', 'target_names', 'DESCR', 'images'])\\nTo remind us what we’re looking at, we’ll visualize the first few data points\\n(Figure 5-78):\\nIn[13]:\\n# set up the figure\\nfig = plt.figure(figsize=(6, 6))  # figure size in inches\\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\\n# plot the digits: each image is 8x8 pixels\\nfor i in range(64):\\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\\n    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\\n    # label the image with the target value\\n    ax.text(0, 7, str(digits.target[i]))\\n430 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 448}, page_content='Figure 5-78. Representation of the digits data\\nWe can quickly classify the digits using a random forest as follows (Figure 5-79):\\nIn[14]:\\nfrom sklearn.cross_validation import train_test_split\\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\\n                                                random_state=0)\\nmodel = RandomForestClassifier(n_estimators=1000)\\nmodel.fit(Xtrain, ytrain)\\nypred = model.predict(Xtest)\\nWe can take a look at the classification report for this classifier:\\nIn[15]: from sklearn import metrics\\n        print(metrics.classification_report(ypred, ytest))\\n             precision    recall  f1-score   support\\n          0       1.00      0.97      0.99        38\\n          1       1.00      0.98      0.99        44\\n          2       0.95      1.00      0.98        42\\n          3       0.98      0.96      0.97        46\\n          4       0.97      1.00      0.99        37\\n          5       0.98      0.96      0.97        49\\n          6       1.00      1.00      1.00        52\\n          7       1.00      0.96      0.98        50\\n          8       0.94      0.98      0.96        46\\n          9       0.96      0.98      0.97        46\\navg / total       0.98      0.98      0.98       450\\nIn-Depth: Decision Trees and Random Forests \\n| \\n431'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 449}, page_content=\"And for good measure, plot the confusion matrix (Figure 5-79):\\nIn[16]: from sklearn.metrics import confusion_matrix\\n        mat = confusion_matrix(ytest, ypred)\\n        sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\\n        plt.xlabel('true label')\\n        plt.ylabel('predicted label');\\nFigure 5-79. Confusion matrix for digit classification with random forests\\nWe find that a simple, untuned random forest results in a very accurate classification\\nof the digits data.\\nSummary of Random Forests\\nThis section contained a brief introduction to the concept of ensemble estimators, and\\nin particular the random forest model—an ensemble of randomized decision trees.\\nRandom forests are a powerful method with several advantages:\\n• Both training and prediction are very fast, because of the simplicity of the under‐\\nlying decision trees. In addition, both tasks can be straightforwardly parallelized,\\nbecause the individual trees are entirely independent entities.\\n• The multiple trees allow for a probabilistic classification: a majority vote among\\nestimators gives an estimate of the probability (accessed in Scikit-Learn with the\\npredict_proba() method).\\n• The nonparametric model is extremely flexible, and can thus perform well on\\ntasks that are underfit by other estimators.\\n432 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 450}, page_content=\"A primary disadvantage of random forests is that the results are not easily interpreta‐\\nble; that is, if you would like to draw conclusions about the meaning of the classifica‐\\ntion model, random forests may not be the best choice.\\nIn Depth: Principal Component Analysis\\nUp until now, we have been looking in depth at supervised learning estimators: those\\nestimators that predict labels based on labeled training data. Here we begin looking at\\nseveral unsupervised estimators, which can highlight interesting aspects of the data\\nwithout reference to any known labels.\\nIn this section, we explore what is perhaps one of the most broadly used of unsuper‐\\nvised algorithms, principal component analysis (PCA). PCA is fundamentally a\\ndimensionality reduction algorithm, but it can also be useful as a tool for visualiza‐\\ntion, for noise filtering, for feature extraction and engineering, and much more. After\\na brief conceptual discussion of the PCA algorithm, we will see a couple examples of\\nthese further applications. We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import numpy as np\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\nIntroducing Principal Component Analysis\\nPrincipal component analysis is a fast and flexible unsupervised method for dimen‐\\nsionality reduction in data, which we saw briefly in “Introducing Scikit-Learn” on\\npage 343. Its behavior is easiest to visualize by looking at a two-dimensional dataset.\\nConsider the following 200 points (Figure 5-80):\\nIn[2]: rng = np.random.RandomState(1)\\n       X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\\n       plt.scatter(X[:, 0], X[:, 1])\\n       plt.axis('equal');\\nBy eye, it is clear that there is a nearly linear relationship between the x and y vari‐\\nables. This is reminiscent of the linear regression data we explored in “In Depth: Lin‐\\near Regression” on page 390, but the problem setting here is slightly different: rather\\nthan attempting to predict the y values from the x values, the unsupervised learning\\nproblem attempts to learn about the relationship between the x and y values.\\nIn Depth: Principal Component Analysis \\n| \\n433\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 451}, page_content=\"Figure 5-80. Data for demonstration of PCA\\nIn principal component analysis, one quantifies this relationship by finding a list of\\nthe principal axes in the data, and using those axes to describe the dataset. Using\\nScikit-Learn’s PCA estimator, we can compute this as follows:\\nIn[3]: from sklearn.decomposition import PCA\\n       pca = PCA(n_components=2)\\n       pca.fit(X)\\nOut[3]: PCA(copy=True, n_components=2, whiten=False)\\nThe fit learns some quantities from the data, most importantly the “components” and\\n“explained variance”:\\nIn[4]: print(pca.components_)\\n[[ 0.94446029  0.32862557]\\n [ 0.32862557 -0.94446029]]\\nIn[5]: print(pca.explained_variance_)\\n[ 0.75871884  0.01838551]\\nTo see what these numbers mean, let’s visualize them as vectors over the input data,\\nusing the “components” to define the direction of the vector, and the “explained var‐\\niance” to define the squared-length of the vector (Figure 5-81):\\nIn[6]: def draw_vector(v0, v1, ax=None):\\n           ax = ax or plt.gca()\\n           arrowprops=dict(arrowstyle='->',\\n                           linewidth=2,\\n                           shrinkA=0, shrinkB=0)\\n           ax.annotate('', v1, v0, arrowprops=arrowprops)\\n       # plot data\\n434 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 452}, page_content=\"plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\\n       for length, vector in zip(pca.explained_variance_, pca.components_):\\n           v = vector * 3 * np.sqrt(length)\\n           draw_vector(pca.mean_, pca.mean_ + v)\\n       plt.axis('equal');\\nFigure 5-81. Visualization of the principal axes in the data\\nThese vectors represent the principal axes of the data, and the length shown in\\nFigure 5-81 is an indication of how “important” that axis is in describing the distribu‐\\ntion of the data—more precisely, it is a measure of the variance of the data when pro‐\\njected onto that axis. The projection of each data point onto the principal axes are the\\n“principal components” of the data.\\nIf we plot these principal components beside the original data, we see the plots shown\\nin Figure 5-82.\\nFigure 5-82. Transformed principal axes in the data\\nIn Depth: Principal Component Analysis \\n| \\n435\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 453}, page_content='This transformation from data axes to principal axes is as an affine transformation,\\nwhich basically means it is composed of a translation, rotation, and uniform scaling.\\nWhile this algorithm to find principal components may seem like just a mathematical\\ncuriosity, it turns out to have very far-reaching applications in the world of machine\\nlearning and data exploration.\\nPCA as dimensionality reduction\\nUsing PCA for dimensionality reduction involves zeroing out one or more of the\\nsmallest principal components, resulting in a lower-dimensional projection of the\\ndata that preserves the maximal data variance.\\nHere is an example of using PCA as a dimensionality reduction transform:\\nIn[7]: pca = PCA(n_components=1)\\n       pca.fit(X)\\n       X_pca = pca.transform(X)\\n       print(\"original shape:   \", X.shape)\\n       print(\"transformed shape:\", X_pca.shape)\\noriginal shape:    (200, 2)\\ntransformed shape: (200, 1)\\nThe transformed data has been reduced to a single dimension. To understand the\\neffect of this dimensionality reduction, we can perform the inverse transform of this\\nreduced data and plot it along with the original data (Figure 5-83):\\nIn[8]: X_new = pca.inverse_transform(X_pca)\\n       plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\\n       plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\\n       plt.axis(\\'equal\\');\\nFigure 5-83. Visualization of PCA as dimensionality reduction\\n436 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 454}, page_content=\"The light points are the original data, while the dark points are the projected version.\\nThis makes clear what a PCA dimensionality reduction means: the information along\\nthe least important principal axis or axes is removed, leaving only the component(s)\\nof the data with the highest variance. The fraction of variance that is cut out (propor‐\\ntional to the spread of points about the line formed in Figure 5-83) is roughly a meas‐\\nure of how much “information” is discarded in this reduction of dimensionality.\\nThis reduced-dimension dataset is in some senses “good enough” to encode the most\\nimportant relationships between the points: despite reducing the dimension of the\\ndata by 50%, the overall relationship between the data points is mostly preserved.\\nPCA for visualization: Handwritten digits\\nThe usefulness of the dimensionality reduction may not be entirely apparent in only\\ntwo dimensions, but becomes much more clear when we look at high-dimensional\\ndata. To see this, let’s take a quick look at the application of PCA to the digits data we\\nsaw in “In-Depth: Decision Trees and Random Forests” on page 421.\\nWe start by loading the data:\\nIn[9]: from sklearn.datasets import load_digits\\n       digits = load_digits()\\n       digits.data.shape\\nOut[9]:\\n(1797, 64)\\nRecall that the data consists of 8×8 pixel images, meaning that they are 64-\\ndimensional. To gain some intuition into the relationships between these points, we\\ncan use PCA to project them to a more manageable number of dimensions, say two:\\nIn[10]: pca = PCA(2)  # project from 64 to 2 dimensions\\n        projected = pca.fit_transform(digits.data)\\n        print(digits.data.shape)\\n        print(projected.shape)\\n(1797, 64)\\n(1797, 2)\\nWe can now plot the first two principal components of each point to learn about the\\ndata (Figure 5-84):\\nIn[11]: plt.scatter(projected[:, 0], projected[:, 1],\\n                    c=digits.target, edgecolor='none', alpha=0.5,\\n                    cmap=plt.cm.get_cmap('spectral', 10))\\n        plt.xlabel('component 1')\\n        plt.ylabel('component 2')\\n        plt.colorbar();\\nIn Depth: Principal Component Analysis \\n| \\n437\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 455}, page_content='Figure 5-84. PCA applied to the handwritten digits data\\nRecall what these components mean: the full data is a 64-dimensional point cloud,\\nand these points are the projection of each data point along the directions with the\\nlargest variance. Essentially, we have found the optimal stretch and rotation in 64-\\ndimensional space that allows us to see the layout of the digits in two dimensions, and\\nhave done this in an unsupervised manner—that is, without reference to the labels.\\nWhat do the components mean?\\nWe can go a bit further here, and begin to ask what the reduced dimensions mean.\\nThis meaning can be understood in terms of combinations of basis vectors. For\\nexample, each image in the training set is defined by a collection of 64 pixel values,\\nwhich we will call the vector x:\\nx = x1, x2, x3⋯x64\\nOne way we can think about this is in terms of a pixel basis. That is, to construct the\\nimage, we multiply each element of the vector by the pixel it describes, and then add\\nthe results together to build the image:\\nimage x = x1 · pixel 1 + x2 · pixel 2 + x3 · pixel 3 ⋯x64 · pixel 64\\nOne way we might imagine reducing the dimension of this data is to zero out all but a\\nfew of these basis vectors. For example, if we use only the first eight pixels, we get an\\neight-dimensional projection of the data (Figure 5-85), but it is not very reflective of\\nthe whole image: we’ve thrown out nearly 90% of the pixels!\\n438 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 456}, page_content='Figure 5-85. A naive dimensionality reduction achieved by discarding pixels\\nThe upper row of panels shows the individual pixels, and the lower row shows the\\ncumulative contribution of these pixels to the construction of the image. Using only\\neight of the pixel-basis components, we can only construct a small portion of the 64-\\npixel image. Were we to continue this sequence and use all 64 pixels, we would\\nrecover the original image.\\nBut the pixel-wise representation is not the only choice of basis. We can also use other\\nbasis functions, which each contain some predefined contribution from each pixel,\\nand write something like:\\nimage x = mean + x1 · basis 1 + x2 · basis 2 + x3 · basis 3 ⋯\\nPCA can be thought of as a process of choosing optimal basis functions, such that\\nadding together just the first few of them is enough to suitably reconstruct the bulk of\\nthe elements in the dataset. The principal components, which act as the low-\\ndimensional representation of our data, are simply the coefficients that multiply each\\nof the elements in this series. Figure 5-86 is a similar depiction of reconstructing this\\ndigit using the mean plus the first eight PCA basis functions.\\nFigure 5-86. A more sophisticated dimensionality reduction achieved by discarding the\\nleast important principal components (compare to Figure 5-85)\\nUnlike the pixel basis, the PCA basis allows us to recover the salient features of the\\ninput image with just a mean plus eight components! The amount of each pixel in\\neach component is the corollary of the orientation of the vector in our two-\\ndimensional example. This is the sense in which PCA provides a low-dimensional\\nrepresentation of the data: it discovers a set of basis functions that are more efficient\\nthan the native pixel-basis of the input data.\\nIn Depth: Principal Component Analysis \\n| \\n439'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 457}, page_content=\"Choosing the number of components\\nA vital part of using PCA in practice is the ability to estimate how many components\\nare needed to describe the data. We can determine this by looking at the cumulative\\nexplained variance ratio as a function of the number of components (Figure 5-87):\\nIn[12]: pca = PCA().fit(digits.data)\\n        plt.plot(np.cumsum(pca.explained_variance_ratio_))\\n        plt.xlabel('number of components')\\n        plt.ylabel('cumulative explained variance');\\nFigure 5-87. The cumulative explained variance, which measures how well PCA pre‐\\nserves the content of the data\\nThis curve quantifies how much of the total, 64-dimensional variance is contained\\nwithin the first N components. For example, we see that with the digits the first 10\\ncomponents contain approximately 75% of the variance, while you need around 50\\ncomponents to describe close to 100% of the variance.\\nHere we see that our two-dimensional projection loses a lot of information (as meas‐\\nured by the explained variance) and that we’d need about 20 components to retain\\n90% of the variance. Looking at this plot for a high-dimensional dataset can help you\\nunderstand the level of redundancy present in multiple observations.\\nPCA as Noise Filtering\\nPCA can also be used as a filtering approach for noisy data. The idea is this: any com‐\\nponents with variance much larger than the effect of the noise should be relatively\\nunaffected by the noise. So if you reconstruct the data using just the largest subset of\\nprincipal components, you should be preferentially keeping the signal and throwing\\nout the noise.\\n440 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 458}, page_content=\"Let’s see how this looks with the digits data. First we will plot several of the input\\nnoise-free data (Figure 5-88):\\nIn[13]: def plot_digits(data):\\n            fig, axes = plt.subplots(4, 10, figsize=(10, 4),\\n                                     subplot_kw={'xticks':[], 'yticks':[]},\\n                                     gridspec_kw=dict(hspace=0.1, wspace=0.1))\\n            for i, ax in enumerate(axes.flat):\\n                ax.imshow(data[i].reshape(8, 8),\\n                          cmap='binary', interpolation='nearest',\\n                          clim=(0, 16))\\n        plot_digits(digits.data)\\nFigure 5-88. Digits without noise\\nNow let’s add some random noise to create a noisy dataset, and replot it\\n(Figure 5-89):\\nIn[14]: np.random.seed(42)\\n        noisy = np.random.normal(digits.data, 4)\\n        plot_digits(noisy)\\nFigure 5-89. Digits with Gaussian random noise added\\nIt’s clear by eye that the images are noisy, and contain spurious pixels. Let’s train a\\nPCA on the noisy data, requesting that the projection preserve 50% of the variance:\\nIn Depth: Principal Component Analysis \\n| \\n441\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 459}, page_content=\"In[15]: pca = PCA(0.50).fit(noisy)\\n        pca.n_components_\\nOut[15]: 12\\nHere 50% of the variance amounts to 12 principal components. Now we compute\\nthese components, and then use the inverse of the transform to reconstruct the fil‐\\ntered digits (Figure 5-90):\\nIn[16]: components = pca.transform(noisy)\\n        filtered = pca.inverse_transform(components)\\n        plot_digits(filtered)\\nFigure 5-90. Digits “denoised” using PCA\\nThis signal preserving/noise filtering property makes PCA a very useful feature selec‐\\ntion routine—for example, rather than training a classifier on very high-dimensional\\ndata, you might instead train the classifier on the lower-dimensional representation,\\nwhich will automatically serve to filter out random noise in the inputs.\\nExample: Eigenfaces\\nEarlier we explored an example of using a PCA projection as a feature selector for\\nfacial recognition with a support vector machine (“In-Depth: Support Vector\\nMachines” on page 405). Here we will take a look back and explore a bit more of what\\nwent into that. Recall that we were using the Labeled Faces in the Wild dataset made\\navailable through Scikit-Learn:\\nIn[17]: from sklearn.datasets import fetch_lfw_people\\n        faces = fetch_lfw_people(min_faces_per_person=60)\\n        print(faces.target_names)\\n        print(faces.images.shape)\\n['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\\n 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\\n(1348, 62, 47)\\nLet’s take a look at the principal axes that span this dataset. Because this is a large\\ndataset, we will use RandomizedPCA—it contains a randomized method to approxi‐\\n442 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 460}, page_content=\"mate the first N principal components much more quickly than the standard PCA esti‐\\nmator, and thus is very useful for high-dimensional data (here, a dimensionality of\\nnearly 3,000). We will take a look at the first 150 components:\\nIn[18]: from sklearn.decomposition import RandomizedPCA\\n        pca = RandomizedPCA(150)\\n        pca.fit(faces.data)\\nOut[18]: RandomizedPCA(copy=True, iterated_power=3, n_components=150,\\n                random_state=None, whiten=False)\\nIn this case, it can be interesting to visualize the images associated with the first sev‐\\neral principal components (these components are technically known as “eigenvec‐\\ntors,” so these types of images are often called “eigenfaces”). As you can see in\\nFigure 5-91, they are as creepy as they sound:\\nIn[19]: fig, axes = plt.subplots(3, 8, figsize=(9, 4),\\n                                 subplot_kw={'xticks':[], 'yticks':[]},\\n                                 gridspec_kw=dict(hspace=0.1, wspace=0.1))\\n        for i, ax in enumerate(axes.flat):\\n            ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')\\nFigure 5-91. A visualization of eigenfaces learned from the LFW dataset\\nThe results are very interesting, and give us insight into how the images vary: for\\nexample, the first few eigenfaces (from the top left) seem to be associated with the\\nangle of lighting on the face, and later principal vectors seem to be picking out certain\\nfeatures, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of\\nthese components to see how much of the data information the projection is preserv‐\\ning (Figure 5-92):\\nIn[20]: plt.plot(np.cumsum(pca.explained_variance_ratio_))\\n        plt.xlabel('number of components')\\n        plt.ylabel('cumulative explained variance');\\nIn Depth: Principal Component Analysis \\n| \\n443\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 461}, page_content=\"Figure 5-92. Cumulative explained variance for the LFW data\\nWe see that these 150 components account for just over 90% of the variance. That\\nwould lead us to believe that using these 150 components, we would recover most of\\nthe essential characteristics of the data. To make this more concrete, we can compare\\nthe input images with the images reconstructed from these 150 components\\n(Figure 5-93):\\nIn[21]: # Compute the components and projected faces\\n        pca = RandomizedPCA(150).fit(faces.data)\\n        components = pca.transform(faces.data)\\n        projected = pca.inverse_transform(components)\\nIn[22]: # Plot the results\\n        fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\\n                               subplot_kw={'xticks':[], 'yticks':[]},\\n                               gridspec_kw=dict(hspace=0.1, wspace=0.1))\\n        for i in range(10):\\n            ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\\n            ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\\n        ax[0, 0].set_ylabel('full-dim\\\\ninput')\\n        ax[1, 0].set_ylabel('150-dim\\\\nreconstruction');\\nFigure 5-93. 150-dimensional PCA reconstruction of the LFW data\\n444 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 462}, page_content='The top row here shows the input images, while the bottom row shows the recon‐\\nstruction of the images from just 150 of the ~3,000 initial features. This visualization\\nmakes clear why the PCA feature selection used in “In-Depth: Support Vector\\nMachines” on page 405 was so successful: although it reduces the dimensionality of\\nthe data by nearly a factor of 20, the projected images contain enough information\\nthat we might, by eye, recognize the individuals in the image. What this means is that\\nour classification algorithm needs to be trained on 150-dimensional data rather than\\n3,000-dimensional data, which depending on the particular algorithm we choose, can\\nlead to a much more efficient classification.\\nPrincipal Component Analysis Summary\\nIn this section we have discussed the use of principal component analysis for dimen‐\\nsionality reduction, for visualization of high-dimensional data, for noise filtering, and\\nfor feature selection within high-dimensional data. Because of the versatility and\\ninterpretability of PCA, it has been shown to be effective in a wide variety of contexts\\nand disciplines. Given any high-dimensional dataset, I tend to start with PCA in\\norder to visualize the relationship between points (as we did with the digits), to\\nunderstand the main variance in the data (as we did with the eigenfaces), and to\\nunderstand the intrinsic dimensionality (by plotting the explained variance ratio).\\nCertainly PCA is not useful for every high-dimensional dataset, but it offers a\\nstraightforward and efficient path to gaining insight into high-dimensional data.\\nPCA’s main weakness is that it tends to be highly affected by outliers in the data. For\\nthis reason, many robust variants of PCA have been developed, many of which act to\\niteratively discard data points that are poorly described by the initial components.\\nScikit-Learn contains a couple interesting variants on PCA, including RandomizedPCA\\nand SparsePCA, both also in the sklearn.decomposition submodule. Randomi\\nzedPCA, which we saw earlier, uses a nondeterministic method to quickly approxi‐\\nmate the first few principal components in very high-dimensional data, while\\nSparsePCA introduces a regularization term (see “In Depth: Linear Regression” on\\npage 390) that serves to enforce sparsity of the components.\\nIn the following sections, we will look at other unsupervised learning methods that\\nbuild on some of the ideas of PCA.\\nIn-Depth: Manifold Learning\\nWe have seen how principal component analysis can be used in the dimensionality\\nreduction task—reducing the number of features of a dataset while maintaining the\\nessential relationships between the points. While PCA is flexible, fast, and easily\\ninterpretable, it does not perform so well when there are nonlinear relationships\\nwithin the data; we will see some examples of these below.\\nIn-Depth: Manifold Learning \\n| \\n445'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 463}, page_content='To address this deficiency, we can turn to a class of methods known as manifold learn‐\\ning—a class of unsupervised estimators that seeks to describe datasets as low-\\ndimensional manifolds embedded in high-dimensional spaces. When you think of a\\nmanifold, I’d suggest imagining a sheet of paper: this is a two-dimensional object that\\nlives in our familiar three-dimensional world, and can be bent or rolled in two\\ndimensions. In the parlance of manifold learning, we can think of this sheet as a two-\\ndimensional manifold embedded in three-dimensional space.\\nRotating, reorienting, or stretching the piece of paper in three-dimensional space\\ndoesn’t change the flat geometry of the paper: such operations are akin to linear\\nembeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional\\nmanifold, but the embedding into the three-dimensional space is no longer linear.\\nManifold learning algorithms would seek to learn about the fundamental two-\\ndimensional nature of the paper, even as it is contorted to fill the three-dimensional\\nspace.\\nHere we will demonstrate a number of manifold methods, going most deeply into a\\ncouple techniques: multidimensional scaling (MDS), locally linear embedding (LLE),\\nand isometric mapping (Isomap). We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\n       import numpy as np\\nManifold Learning: “HELLO”\\nTo make these concepts more clear, let’s start by generating some two-dimensional\\ndata that we can use to define a manifold. Here is a function that will create data in\\nthe shape of the word “HELLO”:\\nIn[2]:\\ndef make_hello(N=1000, rseed=42):\\n    # Make a plot with \"HELLO\" text; save as PNG\\n    fig, ax = plt.subplots(figsize=(4, 1))\\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\\n    ax.axis(\\'off\\')\\n    ax.text(0.5, 0.4, \\'HELLO\\', va=\\'center\\', ha=\\'center\\', weight=\\'bold\\', size=85)\\n    fig.savefig(\\'hello.png\\')\\n    plt.close(fig)\\n    # Open this PNG and draw random points from it\\n    from matplotlib.image import imread\\n    data = imread(\\'hello.png\\')[::-1, :, 0].T\\n    rng = np.random.RandomState(rseed)\\n    X = rng.rand(4 * N, 2)\\n    i, j = (X * data.shape).astype(int).T\\n    mask = (data[i, j] < 1)\\n    X = X[mask]\\n446 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 464}, page_content=\"X[:, 0] *= (data.shape[0] / data.shape[1])\\n    X = X[:N]\\n    return X[np.argsort(X[:, 0])]\\nLet’s call the function and visualize the resulting data (Figure 5-94):\\nIn[3]: X = make_hello(1000)\\n       colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))\\n       plt.scatter(X[:, 0], X[:, 1], **colorize)\\n       plt.axis('equal');\\nFigure 5-94. Data for use with manifold learning\\nThe output is two dimensional, and consists of points drawn in the shape of the word\\n“HELLO”. This data form will help us to see visually what these algorithms are doing.\\nMultidimensional Scaling (MDS)\\nLooking at data like this, we can see that the particular choice of x and y values of the\\ndataset are not the most fundamental description of the data: we can scale, shrink, or\\nrotate the data, and the “HELLO” will still be apparent. For example, if we use a rota‐\\ntion matrix to rotate the data, the x and y values change, but the data is still funda‐\\nmentally the same (Figure 5-95):\\nIn[4]: def rotate(X, angle):\\n           theta = np.deg2rad(angle)\\n           R = [[np.cos(theta), np.sin(theta)],\\n                [-np.sin(theta), np.cos(theta)]]\\n           return np.dot(X, R)\\n       X2 = rotate(X, 20) + 5\\n       plt.scatter(X2[:, 0], X2[:, 1], **colorize)\\n       plt.axis('equal');\\nIn-Depth: Manifold Learning \\n| \\n447\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 465}, page_content=\"Figure 5-95. Rotated dataset\\nThis tells us that the x and y values are not necessarily fundamental to the relation‐\\nships in the data. What is fundamental, in this case, is the distance between each point\\nand the other points in the dataset. A common way to represent this is to use a dis‐\\ntance matrix: for N points, we construct an N × N array such that entry i, j  contains\\nthe distance between point i and point j. Let’s use Scikit-Learn’s efficient pair\\nwise_distances function to do this for our original data:\\nIn[5]: from sklearn.metrics import pairwise_distances\\n       D = pairwise_distances(X)\\n       D.shape\\nOut[5]: (1000, 1000)\\nAs promised, for our N=1,000 points, we obtain a 1,000×1,000 matrix, which can be\\nvisualized as shown in Figure 5-96:\\nIn[6]: plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest')\\n       plt.colorbar();\\n448 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 466}, page_content=\"Figure 5-96. Visualization of the pairwise distances between points\\nIf we similarly construct a distance matrix for our rotated and translated data, we see\\nthat it is the same:\\nIn[7]: D2 = pairwise_distances(X2)\\n       np.allclose(D, D2)\\nOut[7]: True\\nThis distance matrix gives us a representation of our data that is invariant to rotations\\nand translations, but the visualization of the matrix is not entirely intuitive. In the\\nrepresentation presented in Figure 5-96, we have lost any visible sign of the interest‐\\ning structure in the data: the “HELLO” that we saw before.\\nFurther, while computing this distance matrix from the (x, y) coordinates is straight‐\\nforward, transforming the distances back into x and y coordinates is rather difficult.\\nThis is exactly what the multidimensional scaling algorithm aims to do: given a dis‐\\ntance matrix between points, it recovers a D-dimensional coordinate representation\\nof the data. Let’s see how it works for our distance matrix, using the precomputed\\ndissimilarity to specify that we are passing a distance matrix (Figure 5-97):\\nIn[8]: from sklearn.manifold import MDS\\n       model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\\n       out = model.fit_transform(D)\\n       plt.scatter(out[:, 0], out[:, 1], **colorize)\\n       plt.axis('equal');\\nIn-Depth: Manifold Learning \\n| \\n449\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 467}, page_content=\"Figure 5-97. An MDS embedding computed from the pairwise distances\\nThe MDS algorithm recovers one of the possible two-dimensional coordinate repre‐\\nsentations of our data, using only the N × N distance matrix describing the relation‐\\nship between the data points.\\nMDS as Manifold Learning\\nThe usefulness of this becomes more apparent when we consider the fact that dis‐\\ntance matrices can be computed from data in any dimension. So, for example, instead\\nof simply rotating the data in the two-dimensional plane, we can project it into three\\ndimensions using the following function (essentially a three-dimensional generaliza‐\\ntion of the rotation matrix used earlier):\\nIn[9]: def random_projection(X, dimension=3, rseed=42):\\n           assert dimension >= X.shape[1]\\n           rng = np.random.RandomState(rseed)\\n           C = rng.randn(dimension, dimension)\\n           e, V = np.linalg.eigh(np.dot(C, C.T))\\n           return np.dot(X, V[:X.shape[1]])\\n       X3 = random_projection(X, 3)\\n       X3.shape\\nOut[9]: (1000, 3)\\nLet’s visualize these points to see what we’re working with (Figure 5-98):\\nIn[10]: from mpl_toolkits import mplot3d\\n        ax = plt.axes(projection='3d')\\n        ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],\\n                     **colorize)\\n        ax.view_init(azim=70, elev=50)\\n450 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 468}, page_content=\"Figure 5-98. Data embedded linearly into three dimensions\\nWe can now ask the MDS estimator to input this three-dimensional data, compute the\\ndistance matrix, and then determine the optimal two-dimensional embedding for this\\ndistance matrix. The result recovers a representation of the original data\\n(Figure 5-99):\\nIn[11]: model = MDS(n_components=2, random_state=1)\\n        out3 = model.fit_transform(X3)\\n        plt.scatter(out3[:, 0], out3[:, 1], **colorize)\\n        plt.axis('equal');\\nFigure 5-99. The MDS embedding of the three-dimensional data recovers the input up to\\na rotation and reflection\\nThis is essentially the goal of a manifold learning estimator: given high-dimensional\\nembedded data, it seeks a low-dimensional representation of the data that preserves\\nIn-Depth: Manifold Learning \\n| \\n451\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 469}, page_content=\"certain relationships within the data. In the case of MDS, the quantity preserved is the\\ndistance between every pair of points.\\nNonlinear Embeddings: Where MDS Fails\\nOur discussion so far has considered linear embeddings, which essentially consist of\\nrotations, translations, and scalings of data into higher-dimensional spaces. Where\\nMDS breaks down is when the embedding is nonlinear—that is, when it goes beyond\\nthis simple set of operations. Consider the following embedding, which takes the\\ninput and contorts it into an “S” shape in three dimensions:\\nIn[12]: def make_hello_s_curve(X):\\n            t = (X[:, 0] - 2) * 0.75 * np.pi\\n            x = np.sin(t)\\n            y = X[:, 1]\\n            z = np.sign(t) * (np.cos(t) - 1)\\n            return np.vstack((x, y, z)).T\\n        XS = make_hello_s_curve(X)\\nThis is again three-dimensional data, but we can see that the embedding is much\\nmore complicated (Figure 5-100):\\nIn[13]: from mpl_toolkits import mplot3d\\n        ax = plt.axes(projection='3d')\\n        ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],\\n                     **colorize);\\nFigure 5-100. Data embedded nonlinearly into three dimensions\\nThe fundamental relationships between the data points are still there, but this time\\nthe data has been transformed in a nonlinear way: it has been wrapped up into the\\nshape of an “S.”\\n452 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 470}, page_content=\"If we try a simple MDS algorithm on this data, it is not able to “unwrap” this nonlin‐\\near embedding, and we lose track of the fundamental relationships in the embedded\\nmanifold (Figure 5-101):\\nIn[14]: from sklearn.manifold import MDS\\n        model = MDS(n_components=2, random_state=2)\\n        outS = model.fit_transform(XS)\\n        plt.scatter(outS[:, 0], outS[:, 1], **colorize)\\n        plt.axis('equal');\\nFigure 5-101. The MDS algorithm applied to the nonlinear data; it fails to recover the\\nunderlying structure\\nThe best two-dimensional linear embedding does not unwrap the S-curve, but\\ninstead throws out the original y-axis.\\nNonlinear Manifolds: Locally Linear Embedding\\nHow can we move forward here? Stepping back, we can see that the source of the\\nproblem is that MDS tries to preserve distances between faraway points when con‐\\nstructing the embedding. But what if we instead modified the algorithm such that it\\nonly preserves distances between nearby points? The resulting embedding would be\\ncloser to what we want.\\nVisually, we can think of it as illustrated in Figure 5-102.\\nIn-Depth: Manifold Learning \\n| \\n453\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 471}, page_content=\"Figure 5-102. Representation of linkages between points within MDS and LLE\\nHere each faint line represents a distance that should be preserved in the embedding.\\nOn the left is a representation of the model used by MDS: it tries to preserve the dis‐\\ntances between each pair of points in the dataset. On the right is a representation of\\nthe model used by a manifold learning algorithm called locally linear embedding\\n(LLE): rather than preserving all distances, it instead tries to preserve only the distan‐\\nces between neighboring points: in this case, the nearest 100 neighbors of each point.\\nThinking about the left panel, we can see why MDS fails: there is no way to flatten\\nthis data while adequately preserving the length of every line drawn between the two\\npoints. For the right panel, on the other hand, things look a bit more optimistic. We\\ncould imagine unrolling the data in a way that keeps the lengths of the lines approxi‐\\nmately the same. This is precisely what LLE does, through a global optimization of a\\ncost function reflecting this logic.\\nLLE comes in a number of flavors; here we will use the modified LLE algorithm to\\nrecover the embedded two-dimensional manifold. In general, modified LLE does bet‐\\nter than other flavors of the algorithm at recovering well-defined manifolds with very\\nlittle distortion (Figure 5-103):\\nIn[15]:\\nfrom sklearn.manifold import LocallyLinearEmbedding\\nmodel = LocallyLinearEmbedding(n_neighbors=100, n_components=2, method='modified',\\n                               eigen_solver='dense')\\nout = model.fit_transform(XS)\\nfig, ax = plt.subplots()\\nax.scatter(out[:, 0], out[:, 1], **colorize)\\nax.set_ylim(0.15, -0.15);\\n454 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 472}, page_content='Figure 5-103. Locally linear embedding can recover the underlying data from a nonli‐\\nnearly embedded input\\nThe result remains somewhat distorted compared to our original manifold, but cap‐\\ntures the essential relationships in the data!\\nSome Thoughts on Manifold Methods\\nThough this story and motivation is compelling, in practice manifold learning tech‐\\nniques tend to be finicky enough that they are rarely used for anything more than\\nsimple qualitative visualization of high-dimensional data.\\nThe following are some of the particular challenges of manifold learning, which all\\ncontrast poorly with PCA:\\n• In manifold learning, there is no good framework for handling missing data. In\\ncontrast, there are straightforward iterative approaches for missing data in PCA.\\n• In manifold learning, the presence of noise in the data can “short-circuit” the\\nmanifold and drastically change the embedding. In contrast, PCA naturally filters\\nnoise from the most important components.\\n• The manifold embedding result is generally highly dependent on the number of\\nneighbors chosen, and there is generally no solid quantitative way to choose an\\noptimal number of neighbors. In contrast, PCA does not involve such a choice.\\n• In manifold learning, the globally optimal number of output dimensions is diffi‐\\ncult to determine. In contrast, PCA lets you find the output dimension based on\\nthe explained variance.\\n• In manifold learning, the meaning of the embedded dimensions is not always\\nclear. In PCA, the principal components have a very clear meaning.\\nIn-Depth: Manifold Learning \\n| \\n455'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 473}, page_content='• In manifold learning the computational expense of manifold methods scales as\\nO[N2] or O[N3]. For PCA, there exist randomized approaches that are generally\\nmuch faster (though see the megaman package for some more scalable imple‐\\nmentations of manifold learning).\\nWith all that on the table, the only clear advantage of manifold learning methods over\\nPCA is their ability to preserve nonlinear relationships in the data; for that reason I\\ntend to explore data with manifold methods only after first exploring them with PCA.\\nScikit-Learn implements several common variants of manifold learning beyond Iso‐\\nmap and LLE: the Scikit-Learn documentation has a nice discussion and comparison\\nof them. Based on my own experience, I would give the following recommendations:\\n• For toy problems such as the S-curve we saw before, locally linear embedding\\n(LLE) and its variants (especially modified LLE), perform very well. This is imple‐\\nmented in sklearn.manifold.LocallyLinearEmbedding.\\n• For high-dimensional data from real-world sources, LLE often produces poor\\nresults, and isometric mapping (Isomap) seems to generally lead to more mean‐\\ningful embeddings. This is implemented in sklearn.manifold.Isomap.\\n• For data that is highly clustered, t-distributed stochastic neighbor embedding (t-\\nSNE) seems to work very well, though can be very slow compared to other meth‐\\nods. This is implemented in sklearn.manifold.TSNE.\\nIf you’re interested in getting a feel for how these work, I’d suggest running each of\\nthe methods on the data in this section.\\nExample: Isomap on Faces\\nOne place manifold learning is often used is in understanding the relationship\\nbetween high-dimensional data points. A common case of high-dimensional data is\\nimages; for example, a set of images with 1,000 pixels each can be thought of as col‐\\nlection of points in 1,000 dimensions—the brightness of each pixel in each image\\ndefines the coordinate in that dimension.\\nHere let’s apply Isomap on some faces data. We will use the Labeled Faces in the Wild\\ndataset, which we previously saw in “In-Depth: Support Vector Machines” on page\\n405 and “In Depth: Principal Component Analysis” on page 433. Running this com‐\\nmand will download the data and cache it in your home directory for later use:\\nIn[16]: from sklearn.datasets import fetch_lfw_people\\n        faces = fetch_lfw_people(min_faces_per_person=30)\\n        faces.data.shape\\nOut[16]: (2370, 2914)\\n456 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 474}, page_content=\"We have 2,370 images, each with 2,914 pixels. In other words, the images can be\\nthought of as data points in a 2,914-dimensional space!\\nLet’s quickly visualize several of these images to see what we’re working with\\n(Figure 5-104):\\nIn[17]: fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))\\n        for i, axi in enumerate(ax.flat):\\n            axi.imshow(faces.images[i], cmap='gray')\\nFigure 5-104. Examples of the input faces\\nWe would like to plot a low-dimensional embedding of the 2,914-dimensional data to\\nlearn the fundamental relationships between the images. One useful way to start is to\\ncompute a PCA, and examine the explained variance ratio, which will give us an idea\\nof how many linear features are required to describe the data (Figure 5-105):\\nIn[18]: from sklearn.decomposition import RandomizedPCA\\n        model = RandomizedPCA(100).fit(faces.data)\\n        plt.plot(np.cumsum(model.explained_variance_ratio_))\\n        plt.xlabel('n components')\\n        plt.ylabel('cumulative variance');\\nWe see that for this data, nearly 100 components are required to preserve 90% of the\\nvariance. This tells us that the data is intrinsically very high dimensional—it can’t be\\ndescribed linearly with just a few components.\\nIn-Depth: Manifold Learning \\n| \\n457\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 475}, page_content=\"Figure 5-105. Cumulative variance from the PCA projection\\nWhen this is the case, nonlinear manifold embeddings like LLE and Isomap can be\\nhelpful. We can compute an Isomap embedding on these faces using the same pattern\\nshown before:\\nIn[19]: from sklearn.manifold import Isomap\\n        model = Isomap(n_components=2)\\n        proj = model.fit_transform(faces.data)\\n        proj.shape\\nOut[19]: (2370, 2)\\nThe output is a two-dimensional projection of all the input images. To get a better\\nidea of what the projection tells us, let’s define a function that will output image\\nthumbnails at the locations of the projections:\\nIn[20]: from matplotlib import offsetbox\\n        def plot_components(data, model, images=None, ax=None,\\n                            thumb_frac=0.05, cmap='gray'):\\n            ax = ax or plt.gca()\\n            proj = model.fit_transform(data)\\n            ax.plot(proj[:, 0], proj[:, 1], '.k')\\n            if images is not None:\\n                min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\\n                shown_images = np.array([2 * proj.max(0)])\\n                for i in range(data.shape[0]):\\n                    dist = np.sum((proj[i] - shown_images) ** 2, 1)\\n                    if np.min(dist) < min_dist_2:\\n                        # don't show points that are too close\\n                        continue\\n458 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 476}, page_content='shown_images = np.vstack([shown_images, proj[i]])\\n                    imagebox = offsetbox.AnnotationBbox(\\n                        offsetbox.OffsetImage(images[i], cmap=cmap),\\n                                              proj[i])\\n                    ax.add_artist(imagebox)\\nCalling this function now, we see the result (Figure 5-106):\\nIn[21]: fig, ax = plt.subplots(figsize=(10, 10))\\n        plot_components(faces.data,\\n                        model=Isomap(n_components=2),\\n                        images=faces.images[:, ::2, ::2])\\nFigure 5-106. Isomap embedding of the faces data\\nThe result is interesting: the first two Isomap dimensions seem to describe global\\nimage features: the overall darkness or lightness of the image from left to right, and\\nthe general orientation of the face from bottom to top. This gives us a nice visual\\nindication of some of the fundamental features in our data.\\nIn-Depth: Manifold Learning \\n| \\n459'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 477}, page_content=\"We could then go on to classify this data, perhaps using manifold features as inputs to\\nthe classification algorithm as we did in “In-Depth: Support Vector Machines” on\\npage 405.\\nExample: Visualizing Structure in Digits\\nAs another example of using manifold learning for visualization, let’s take a look at\\nthe MNIST handwritten digits set. This data is similar to the digits we saw in “In-\\nDepth: Decision Trees and Random Forests” on page 421, but with many more pixels\\nper image. It can be downloaded from http://mldata.org/ with the Scikit-Learn utility:\\nIn[22]: from sklearn.datasets import fetch_mldata\\n        mnist = fetch_mldata('MNIST original')\\n        mnist.data.shape\\nOut[22]: (70000, 784)\\nThis consists of 70,000 images, each with 784 pixels (i.e., the images are 28×28). As\\nbefore, we can take a look at the first few images (Figure 5-107):\\nIn[23]: fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))\\n        for i, axi in enumerate(ax.flat):\\n            axi.imshow(mnist.data[1250 * i].reshape(28, 28), cmap='gray_r')\\nFigure 5-107. Examples of the MNIST digits\\nThis gives us an idea of the variety of handwriting styles in the dataset.\\nLet’s compute a manifold learning projection across the data, illustrated in\\nFigure 5-108. For speed here, we’ll only use 1/30 of the data, which is about ~2,000\\npoints (because of the relatively poor scaling of manifold learning, I find that a few\\nthousand samples is a good number to start with for relatively quick exploration\\nbefore moving to a full calculation):\\n460 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 478}, page_content='In[24]:\\n# use only 1/30 of the data: full dataset takes a long time!\\ndata = mnist.data[::30]\\ntarget = mnist.target[::30]\\nmodel = Isomap(n_components=2)\\nproj = model.fit_transform(data)\\nplt.scatter(proj[:, 0], proj[:, 1], c=target, cmap=plt.cm.get_cmap(\\'jet\\', 10))\\nplt.colorbar(ticks=range(10))\\nplt.clim(-0.5, 9.5);\\nFigure 5-108. Isomap embedding of the MNIST digit data\\nThe resulting scatter plot shows some of the relationships between the data points,\\nbut is a bit crowded. We can gain more insight by looking at just a single number at a\\ntime (Figure 5-109):\\nIn[25]: from sklearn.manifold import Isomap\\n        # Choose 1/4 of the \"1\" digits to project\\n        data = mnist.data[mnist.target == 1][::4]\\n        fig, ax = plt.subplots(figsize=(10, 10))\\n        model = Isomap(n_neighbors=5, n_components=2, eigen_solver=\\'dense\\')\\n        plot_components(data, model, images=data.reshape((-1, 28, 28)),\\n                        ax=ax, thumb_frac=0.05, cmap=\\'gray_r\\')\\nIn-Depth: Manifold Learning \\n| \\n461'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 479}, page_content='Figure 5-109. Isomap embedding of only the 1s within the digits data\\nThe result gives you an idea of the variety of forms that the number “1” can take\\nwithin the dataset. The data lies along a broad curve in the projected space, which\\nappears to trace the orientation of the digit. As you move up the plot, you find ones\\nthat have hats and/or bases, though these are very sparse within the dataset. The pro‐\\njection lets us identify outliers that have data issues (i.e., pieces of the neighboring\\ndigits that snuck into the extracted images).\\nNow, this in itself may not be useful for the task of classifying digits, but it does help\\nus get an understanding of the data, and may give us ideas about how to move for‐\\nward, such as how we might want to preprocess the data before building a classifica‐\\ntion pipeline.\\nIn Depth: k-Means Clustering\\nIn the previous few sections, we have explored one category of unsupervised machine\\nlearning models: dimensionality reduction. Here we will move on to another class of\\nunsupervised machine learning models: clustering algorithms. Clustering algorithms\\n462 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 480}, page_content='seek to learn, from the properties of the data, an optimal division or discrete labeling\\nof groups of points.\\nMany clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps\\nthe simplest to understand is an algorithm known as k-means clustering, which is\\nimplemented in sklearn.cluster.KMeans. We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()  # for plot styling\\n       import numpy as np\\nIntroducing k-Means\\nThe k-means algorithm searches for a predetermined number of clusters within an\\nunlabeled multidimensional dataset. It accomplishes this using a simple conception of\\nwhat the optimal clustering looks like:\\n• The “cluster center” is the arithmetic mean of all the points belonging to the\\ncluster.\\n• Each point is closer to its own cluster center than to other cluster centers.\\nThose two assumptions are the basis of the k-means model. We will soon dive into\\nexactly how the algorithm reaches this solution, but for now let’s take a look at a sim‐\\nple dataset and see the k-means result.\\nFirst, let’s generate a two-dimensional dataset containing four distinct blobs. To\\nemphasize that this is an unsupervised algorithm, we will leave the labels out of the\\nvisualization (Figure 5-110):\\nIn[2]: from sklearn.datasets.samples_generator import make_blobs\\n       X, y_true = make_blobs(n_samples=300, centers=4,\\n                              cluster_std=0.60, random_state=0)\\n       plt.scatter(X[:, 0], X[:, 1], s=50);\\nBy eye, it is relatively easy to pick out the four clusters. The k-means algorithm does\\nthis automatically, and in Scikit-Learn uses the typical estimator API:\\nIn[3]: from sklearn.cluster import KMeans\\n       kmeans = KMeans(n_clusters=4)\\n       kmeans.fit(X)\\n       y_kmeans = kmeans.predict(X)\\nIn Depth: k-Means Clustering \\n| \\n463'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 481}, page_content=\"Figure 5-110. Data for demonstration of clustering\\nLet’s visualize the results by plotting the data colored by these labels. We will also plot\\nthe cluster centers as determined by the k-means estimator (Figure 5-111):\\nIn[4]: plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\\n       centers = kmeans.cluster_centers_\\n       plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\\nFigure 5-111. k-means cluster centers with clusters indicated by color\\nThe good news is that the k-means algorithm (at least in this simple case) assigns the\\npoints to clusters very similarly to how we might assign them by eye. But you might\\nwonder how this algorithm finds these clusters so quickly! After all, the number of\\npossible combinations of cluster assignments is exponential in the number of data\\n464 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 482}, page_content='points—an exhaustive search would be very, very costly. Fortunately for us, such an\\nexhaustive search is not necessary; instead, the typical approach to k-means involves\\nan intuitive iterative approach known as expectation–maximization.\\nk-Means Algorithm: Expectation–Maximization\\nExpectation–maximization (E–M) is a powerful algorithm that comes up in a variety\\nof contexts within data science. k-means is a particularly simple and easy-to-\\nunderstand application of the algorithm, and we will walk through it briefly here. In\\nshort, the expectation–maximization approach consists of the following procedure:\\n1. Guess some cluster centers\\n2. Repeat until converged\\na. E-Step: assign points to the nearest cluster center\\nb. M-Step: set the cluster centers to the mean\\nHere the “E-step” or “Expectation step” is so named because it involves updating our\\nexpectation of which cluster each point belongs to. The “M-step” or “Maximization\\nstep” is so named because it involves maximizing some fitness function that defines\\nthe location of the cluster centers—in this case, that maximization is accomplished by\\ntaking a simple mean of the data in each cluster.\\nThe literature about this algorithm is vast, but can be summarized as follows: under\\ntypical circumstances, each repetition of the E-step and M-step will always result in a\\nbetter estimate of the cluster characteristics.\\nWe can visualize the algorithm as shown in Figure 5-112.\\nFor the particular initialization shown here, the clusters converge in just three itera‐\\ntions. For an interactive version of this figure, refer to the code in the online appen‐\\ndix.\\nFigure 5-112. Visualization of the E–M algorithm for k-means\\nThe k-means algorithm is simple enough that we can write it in a few lines of code.\\nThe following is a very basic implementation (Figure 5-113):\\nIn Depth: k-Means Clustering \\n| \\n465'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 483}, page_content=\"In[5]: from sklearn.metrics import pairwise_distances_argmin\\n       def find_clusters(X, n_clusters, rseed=2):\\n           # 1. Randomly choose clusters\\n           rng = np.random.RandomState(rseed)\\n           i = rng.permutation(X.shape[0])[:n_clusters]\\n           centers = X[i]\\n           while True:\\n               # 2a. Assign labels based on closest center\\n               labels = pairwise_distances_argmin(X, centers)\\n               # 2b. Find new centers from means of points\\n               new_centers = np.array([X[labels == i].mean(0)\\n                                       for i in range(n_clusters)])\\n               # 2c. Check for convergence\\n               if np.all(centers == new_centers):\\n                   break\\n               centers = new_centers\\n           return centers, labels\\n       centers, labels = find_clusters(X, 4)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels,\\n                   s=50, cmap='viridis');\\nFigure 5-113. Data labeled with k-means\\nMost well-tested implementations will do a bit more than this under the hood, but\\nthe preceding function gives the gist of the expectation–maximization approach.\\n466 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 484}, page_content=\"Caveats of expectation–maximization\\nThere are a few issues to be aware of when using the expectation–maximization\\nalgorithm.\\nThe globally optimal result may not be achieved\\nFirst, although the E–M procedure is guaranteed to improve the result in each\\nstep, there is no assurance that it will lead to the global best solution. For exam‐\\nple, if we use a different random seed in our simple procedure, the particular\\nstarting guesses lead to poor results (Figure 5-114):\\nIn[6]: centers, labels = find_clusters(X, 4, rseed=0)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels,\\n                   s=50, cmap='viridis');\\nFigure 5-114. An example of poor convergence in k-means\\nHere the E–M approach has converged, but has not converged to a globally opti‐\\nmal configuration. For this reason, it is common for the algorithm to be run for\\nmultiple starting guesses, as indeed Scikit-Learn does by default (set by the\\nn_init parameter, which defaults to 10).\\nThe number of clusters must be selected beforehand\\nAnother common challenge with k-means is that you must tell it how many clus‐\\nters you expect: it cannot learn the number of clusters from the data. For exam‐\\nple, if we ask the algorithm to identify six clusters, it will happily proceed and\\nfind the best six clusters (Figure 5-115):\\nIn[7]: labels = KMeans(6, random_state=0).fit_predict(X)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels,\\n                   s=50, cmap='viridis');\\nIn Depth: k-Means Clustering \\n| \\n467\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 485}, page_content=\"Figure 5-115. An example where the number of clusters is chosen poorly\\nWhether the result is meaningful is a question that is difficult to answer defini‐\\ntively; one approach that is rather intuitive, but that we won’t discuss further\\nhere, is called silhouette analysis.\\nAlternatively, you might use a more complicated clustering algorithm which has\\na better quantitative measure of the fitness per number of clusters (e.g., Gaussian\\nmixture models; see “In Depth: Gaussian Mixture Models” on page 476) or which\\ncan choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity\\npropagation, all available in the sklearn.cluster submodule).\\nk-means is limited to linear cluster boundaries\\nThe fundamental model assumptions of k-means (points will be closer to their\\nown cluster center than to others) means that the algorithm will often be ineffec‐\\ntive if the clusters have complicated geometries.\\nIn particular, the boundaries between k-means clusters will always be linear,\\nwhich means that it will fail for more complicated boundaries. Consider the fol‐\\nlowing data, along with the cluster labels found by the typical k-means approach\\n(Figure 5-116):\\nIn[8]: from sklearn.datasets import make_moons\\n       X, y = make_moons(200, noise=.05, random_state=0)\\nIn[9]: labels = KMeans(2, random_state=0).fit_predict(X)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels,\\n                   s=50, cmap='viridis');\\n468 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 486}, page_content=\"Figure 5-116. Failure of k-means with nonlinear boundaries\\nThis situation is reminiscent of the discussion in “In-Depth: Support Vector\\nMachines” on page 405, where we used a kernel transformation to project the\\ndata into a higher dimension where a linear separation is possible. We might\\nimagine using the same trick to allow k-means to discover nonlinear boundaries.\\nOne version of this kernelized k-means is implemented in Scikit-Learn within the\\nSpectralClustering estimator. It uses the graph of nearest neighbors to com‐\\npute a higher-dimensional representation of the data, and then assigns labels\\nusing a k-means algorithm (Figure 5-117):\\nIn[10]: from sklearn.cluster import SpectralClustering\\n        model = SpectralClustering(n_clusters=2,\\n                                   affinity='nearest_neighbors',\\n                                   assign_labels='kmeans')\\n        labels = model.fit_predict(X)\\n        plt.scatter(X[:, 0], X[:, 1], c=labels,\\n                    s=50, cmap='viridis');\\nWe see that with this kernel transform approach, the kernelized k-means is able\\nto find the more complicated nonlinear boundaries between clusters.\\nIn Depth: k-Means Clustering \\n| \\n469\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 487}, page_content='Figure 5-117. Nonlinear boundaries learned by SpectralClustering\\nk-means can be slow for large numbers of samples\\nBecause each iteration of k-means must access every point in the dataset, the\\nalgorithm can be relatively slow as the number of samples grows. You might\\nwonder if this requirement to use all data at each iteration can be relaxed; for\\nexample, you might just use a subset of the data to update the cluster centers at\\neach step. This is the idea behind batch-based k-means algorithms, one form of\\nwhich is implemented in sklearn.cluster.MiniBatchKMeans. The interface for\\nthis is the same as for standard KMeans; we will see an example of its use as we\\ncontinue our discussion.\\nExamples\\nBeing careful about these limitations of the algorithm, we can use k-means to our\\nadvantage in a wide variety of situations. We’ll now take a look at a couple examples.\\nExample 1: k-Means on digits\\nTo start, let’s take a look at applying k-means on the same simple digits data that we\\nsaw in “In-Depth: Decision Trees and Random Forests” on page 421 and “In Depth:\\nPrincipal Component Analysis” on page 433. Here we will attempt to use k-means to\\ntry to identify similar digits without using the original label information; this might be\\nsimilar to a first step in extracting meaning from a new dataset about which you don’t\\nhave any a priori label information.\\nWe will start by loading the digits and then finding the KMeans clusters. Recall that the\\ndigits consist of 1,797 samples with 64 features, where each of the 64 features is the\\nbrightness of one pixel in an 8×8 image:\\n470 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 488}, page_content=\"In[11]: from sklearn.datasets import load_digits\\n        digits = load_digits()\\n        digits.data.shape\\nOut[11]: (1797, 64)\\nThe clustering can be performed as we did before:\\nIn[12]: kmeans = KMeans(n_clusters=10, random_state=0)\\n        clusters = kmeans.fit_predict(digits.data)\\n        kmeans.cluster_centers_.shape\\nOut[12]: (10, 64)\\nThe result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves\\nare 64-dimensional points, and can themselves be interpreted as the “typical” digit\\nwithin the cluster. Let’s see what these cluster centers look like (Figure 5-118):\\nIn[13]: fig, ax = plt.subplots(2, 5, figsize=(8, 3))\\n        centers = kmeans.cluster_centers_.reshape(10, 8, 8)\\n        for axi, center in zip(ax.flat, centers):\\n            axi.set(xticks=[], yticks=[])\\n            axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)\\nFigure 5-118. Cluster centers learned by k-means\\nWe see that even without the labels, KMeans is able to find clusters whose centers are\\nrecognizable digits, with perhaps the exception of 1 and 8.\\nBecause k-means knows nothing about the identity of the cluster, the 0–9 labels may\\nbe permuted. We can fix this by matching each learned cluster label with the true\\nlabels found in them:\\nIn[14]: from scipy.stats import mode\\n        labels = np.zeros_like(clusters)\\n        for i in range(10):\\n            mask = (clusters == i)\\n            labels[mask] = mode(digits.target[mask])[0]\\nNow we can check how accurate our unsupervised clustering was in finding similar\\ndigits within the data:\\nIn Depth: k-Means Clustering \\n| \\n471\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 489}, page_content=\"In[15]: from sklearn.metrics import accuracy_score\\n        accuracy_score(digits.target, labels)\\nOut[15]: 0.79354479688369506\\nWith just a simple k-means algorithm, we discovered the correct grouping for 80% of\\nthe input digits! Let’s check the confusion matrix for this (Figure 5-119):\\nIn[16]: from sklearn.metrics import confusion_matrix\\n        mat = confusion_matrix(digits.target, labels)\\n        sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\\n                    xticklabels=digits.target_names,\\n                    yticklabels=digits.target_names)\\n        plt.xlabel('true label')\\n        plt.ylabel('predicted label');\\nFigure 5-119. Confusion matrix for the k-means classifier\\nAs we might expect from the cluster centers we visualized before, the main point of\\nconfusion is between the eights and ones. But this still shows that using k-means, we\\ncan essentially build a digit classifier without reference to any known labels!\\nJust for fun, let’s try to push this even further. We can use the t-distributed stochastic\\nneighbor embedding (t-SNE) algorithm (mentioned in “In-Depth: Manifold Learn‐\\ning” on page 445) to preprocess the data before performing k-means. t-SNE is a non‐\\nlinear embedding algorithm that is particularly adept at preserving points within\\nclusters. Let’s see how it does:\\nIn[17]: from sklearn.manifold import TSNE\\n        # Project the data: this step will take several seconds\\n        tsne = TSNE(n_components=2, init='pca', random_state=0)\\n        digits_proj = tsne.fit_transform(digits.data)\\n472 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 490}, page_content='# Compute the clusters\\n        kmeans = KMeans(n_clusters=10, random_state=0)\\n        clusters = kmeans.fit_predict(digits_proj)\\n        # Permute the labels\\n        labels = np.zeros_like(clusters)\\n        for i in range(10):\\n            mask = (clusters == i)\\n            labels[mask] = mode(digits.target[mask])[0]\\n        # Compute the accuracy\\n        accuracy_score(digits.target, labels)\\nOut[17]: 0.93356149137451305\\nThat’s nearly 94% classification accuracy without using the labels. This is the power of\\nunsupervised learning when used carefully: it can extract information from the data‐\\nset that it might be difficult to do by hand or by eye.\\nExample 2: k-means for color compression\\nOne interesting application of clustering is in color compression within images. For\\nexample, imagine you have an image with millions of colors. In most images, a large\\nnumber of the colors will be unused, and many of the pixels in the image will have\\nsimilar or even identical colors.\\nFor example, consider the image shown in Figure 5-120, which is from Scikit-Learn’s\\ndatasets module (for this to work, you’ll have to have the pillow Python package\\ninstalled):\\nIn[18]: # Note: this requires the pillow package to be installed\\n        from sklearn.datasets import load_sample_image\\n        china = load_sample_image(\"china.jpg\")\\n        ax = plt.axes(xticks=[], yticks=[])\\n        ax.imshow(china);\\nThe image itself is stored in a three-dimensional array of size (height, width, RGB),\\ncontaining red/blue/green contributions as integers from 0 to 255:\\nIn[19]: china.shape\\nOut[19]: (427, 640, 3)\\nIn Depth: k-Means Clustering \\n| \\n473'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 491}, page_content=\"Figure 5-120. The input image\\nOne way we can view this set of pixels is as a cloud of points in a three-dimensional\\ncolor space. We will reshape the data to [n_samples x n_features], and rescale the\\ncolors so that they lie between 0 and 1:\\nIn[20]: data = china / 255.0 # use 0...1 scale\\n        data = data.reshape(427 * 640, 3)\\n        data.shape\\nOut[20]: (273280, 3)\\nWe can visualize these pixels in this color space, using a subset of 10,000 pixels for\\nefficiency (Figure 5-121):\\nIn[21]: def plot_pixels(data, title, colors=None, N=10000):\\n            if colors is None:\\n                colors = data\\n            # choose a random subset\\n            rng = np.random.RandomState(0)\\n            i = rng.permutation(data.shape[0])[:N]\\n            colors = colors[i]\\n            R, G, B = data[i].T\\n            fig, ax = plt.subplots(1, 2, figsize=(16, 6))\\n            ax[0].scatter(R, G, color=colors, marker='.')\\n            ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\\n            ax[1].scatter(R, B, color=colors, marker='.')\\n            ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\\n            fig.suptitle(title, size=20);\\nIn[22]: plot_pixels(data, title='Input color space: 16 million possible colors')\\n474 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 492}, page_content='Figure 5-121. The distribution of the pixels in RGB color space\\nNow let’s reduce these 16 million colors to just 16 colors, using a k-means clustering\\nacross the pixel space. Because we are dealing with a very large dataset, we will use\\nthe mini batch k-means, which operates on subsets of the data to compute the result\\nmuch more quickly than the standard k-means algorithm (Figure 5-122):\\nIn[23]: from sklearn.cluster import MiniBatchKMeans\\n        kmeans = MiniBatchKMeans(16)\\n        kmeans.fit(data)\\n        new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\\n        plot_pixels(data, colors=new_colors,\\n                    title=\"Reduced color space: 16 colors\")\\nFigure 5-122. 16 clusters in RGB color space\\nIn Depth: k-Means Clustering \\n| \\n475'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 493}, page_content=\"The result is a recoloring of the original pixels, where each pixel is assigned the color\\nof its closest cluster center. Plotting these new colors in the image space rather than\\nthe pixel space shows us the effect of this (Figure 5-123):\\nIn[24]:\\nchina_recolored = new_colors.reshape(china.shape)\\nfig, ax = plt.subplots(1, 2, figsize=(16, 6),\\n          subplot_kw=dict(xticks=[], yticks=[]))\\nfig.subplots_adjust(wspace=0.05)\\nax[0].imshow(china)\\nax[0].set_title('Original Image', size=16)\\nax[1].imshow(china_recolored)\\nax[1].set_title('16-color Image', size=16);\\nFigure 5-123. A comparison of the full-color image (left) and the 16-color image (right)\\nSome detail is certainly lost in the rightmost panel, but the overall image is still easily\\nrecognizable. This image on the right achieves a compression factor of around 1 mil‐\\nlion! While this is an interesting application of k-means, there are certainly better way\\nto compress information in images. But the example shows the power of thinking\\noutside of the box with unsupervised methods like k-means.\\nIn Depth: Gaussian Mixture Models\\nThe k-means clustering model explored in the previous section is simple and rela‐\\ntively easy to understand, but its simplicity leads to practical challenges in its applica‐\\ntion. In particular, the nonprobabilistic nature of k-means and its use of simple\\ndistance-from-cluster-center to assign cluster membership leads to poor performance\\nfor many real-world situations. In this section we will take a look at Gaussian mixture\\nmodels, which can be viewed as an extension of the ideas behind k-means, but can\\nalso be a powerful tool for estimation beyond simple clustering. We begin with the\\nstandard imports:\\n476 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 494}, page_content=\"In[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\n       import numpy as np\\nMotivating GMM: Weaknesses of k-Means\\nLet’s take a look at some of the weaknesses of k-means and think about how we might\\nimprove the cluster model. As we saw in the previous section, given simple, well-\\nseparated data, k-means finds suitable clustering results.\\nFor example, if we have simple blobs of data, the k-means algorithm can quickly label\\nthose clusters in a way that closely matches what we might do by eye (Figure 5-124):\\nIn[2]: # Generate some data\\n       from sklearn.datasets.samples_generator import make_blobs\\n       X, y_true = make_blobs(n_samples=400, centers=4,\\n                              cluster_std=0.60, random_state=0)\\n       X = X[:, ::-1] # flip axes for better plotting\\nIn[3]: # Plot the data with k-means labels\\n       from sklearn.cluster import KMeans\\n       kmeans = KMeans(4, random_state=0)\\n       labels = kmeans.fit(X).predict(X)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\\nFigure 5-124. k-means labels for simple data\\nFrom an intuitive standpoint, we might expect that the clustering assignment for\\nsome points is more certain than others; for example, there appears to be a very slight\\noverlap between the two middle clusters, such that we might not have complete confi‐\\ndence in the cluster assignment of points between them. Unfortunately, the k-means\\nmodel has no intrinsic measure of probability or uncertainty of cluster assignments\\nIn Depth: Gaussian Mixture Models \\n| \\n477\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 495}, page_content=\"(although it may be possible to use a bootstrap approach to estimate this uncertainty).\\nFor this, we must think about generalizing the model.\\nOne way to think about the k-means model is that it places a circle (or, in higher\\ndimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the\\nmost distant point in the cluster. This radius acts as a hard cutoff for cluster assign‐\\nment within the training set: any point outside this circle is not considered a member\\nof the cluster. We can visualize this cluster model with the following function\\n(Figure 5-125):\\nIn[4]:\\nfrom sklearn.cluster import KMeans\\nfrom scipy.spatial.distance import cdist\\ndef plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\\n    labels = kmeans.fit_predict(X)\\n    # plot the input data\\n    ax = ax or plt.gca()\\n    ax.axis('equal')\\n    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\\n    # plot the representation of the k-means model\\n    centers = kmeans.cluster_centers_\\n    radii = [cdist(X[labels == i], [center]).max()\\n             for i, center in enumerate(centers)]\\n    for c, r in zip(centers, radii):\\n        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))\\nIn[5]: kmeans = KMeans(n_clusters=4, random_state=0)\\n       plot_kmeans(kmeans, X)\\nFigure 5-125. The circular clusters implied by the k-means model\\n478 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 496}, page_content='An important observation for k-means is that these cluster models must be circular: k-\\nmeans has no built-in way of accounting for oblong or elliptical clusters. So, for\\nexample, if we take the same data and transform it, the cluster assignments end up\\nbecoming muddled (Figure 5-126):\\nIn[6]: rng = np.random.RandomState(13)\\n       X_stretched = np.dot(X, rng.randn(2, 2))\\n       kmeans = KMeans(n_clusters=4, random_state=0)\\n       plot_kmeans(kmeans, X_stretched)\\nFigure 5-126. Poor performance of k-means for noncircular clusters\\nBy eye, we recognize that these transformed clusters are noncircular, and thus circu‐\\nlar clusters would be a poor fit. Nevertheless, k-means is not flexible enough to\\naccount for this, and tries to force-fit the data into four circular clusters. This results\\nin a mixing of cluster assignments where the resulting circles overlap: see especially\\nthe bottom right of this plot. One might imagine addressing this particular situation\\nby preprocessing the data with PCA (see “In Depth: Principal Component Analysis”\\non page 433), but in practice there is no guarantee that such a global operation will\\ncircularize the individual data.\\nThese two disadvantages of k-means—its lack of flexibility in cluster shape and lack\\nof probabilistic cluster assignment—mean that for many datasets (especially low-\\ndimensional datasets) it may not perform as well as you might hope.\\nYou might imagine addressing these weaknesses by generalizing the k-means model:\\nfor example, you could measure uncertainty in cluster assignment by comparing the\\ndistances of each point to all cluster centers, rather than focusing on just the closest.\\nYou might also imagine allowing the cluster boundaries to be ellipses rather than cir‐\\nIn Depth: Gaussian Mixture Models \\n| \\n479'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 497}, page_content=\"cles, so as to account for noncircular clusters. It turns out these are two essential com‐\\nponents of a different type of clustering model, Gaussian mixture models.\\nGeneralizing E–M: Gaussian Mixture Models\\nA Gaussian mixture model (GMM) attempts to find a mixture of multidimensional\\nGaussian probability distributions that best model any input dataset. In the simplest\\ncase, GMMs can be used for finding clusters in the same manner as k-means\\n(Figure 5-127):\\nIn[7]: from sklearn.mixture import GMM\\n       gmm = GMM(n_components=4).fit(X)\\n       labels = gmm.predict(X)\\n       plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\\nFigure 5-127. Gaussian mixture model labels for the data\\nBut because GMM contains a probabilistic model under the hood, it is also possible\\nto find probabilistic cluster assignments—in Scikit-Learn we do this using the pre\\ndict_proba method. This returns a matrix of size [n_samples, n_clusters] that\\nmeasures the probability that any point belongs to the given cluster:\\nIn[8]: probs = gmm.predict_proba(X)\\n       print(probs[:5].round(3))\\n[[ 0.     0.     0.475  0.525]\\n [ 0.     1.     0.     0.   ]\\n [ 0.     1.     0.     0.   ]\\n [ 0.     0.     0.     1.   ]\\n [ 0.     1.     0.     0.   ]]\\nWe can visualize this uncertainty by, for example, making the size of each point pro‐\\nportional to the certainty of its prediction; looking at Figure 5-128, we can see that it\\n480 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 498}, page_content='is precisely the points at the boundaries between clusters that reflect this uncertainty\\nof cluster assignment:\\nIn[9]: size = 50 * probs.max(1) ** 2  # square emphasizes differences\\n       plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\\'viridis\\', s=size);\\nFigure 5-128. GMM probablistic labels: probabilities are shown by the size of points\\nUnder the hood, a Gaussian mixture model is very similar to k-means: it uses an\\nexpectation–maximization approach that qualitatively does the following:\\n1. Choose starting guesses for the location and shape\\n2. Repeat until converged:\\na. E-step: for each point, find weights encoding the probability of membership in\\neach cluster\\nb. M-step: for each cluster, update its location, normalization, and shape based\\non all data points, making use of the weights\\nThe result of this is that each cluster is associated not with a hard-edged sphere, but\\nwith a smooth Gaussian model. Just as in the k-means expectation–maximization\\napproach, this algorithm can sometimes miss the globally optimal solution, and thus\\nin practice multiple random initializations are used.\\nLet’s create a function that will help us visualize the locations and shapes of the GMM\\nclusters by drawing ellipses based on the gmm output:\\nIn[10]:\\nfrom matplotlib.patches import Ellipse\\ndef draw_ellipse(position, covariance, ax=None, **kwargs):\\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\\nIn Depth: Gaussian Mixture Models \\n| \\n481'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 499}, page_content=\"ax = ax or plt.gca()\\n    # Convert covariance to principal axes\\n    if covariance.shape == (2, 2):\\n        U, s, Vt = np.linalg.svd(covariance)\\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\\n        width, height = 2 * np.sqrt(s)\\n    else:\\n        angle = 0\\n        width, height = 2 * np.sqrt(covariance)\\n    # Draw the ellipse\\n    for nsig in range(1, 4):\\n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\\n                             angle, **kwargs))\\ndef plot_gmm(gmm, X, label=True, ax=None):\\n    ax = ax or plt.gca()\\n    labels = gmm.fit(X).predict(X)\\n    if label:\\n        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\\n    else:\\n        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\\n    ax.axis('equal')\\n    w_factor = 0.2 / gmm.weights_.max()\\n    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):\\n        draw_ellipse(pos, covar, alpha=w * w_factor)\\nWith this in place, we can take a look at what the four-component GMM gives us for\\nour initial data (Figure 5-129):\\nIn[11]: gmm = GMM(n_components=4, random_state=42)\\n        plot_gmm(gmm, X)\\n482 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 500}, page_content=\"Figure 5-129. Representation of the four-component GMM in the presence of circular\\nclusters\\nSimilarly, we can use the GMM approach to fit our stretched dataset; allowing for a\\nfull covariance, the model will fit even very oblong, stretched-out clusters\\n(Figure 5-130):\\nIn[12]: gmm = GMM(n_components=4, covariance_type='full', random_state=42)\\n        plot_gmm(gmm, X_stretched)\\nFigure 5-130. Representation of the four-component GMM in the presence of noncircu‐\\nlar clusters\\nThis makes clear that GMMs address the two main practical issues with k-means\\nencountered before.\\nIn Depth: Gaussian Mixture Models \\n| \\n483\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 501}, page_content='Choosing the covariance type\\nIf you look at the details of the preceding fits, you will see that the covariance_type\\noption was set differently within each. This hyperparameter controls the degrees of\\nfreedom in the shape of each cluster; it is essential to set this carefully for any given\\nproblem. The default is covariance_type=\"diag\", which means that the size of the\\ncluster along each dimension can be set independently, with the resulting ellipse\\nconstrained to align with the axes. A slightly simpler and faster model is cova\\nriance_type=\"spherical\", which constrains the shape of the cluster such that all\\ndimensions are equal. The resulting clustering will have similar characteristics to that\\nof k-means, though it is not entirely equivalent. A more complicated and computa‐\\ntionally expensive model (especially as the number of dimensions grows) is to use\\ncovariance_type=\"full\", which allows each cluster to be modeled as an ellipse with\\narbitrary orientation.\\nWe can see a visual representation of these three choices for a single cluster within\\nFigure 5-131:\\nFigure 5-131. Visualization of GMM covariance types\\nGMM as Density Estimation\\nThough GMM is often categorized as a clustering algorithm, fundamentally it is an\\nalgorithm for density estimation. That is to say, the result of a GMM fit to some data is\\ntechnically not a clustering model, but a generative probabilistic model describing the\\ndistribution of the data.\\nAs an example, consider some data generated from Scikit-Learn’s make_moons func‐\\ntion (visualized in Figure 5-132), which we saw in “In Depth: k-Means Clustering” on\\npage 462:\\nIn[13]: from sklearn.datasets import make_moons\\n        Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\\n        plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);\\n484 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 502}, page_content=\"Figure 5-132. GMM applied to clusters with nonlinear boundaries\\nIf we try to fit this to a two-component GMM viewed as a clustering model, the\\nresults are not particularly useful (Figure 5-133):\\nIn[14]: gmm2 = GMM(n_components=2, covariance_type='full', random_state=0)\\n        plot_gmm(gmm2, Xmoon)\\nFigure 5-133. Two component GMM fit to nonlinear clusters\\nBut if we instead use many more components and ignore the cluster labels, we find a\\nfit that is much closer to the input data (Figure 5-134):\\nIn[15]: gmm16 = GMM(n_components=16, covariance_type='full', random_state=0)\\n        plot_gmm(gmm16, Xmoon, label=False)\\nIn Depth: Gaussian Mixture Models \\n| \\n485\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 503}, page_content='Figure 5-134. Using many GMM clusters to model the distribution of points\\nHere the mixture of 16 Gaussians serves not to find separated clusters of data, but\\nrather to model the overall distribution of the input data. This is a generative model of\\nthe distribution, meaning that the GMM gives us the recipe to generate new random\\ndata distributed similarly to our input. For example, here are 400 new points drawn\\nfrom this 16-component GMM fit to our original data (Figure 5-135):\\nIn[16]: Xnew = gmm16.sample(400, random_state=42)\\n        plt.scatter(Xnew[:, 0], Xnew[:, 1]);\\nFigure 5-135. New data drawn from the 16-component GMM\\nGMM is convenient as a flexible means of modeling an arbitrary multidimensional\\ndistribution of data.\\n486 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 504}, page_content=\"How many components?\\nThe fact that GMM is a generative model gives us a natural means of determining the\\noptimal number of components for a given dataset. A generative model is inherently\\na probability distribution for the dataset, and so we can simply evaluate the likelihood\\nof the data under the model, using cross-validation to avoid overfitting. Another\\nmeans of correcting for overfitting is to adjust the model likelihoods using some ana‐\\nlytic criterion such as the Akaike information criterion (AIC) or the Bayesian infor‐\\nmation criterion (BIC). Scikit-Learn’s GMM estimator actually includes built-in\\nmethods that compute both of these, and so it is very easy to operate on this\\napproach.\\nLet’s look at the AIC and BIC as a function as the number of GMM components for\\nour moon dataset (Figure 5-136):\\nIn[17]: n_components = np.arange(1, 21)\\n        models = [GMM(n, covariance_type='full', random_state=0).fit(Xmoon)\\n                  for n in n_components]\\n        plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')\\n        plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')\\n        plt.legend(loc='best')\\n        plt.xlabel('n_components');\\nFigure 5-136. Visualization of AIC and BIC for choosing the number of GMM\\ncomponents\\nThe optimal number of clusters is the value that minimizes the AIC or BIC, depend‐\\ning on which approximation we wish to use. The AIC tells us that our choice of 16\\ncomponents was probably too many: around 8–12 components would have been a\\nIn Depth: Gaussian Mixture Models \\n| \\n487\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 505}, page_content=\"better choice. As is typical with this sort of problem, the BIC recommends a simpler\\nmodel.\\nNotice the important point: this choice of number of components measures how well\\nGMM works as a density estimator, not how well it works as a clustering algorithm. I’d\\nencourage you to think of GMM primarily as a density estimator, and use it for clus‐\\ntering only when warranted within simple datasets.\\nExample: GMM for Generating New Data\\nWe just saw a simple example of using GMM as a generative model of data in order to\\ncreate new samples from the distribution defined by the input data. Here we will run\\nwith this idea and generate new handwritten digits from the standard digits corpus\\nthat we have used before.\\nTo start with, let’s load the digits data using Scikit-Learn’s data tools:\\nIn[18]: from sklearn.datasets import load_digits\\n        digits = load_digits()\\n        digits.data.shape\\nOut[18]: (1797, 64)\\nNext let’s plot the first 100 of these to recall exactly what we’re looking at\\n(Figure 5-137):\\nIn[19]: def plot_digits(data):\\n            fig, ax = plt.subplots(10, 10, figsize=(8, 8),\\n                                   subplot_kw=dict(xticks=[], yticks=[]))\\n            fig.subplots_adjust(hspace=0.05, wspace=0.05)\\n            for i, axi in enumerate(ax.flat):\\n                im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\\n                im.set_clim(0, 16)\\n        plot_digits(digits.data)\\nWe have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top of\\nthese to generate more. GMMs can have difficulty converging in such a high dimen‐\\nsional space, so we will start with an invertible dimensionality reduction algorithm on\\nthe data. Here we will use a straightforward PCA, asking it to preserve 99% of the\\nvariance in the projected data:\\nIn[20]: from sklearn.decomposition import PCA\\n        pca = PCA(0.99, whiten=True)\\n        data = pca.fit_transform(digits.data)\\n        data.shape\\nOut[20]: (1797, 41)\\n488 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 506}, page_content=\"Figure 5-137. Handwritten digits input\\nThe result is 41 dimensions, a reduction of nearly 1/3 with almost no information\\nloss. Given this projected data, let’s use the AIC to get a gauge for the number of\\nGMM components we should use (Figure 5-138):\\nIn[21]: n_components = np.arange(50, 210, 10)\\n        models = [GMM(n, covariance_type='full', random_state=0)\\n                  for n in n_components]\\n        aics = [model.fit(data).aic(data) for model in models]\\n        plt.plot(n_components, aics);\\nIt appears that around 110 components minimizes the AIC; we will use this model.\\nLet’s quickly fit this to the data and confirm that it has converged:\\nIn[22]: gmm = GMM(110, covariance_type='full', random_state=0)\\n        gmm.fit(data)\\n        print(gmm.converged_)\\nTrue\\nNow we can draw samples of 100 new points within this 41-dimensional projected\\nspace, using the GMM as a generative model:\\nIn[23]: data_new = gmm.sample(100, random_state=0)\\n        data_new.shape\\nOut[23]: (100, 41)\\nIn Depth: Gaussian Mixture Models \\n| \\n489\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 507}, page_content='Figure 5-138. AIC curve for choosing the appropriate number of GMM components\\nFinally, we can use the inverse transform of the PCA object to construct the new dig‐\\nits (Figure 5-139):\\nIn[24]: digits_new = pca.inverse_transform(data_new)\\n        plot_digits(digits_new)\\nFigure 5-139. “New” digits randomly drawn from the underlying model of the GMM\\nestimator\\n490 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 508}, page_content='The results for the most part look like plausible digits from the dataset!\\nConsider what we’ve done here: given a sampling of handwritten digits, we have\\nmodeled the distribution of that data in such a way that we can generate brand new\\nsamples of digits from the data: these are “handwritten digits” that do not individu‐\\nally appear in the original dataset, but rather capture the general features of the input\\ndata as modeled by the mixture model. Such a generative model of digits can prove\\nvery useful as a component of a Bayesian generative classifier, as we shall see in the\\nnext section.\\nIn-Depth: Kernel Density Estimation\\nIn the previous section we covered Gaussian mixture models (GMM), which are a\\nkind of hybrid between a clustering estimator and a density estimator. Recall that a\\ndensity estimator is an algorithm that takes a D-dimensional dataset and produces an\\nestimate of the D-dimensional probability distribution which that data is drawn from.\\nThe GMM algorithm accomplishes this by representing the density as a weighted\\nsum of Gaussian distributions. Kernel density estimation (KDE) is in some senses an\\nalgorithm that takes the mixture-of-Gaussians idea to its logical extreme: it uses a\\nmixture consisting of one Gaussian component per point, resulting in an essentially\\nnonparametric estimator of density. In this section, we will explore the motivation\\nand uses of KDE. We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\n       import numpy as np\\nMotivating KDE: Histograms\\nAs already discussed, a density estimator is an algorithm that seeks to model the\\nprobability distribution that generated a dataset. For one-dimensional data, you are\\nprobably already familiar with one simple density estimator: the histogram. A histo‐\\ngram divides the data into discrete bins, counts the number of points that fall in each\\nbin, and then visualizes the results in an intuitive manner.\\nFor example, let’s create some data that is drawn from two normal distributions:\\nIn[2]: def make_data(N, f=0.3, rseed=1):\\n           rand = np.random.RandomState(rseed)\\n           x = rand.randn(N)\\n           x[int(f * N):] += 5\\n           return x\\n       x = make_data(1000)\\nIn-Depth: Kernel Density Estimation \\n| \\n491'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 509}, page_content=\"We have previously seen that the standard count-based histogram can be created with\\nthe plt.hist() function. By specifying the normed parameter of the histogram, we\\nend up with a normalized histogram where the height of the bins does not reflect\\ncounts, but instead reflects probability density (Figure 5-140):\\nIn[3]: hist = plt.hist(x, bins=30, normed=True)\\nFigure 5-140. Data drawn from a combination of normal distributions\\nNotice that for equal binning, this normalization simply changes the scale on the y-\\naxis, leaving the relative heights essentially the same as in a histogram built from\\ncounts. This normalization is chosen so that the total area under the histogram is\\nequal to 1, as we can confirm by looking at the output of the histogram function:\\nIn[4]: density, bins, patches = hist\\n       widths = bins[1:] - bins[:-1]\\n       (density * widths).sum()\\nOut[4]: 1.0\\nOne of the issues with using a histogram as a density estimator is that the choice of\\nbin size and location can lead to representations that have qualitatively different fea‐\\ntures. For example, if we look at a version of this data with only 20 points, the choice\\nof how to draw the bins can lead to an entirely different interpretation of the data!\\nConsider this example (visualized in Figure 5-141):\\nIn[5]: x = make_data(20)\\n       bins = np.linspace(-5, 10, 10)\\nIn[6]: fig, ax = plt.subplots(1, 2, figsize=(12, 4),\\n                              sharex=True, sharey=True,\\n                              subplot_kw={'xlim':(-4, 9),\\n                                          'ylim':(-0.02, 0.3)})\\n       fig.subplots_adjust(wspace=0.05)\\n492 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 510}, page_content=\"for i, offset in enumerate([0.0, 0.6]):\\n           ax[i].hist(x, bins=bins + offset, normed=True)\\n           ax[i].plot(x, np.full_like(x, -0.01), '|k',\\n                      markeredgewidth=1)\\nFigure 5-141. The problem with histograms: the location of bins can affect interpretation\\nOn the left, the histogram makes clear that this is a bimodal distribution. On the\\nright, we see a unimodal distribution with a long tail. Without seeing the preceding\\ncode, you would probably not guess that these two histograms were built from the\\nsame data. With that in mind, how can you trust the intuition that histograms confer?\\nAnd how might we improve on this?\\nStepping back, we can think of a histogram as a stack of blocks, where we stack one\\nblock within each bin on top of each point in the dataset. Let’s view this directly\\n(Figure 5-142):\\nIn[7]: fig, ax = plt.subplots()\\n       bins = np.arange(-3, 8)\\n       ax.plot(x, np.full_like(x, -0.1), '|k',\\n               markeredgewidth=1)\\n       for count, edge in zip(*np.histogram(x, bins)):\\n           for i in range(count):\\n               ax.add_patch(plt.Rectangle((edge, i), 1, 1,\\n                                          alpha=0.5))\\n       ax.set_xlim(-4, 8)\\n       ax.set_ylim(-0.2, 8)\\nOut[7]: (-0.2, 8)\\nIn-Depth: Kernel Density Estimation \\n| \\n493\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 511}, page_content=\"Figure 5-142. Histogram as stack of blocks\\nThe problem with our two binnings stems from the fact that the height of the block\\nstack often reflects not on the actual density of points nearby, but on coincidences of\\nhow the bins align with the data points. This misalignment between points and their\\nblocks is a potential cause of the poor histogram results seen here. But what if, instead\\nof stacking the blocks aligned with the bins, we were to stack the blocks aligned with\\nthe points they represent? If we do this, the blocks won’t be aligned, but we can add\\ntheir contributions at each location along the x-axis to find the result. Let’s try this\\n(Figure 5-143):\\nIn[8]: x_d = np.linspace(-4, 8, 2000)\\n       density = sum((abs(xi - x_d) < 0.5) for xi in x)\\n       plt.fill_between(x_d, density, alpha=0.5)\\n       plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\\n       plt.axis([-4, 8, -0.2, 8]);\\n494 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 512}, page_content=\"Figure 5-143. A “histogram” where blocks center on each individual point; this is an\\nexample of a kernel density estimate\\nThe result looks a bit messy, but is a much more robust reflection of the actual data\\ncharacteristics than is the standard histogram. Still, the rough edges are not aestheti‐\\ncally pleasing, nor are they reflective of any true properties of the data. In order to\\nsmooth them out, we might decide to replace the blocks at each location with a\\nsmooth function, like a Gaussian. Let’s use a standard normal curve at each point\\ninstead of a block (Figure 5-144):\\nIn[9]: from scipy.stats import norm\\n       x_d = np.linspace(-4, 8, 1000)\\n       density = sum(norm(xi).pdf(x_d) for xi in x)\\n       plt.fill_between(x_d, density, alpha=0.5)\\n       plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\\n       plt.axis([-4, 8, -0.2, 5]);\\nIn-Depth: Kernel Density Estimation \\n| \\n495\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 513}, page_content='Figure 5-144. A kernel density estimate with a Gaussian kernel\\nThis smoothed-out plot, with a Gaussian distribution contributed at the location of\\neach input point, gives a much more accurate idea of the shape of the data distribu‐\\ntion, and one that has much less variance (i.e., changes much less in response to dif‐\\nferences in sampling).\\nThese last two plots are examples of kernel density estimation in one dimension: the\\nfirst uses a so-called “tophat” kernel and the second uses a Gaussian kernel. We’ll now\\nlook at kernel density estimation in more detail.\\nKernel Density Estimation in Practice\\nThe free parameters of kernel density estimation are the kernel, which specifies the\\nshape of the distribution placed at each point, and the kernel bandwidth, which con‐\\ntrols the size of the kernel at each point. In practice, there are many kernels you might\\nuse for a kernel density estimation: in particular, the Scikit-Learn KDE implementa‐\\ntion supports one of six kernels, which you can read about in Scikit-Learn’s Density\\nEstimation documentation.\\nWhile there are several versions of kernel density estimation implemented in Python\\n(notably in the SciPy and StatsModels packages), I prefer to use Scikit-Learn’s version\\nbecause of its efficiency and flexibility. It is implemented in the sklearn.neigh\\nbors.KernelDensity estimator, which handles KDE in multiple dimensions with one\\nof six kernels and one of a couple dozen distance metrics. Because KDE can be fairly\\ncomputationally intensive, the Scikit-Learn estimator uses a tree-based algorithm\\nunder the hood and can trade off computation time for accuracy using the atol\\n(absolute tolerance) and rtol (relative tolerance) parameters. We can determine the\\n496 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 514}, page_content=\"kernel bandwidth, which is a free parameter, using Scikit-Learn’s standard cross-\\nvalidation tools, as we will soon see.\\nLet’s first see a simple example of replicating the preceding plot using the Scikit-Learn\\nKernelDensity estimator (Figure 5-145):\\nIn[10]: from sklearn.neighbors import KernelDensity\\n        # instantiate and fit the KDE model\\n        kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\\n        kde.fit(x[:, None])\\n        # score_samples returns the log of the probability density\\n        logprob = kde.score_samples(x_d[:, None])\\n        plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\\n        plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\\n        plt.ylim(-0.02, 0.22)\\nOut[10]: (-0.02, 0.22)\\nFigure 5-145. A kernel density estimate computed with Scikit-Learn\\nThe result here is normalized such that the area under the curve is equal to 1.\\nSelecting the bandwidth via cross-validation\\nThe choice of bandwidth within KDE is extremely important to finding a suitable\\ndensity estimate, and is the knob that controls the bias–variance trade-off in the esti‐\\nmate of density: too narrow a bandwidth leads to a high-variance estimate (i.e., over‐\\nfitting), where the presence or absence of a single point makes a large difference. Too\\nwide a bandwidth leads to a high-bias estimate (i.e., underfitting) where the structure\\nin the data is washed out by the wide kernel.\\nIn-Depth: Kernel Density Estimation \\n| \\n497\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 515}, page_content=\"There is a long history in statistics of methods to quickly estimate the best bandwidth\\nbased on rather stringent assumptions about the data: if you look up the KDE imple‐\\nmentations in the SciPy and StatsModels packages, for example, you will see imple‐\\nmentations based on some of these rules.\\nIn machine learning contexts, we’ve seen that such hyperparameter tuning often is\\ndone empirically via a cross-validation approach. With this in mind, the KernelDen\\nsity estimator in Scikit-Learn is designed such that it can be used directly within\\nScikit-Learn’s standard grid search tools. Here we will use GridSearchCV to optimize\\nthe bandwidth for the preceding dataset. Because we are looking at such a small data‐\\nset, we will use leave-one-out cross-validation, which minimizes the reduction in\\ntraining set size for each cross-validation trial:\\nIn[11]: from sklearn.grid_search import GridSearchCV\\n        from sklearn.cross_validation import LeaveOneOut\\n        bandwidths = 10 ** np.linspace(-1, 1, 100)\\n        grid = GridSearchCV(KernelDensity(kernel='gaussian'),\\n                            {'bandwidth': bandwidths},\\n                            cv=LeaveOneOut(len(x)))\\n        grid.fit(x[:, None]);\\nNow we can find the choice of bandwidth that maximizes the score (which in this\\ncase defaults to the log-likelihood):\\nIn[12]: grid.best_params_\\nOut[12]: {'bandwidth': 1.1233240329780276}\\nThe optimal bandwidth happens to be very close to what we used in the example plot\\nearlier, where the bandwidth was 1.0 (i.e., the default width of scipy.stats.norm).\\nExample: KDE on a Sphere\\nPerhaps the most common use of KDE is in graphically representing distributions of\\npoints. For example, in the Seaborn visualization library (discussed earlier in “Visual‐\\nization with Seaborn” on page 311), KDE is built in and automatically used to help\\nvisualize points in one and two dimensions.\\nHere we will look at a slightly more sophisticated use of KDE for visualization of dis‐\\ntributions. We will make use of some geographic data that can be loaded with Scikit-\\nLearn: the geographic distributions of recorded observations of two South American\\nmammals, Bradypus variegatus (the brown-throated sloth) and Microryzomys minu‐\\ntus (the forest small rice rat).\\nWith Scikit-Learn, we can fetch this data as follows:\\nIn[13]: from sklearn.datasets import fetch_species_distributions\\n        data = fetch_species_distributions()\\n498 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 516}, page_content=\"# Get matrices/arrays of species IDs and locations\\n        latlon = np.vstack([data.train['dd lat'],\\n                            data.train['dd long']]).T\\n        species = np.array([d.decode('ascii').startswith('micro')\\n                            for d in data.train['species']], dtype='int')\\nWith this data loaded, we can use the Basemap toolkit (mentioned previously in\\n“Geographic Data with Basemap” on page 298) to plot the observed locations of these\\ntwo species on the map of South America (Figure 5-146):\\nIn[14]: from mpl_toolkits.basemap import Basemap\\n        from sklearn.datasets.species_distributions import construct_grids\\n        xgrid, ygrid = construct_grids(data)\\n        # plot coastlines with Basemap\\n        m = Basemap(projection='cyl', resolution='c',\\n                    llcrnrlat=ygrid.min(), urcrnrlat=ygrid.max(),\\n                    llcrnrlon=xgrid.min(), urcrnrlon=xgrid.max())\\n        m.drawmapboundary(fill_color='#DDEEFF')\\n        m.fillcontinents(color='#FFEEDD')\\n        m.drawcoastlines(color='gray', zorder=2)\\n        m.drawcountries(color='gray', zorder=2)\\n        # plot locations\\n        m.scatter(latlon[:, 1], latlon[:, 0], zorder=3,\\n                  c=species, cmap='rainbow', latlon=True);\\nFigure 5-146. Location of species in training data\\nUnfortunately, this doesn’t give a very good idea of the density of the species, because\\npoints in the species range may overlap one another. You may not realize it by looking\\nat this plot, but there are over 1,600 points shown here!\\nIn-Depth: Kernel Density Estimation \\n| \\n499\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 517}, page_content=\"Let’s use kernel density estimation to show this distribution in a more interpretable\\nway: as a smooth indication of density on the map. Because the coordinate system\\nhere lies on a spherical surface rather than a flat plane, we will use the haversine\\ndistance metric, which will correctly represent distances on a curved surface.\\nThere is a bit of boilerplate code here (one of the disadvantages of the Basemap tool‐\\nkit), but the meaning of each code block should be clear (Figure 5-147):\\nIn[15]:\\n# Set up the data grid for the contour plot\\nX, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\\nland_reference = data.coverages[6][::5, ::5]\\nland_mask = (land_reference > -9999).ravel()\\nxy = np.vstack([Y.ravel(), X.ravel()]).T\\nxy = np.radians(xy[land_mask])\\n# Create two side-by-side plots\\nfig, ax = plt.subplots(1, 2)\\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\\nspecies_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\\ncmaps = ['Purples', 'Reds']\\nfor i, axi in enumerate(ax):\\n    axi.set_title(species_names[i])\\n    # plot coastlines with Basemap\\n    m = Basemap(projection='cyl', llcrnrlat=Y.min(),\\n                urcrnrlat=Y.max(), llcrnrlon=X.min(),\\n                urcrnrlon=X.max(), resolution='c', ax=axi)\\n    m.drawmapboundary(fill_color='#DDEEFF')\\n    m.drawcoastlines()\\n    m.drawcountries()\\n    # construct a spherical kernel density estimate of the distribution\\n    kde = KernelDensity(bandwidth=0.03, metric='haversine')\\n    kde.fit(np.radians(latlon[species == i]))\\n    # evaluate only on the land: -9999 indicates ocean\\n    Z = np.full(land_mask.shape[0], -9999.0)\\n    Z[land_mask] = np.exp(kde.score_samples(xy))\\n    Z = Z.reshape(X.shape)\\n    # plot contours of the density\\n    levels = np.linspace(0, Z.max(), 25)\\n    axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])\\n500 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 518}, page_content='Figure 5-147. A kernel density representation of the species distributions\\nCompared to the simple scatter plot we initially used, this visualization paints a much\\nclearer picture of the geographical distribution of observations of these two species.\\nExample: Not-So-Naive Bayes\\nThis example looks at Bayesian generative classification with KDE, and demonstrates\\nhow to use the Scikit-Learn architecture to create a custom estimator.\\nIn “In Depth: Naive Bayes Classification” on page 382, we took a look at naive Baye‐\\nsian classification, in which we created a simple generative model for each class, and\\nused these models to build a fast classifier. For naive Bayes, the generative model is a\\nsimple axis-aligned Gaussian. With a density estimation algorithm like KDE, we can\\nremove the “naive” element and perform the same classification with a more sophisti‐\\ncated generative model for each class. It’s still Bayesian classification, but it’s no longer\\nnaive.\\nThe general approach for generative classification is this:\\n1. Split the training data by label.\\n2. For each set, fit a KDE to obtain a generative model of the data. This allows you\\nfor any observation x and label y to compute a likelihood P x\\ny .\\n3. From the number of examples of each class in the training set, compute the class\\nprior, P y .\\n4. For an unknown point x, the posterior probability for each class is\\nP y\\nx ∝P x\\ny P y . The class that maximizes this posterior is the label\\nassigned to the point.\\nIn-Depth: Kernel Density Estimation \\n| \\n501'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 519}, page_content='The algorithm is straightforward and intuitive to understand; the more difficult piece\\nis couching it within the Scikit-Learn framework in order to make use of the grid\\nsearch and cross-validation architecture.\\nThis is the code that implements the algorithm within the Scikit-Learn framework;\\nwe will step through it following the code block:\\nIn[16]: from sklearn.base import BaseEstimator, ClassifierMixin\\n        class KDEClassifier(BaseEstimator, ClassifierMixin):\\n            \"\"\"Bayesian generative classification based on KDE\\n            Parameters\\n            ----------\\n            bandwidth : float\\n                the kernel bandwidth within each class\\n            kernel : str\\n                the kernel name, passed to KernelDensity\\n            \"\"\"\\n            def __init__(self, bandwidth=1.0, kernel=\\'gaussian\\'):\\n                self.bandwidth = bandwidth\\n                self.kernel = kernel\\n            def fit(self, X, y):\\n                self.classes_ = np.sort(np.unique(y))\\n                training_sets = [X[y == yi] for yi in self.classes_]\\n                self.models_ = [KernelDensity(bandwidth=self.bandwidth,\\n                                              kernel=self.kernel).fit(Xi)\\n                                for Xi in training_sets]\\n                self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\\n                                   for Xi in training_sets]\\n                return self\\n            def predict_proba(self, X):\\n                logprobs = np.array([model.score_samples(X)\\n                                     for model in self.models_]).T\\n                result = np.exp(logprobs + self.logpriors_)\\n                return result / result.sum(1, keepdims=True)\\n            def predict(self, X):\\n                return self.classes_[np.argmax(self.predict_proba(X), 1)]\\nThe anatomy of a custom estimator\\nLet’s step through this code and discuss the essential features:\\nfrom sklearn.base import BaseEstimator, ClassifierMixin\\nclass KDEClassifier(BaseEstimator, ClassifierMixin):\\n    \"\"\"Bayesian generative classification based on KDE\\n502 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 520}, page_content='Parameters\\n    ----------\\n    bandwidth : float\\n        the kernel bandwidth within each class\\n    kernel : str\\n        the kernel name, passed to KernelDensity\\n    \"\"\"\\nEach estimator in Scikit-Learn is a class, and it is most convenient for this class to\\ninherit from the BaseEstimator class as well as the appropriate mixin, which pro‐\\nvides standard functionality. For example, among other things, here the BaseEstima\\ntor contains the logic necessary to clone/copy an estimator for use in a cross-\\nvalidation procedure, and ClassifierMixin defines a default score() method used\\nby such routines. We also provide a docstring, which will be captured by IPython’s\\nhelp functionality (see “Help and Documentation in IPython” on page 3).\\nNext comes the class initialization method:\\n    def __init__(self, bandwidth=1.0, kernel=\\'gaussian\\'):\\n        self.bandwidth = bandwidth\\n        self.kernel = kernel\\nThis is the actual code executed when the object is instantiated with KDEClassi\\nfier(). In Scikit-Learn, it is important that initialization contains no operations other\\nthan assigning the passed values by name to self. This is due to the logic contained\\nin BaseEstimator required for cloning and modifying estimators for cross-\\nvalidation, grid search, and other functions. Similarly, all arguments to __init__\\nshould be explicit; that is, *args or **kwargs should be avoided, as they will not be\\ncorrectly handled within cross-validation routines.\\nNext comes the fit() method, where we handle training data:\\n    def fit(self, X, y):\\n        self.classes_ = np.sort(np.unique(y))\\n        training_sets = [X[y == yi] for yi in self.classes_]\\n        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\\n                                      kernel=self.kernel).fit(Xi)\\n                        for Xi in training_sets]\\n        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\\n                           for Xi in training_sets]\\n        return self\\nHere we find the unique classes in the training data, train a KernelDensity model for\\neach class, and compute the class priors based on the number of input samples.\\nFinally, fit() should always return self so that we can chain commands. For\\nexample:\\nlabel = model.fit(X, y).predict(X)\\nIn-Depth: Kernel Density Estimation \\n| \\n503'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 521}, page_content=\"Notice that each persistent result of the fit is stored with a trailing underscore (e.g.,\\nself.logpriors_). This is a convention used in Scikit-Learn so that you can quickly\\nscan the members of an estimator (using IPython’s tab completion) and see exactly\\nwhich members are fit to training data.\\nFinally, we have the logic for predicting labels on new data:\\n    def predict_proba(self, X):\\n        logprobs = np.vstack([model.score_samples(X)\\n                              for model in self.models_]).T\\n        result = np.exp(logprobs + self.logpriors_)\\n        return result / result.sum(1, keepdims=True)\\n    def predict(self, X):\\n        return self.classes_[np.argmax(self.predict_proba(X), 1)]\\nBecause this is a probabilistic classifier, we first implement predict_proba(), which\\nreturns an array of class probabilities of shape [n_samples, n_classes]. Entry [i,\\nj] of this array is the posterior probability that sample i is a member of class j, com‐\\nputed by multiplying the likelihood by the class prior and normalizing.\\nFinally, the predict() method uses these probabilities and simply returns the class\\nwith the largest probability.\\nUsing our custom estimator\\nLet’s try this custom estimator on a problem we have seen before: the classification of\\nhandwritten digits. Here we will load the digits, and compute the cross-validation\\nscore for a range of candidate bandwidths using the GridSearchCV meta-estimator\\n(refer back to “Hyperparameters and Model Validation” on page 359 for more infor‐\\nmation on this):\\nIn[17]: from sklearn.datasets import load_digits\\n        from sklearn.grid_search import GridSearchCV\\n        digits = load_digits()\\n        bandwidths = 10 ** np.linspace(0, 2, 100)\\n        grid = GridSearchCV(KDEClassifier(), {'bandwidth': bandwidths})\\n        grid.fit(digits.data, digits.target)\\n        scores = [val.mean_validation_score for val in grid.grid_scores_]\\nNext we can plot the cross-validation score as a function of bandwidth\\n(Figure 5-148):\\nIn[18]: plt.semilogx(bandwidths, scores)\\n        plt.xlabel('bandwidth')\\n        plt.ylabel('accuracy')\\n        plt.title('KDE Model Performance')\\n504 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 522}, page_content=\"print(grid.best_params_)\\n        print('accuracy =', grid.best_score_)\\n{'bandwidth': 7.0548023107186433}\\naccuracy = 0.966611018364\\nFigure 5-148. Validation curve for the KDE-based Bayesian classifier\\nWe see that this not-so-naive Bayesian classifier reaches a cross-validation accuracy of\\njust over 96%; this is compared to around 80% for the naive Bayesian classification:\\nIn[19]: from sklearn.naive_bayes import GaussianNB\\n        from sklearn.cross_validation import cross_val_score\\n        cross_val_score(GaussianNB(), digits.data, digits.target).mean()\\nOut[19]: 0.81860038035501381\\nOne benefit of such a generative classifier is interpretability of results: for each\\nunknown sample, we not only get a probabilistic classification, but a full model of the\\ndistribution of points we are comparing it to! If desired, this offers an intuitive win‐\\ndow into the reasons for a particular classification that algorithms like SVMs and ran‐\\ndom forests tend to obscure.\\nIf you would like to take this further, there are some improvements that could be\\nmade to our KDE classifier model:\\n• We could allow the bandwidth in each class to vary independently.\\n• We could optimize these bandwidths not based on their prediction score, but on\\nthe likelihood of the training data under the generative model within each class\\n(i.e., use the scores from KernelDensity itself rather than the global prediction\\naccuracy).\\nIn-Depth: Kernel Density Estimation \\n| \\n505\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 523}, page_content='Finally, if you want some practice building your own estimator, you might tackle\\nbuilding a similar Bayesian classifier using Gaussian mixture models instead of KDE.\\nApplication: A Face Detection Pipeline\\nThis chapter has explored a number of the central concepts and algorithms of\\nmachine learning. But moving from these concepts to real-world application can be a\\nchallenge. Real-world datasets are noisy and heterogeneous, may have missing fea‐\\ntures, and may include data in a form that is difficult to map to a clean [n_samples,\\nn_features] matrix. Before applying any of the methods discussed here, you must\\nfirst extract these features from your data; there is no formula for how to do this that\\napplies across all domains, and thus this is where you as a data scientist must exercise\\nyour own intuition and expertise.\\nOne interesting and compelling application of machine learning is to images, and we\\nhave already seen a few examples of this where pixel-level features are used for classi‐\\nfication. In the real world, data is rarely so uniform and simple pixels will not be suit‐\\nable, a fact that has led to a large literature on feature extraction methods for image\\ndata (see “Feature Engineering” on page 375).\\nIn this section, we will take a look at one such feature extraction technique, the Histo‐\\ngram of Oriented Gradients (HOG), which transforms image pixels into a vector rep‐\\nresentation that is sensitive to broadly informative image features regardless of\\nconfounding factors like illumination. We will use these features to develop a simple\\nface detection pipeline, using machine learning algorithms and concepts we’ve seen\\nthroughout this chapter. We begin with the standard imports:\\nIn[1]: %matplotlib inline\\n       import matplotlib.pyplot as plt\\n       import seaborn as sns; sns.set()\\n       import numpy as np\\nHOG Features\\nThe Histogram of Gradients is a straightforward feature extraction procedure that\\nwas developed in the context of identifying pedestrians within images. HOG involves\\nthe following steps:\\n1. Optionally prenormalize images. This leads to features that resist dependence on\\nvariations in illumination.\\n2. Convolve the image with two filters that are sensitive to horizontal and vertical\\nbrightness gradients. These capture edge, contour, and texture information.\\n3. Subdivide the image into cells of a predetermined size, and compute a histogram\\nof the gradient orientations within each cell.\\n506 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 524}, page_content=\"4. Normalize the histograms in each cell by comparing to the block of neighboring\\ncells. This further suppresses the effect of illumination across the image.\\n5. Construct a one-dimensional feature vector from the information in each cell.\\nA fast HOG extractor is built into the Scikit-Image project, and we can try it out rela‐\\ntively quickly and visualize the oriented gradients within each cell (Figure 5-149):\\nIn[2]: from skimage import data, color, feature\\n       import skimage.data\\n       image = color.rgb2gray(data.chelsea())\\n       hog_vec, hog_vis = feature.hog(image, visualise=True)\\n       fig, ax = plt.subplots(1, 2, figsize=(12, 6),\\n                              subplot_kw=dict(xticks=[], yticks=[]))\\n       ax[0].imshow(image, cmap='gray')\\n       ax[0].set_title('input image')\\n       ax[1].imshow(hog_vis)\\n       ax[1].set_title('visualization of HOG features');\\nFigure 5-149. Visualization of HOG features computed from an image\\nHOG in Action: A Simple Face Detector\\nUsing these HOG features, we can build up a simple facial detection algorithm with\\nany Scikit-Learn estimator; here we will use a linear support vector machine (refer\\nback to “In-Depth: Support Vector Machines” on page 405 if you need a refresher on\\nthis). The steps are as follows:\\n1. Obtain a set of image thumbnails of faces to constitute “positive” training\\nsamples.\\n2. Obtain a set of image thumbnails of nonfaces to constitute “negative” training\\nsamples.\\n3. Extract HOG features from these training samples.\\nApplication: A Face Detection Pipeline \\n| \\n507\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 525}, page_content=\"4. Train a linear SVM classifier on these samples.\\n5. For an “unknown” image, pass a sliding window across the image, using the\\nmodel to evaluate whether that window contains a face or not.\\n6. If detections overlap, combine them into a single window.\\nLet’s go through these steps and try it out:\\n1. Obtain a set of positive training samples.\\nLet’s start by finding some positive training samples that show a variety of faces.\\nWe have one easy set of data to work with—the Labeled Faces in the Wild dataset,\\nwhich can be downloaded by Scikit-Learn:\\nIn[3]: from sklearn.datasets import fetch_lfw_people\\n       faces = fetch_lfw_people()\\n       positive_patches = faces.images\\n       positive_patches.shape\\nOut[3]: (13233, 62, 47)\\nThis gives us a sample of 13,000 face images to use for training.\\n2. Obtain a set of negative training samples.\\nNext we need a set of similarly sized thumbnails that do not have a face in them.\\nOne way to do this is to take any corpus of input images, and extract thumbnails\\nfrom them at a variety of scales. Here we can use some of the images shipped\\nwith Scikit-Image, along with Scikit-Learn’s PatchExtractor:\\nIn[4]: from skimage import data, transform\\n       imgs_to_use = ['camera', 'text', 'coins', 'moon',\\n                      'page', 'clock', 'immunohistochemistry',\\n                      'chelsea', 'coffee', 'hubble_deep_field']\\n       images = [color.rgb2gray(getattr(data, name)())\\n                 for name in imgs_to_use]\\nIn[5]:\\nfrom sklearn.feature_extraction.image import PatchExtractor\\ndef extract_patches(img, N, scale=1.0,\\n                    patch_size=positive_patches[0].shape):\\n    extracted_patch_size = \\\\\\n    tuple((scale * np.array(patch_size)).astype(int))\\n    extractor = PatchExtractor(patch_size=extracted_patch_size,\\n                               max_patches=N, random_state=0)\\n    patches = extractor.transform(img[np.newaxis])\\n    if scale != 1:\\n        patches = np.array([transform.resize(patch, patch_size)\\n                            for patch in patches])\\n    return patches\\n508 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 526}, page_content=\"negative_patches = np.vstack([extract_patches(im, 1000, scale)\\n                       for im in images for scale in [0.5, 1.0, 2.0]])\\nnegative_patches.shape\\nOut[5]: (30000, 62, 47)\\nWe now have 30,000 suitable image patches that do not contain faces. Let’s take a\\nlook at a few of them to get an idea of what they look like (Figure 5-150):\\nIn[6]: fig, ax = plt.subplots(6, 10)\\n       for i, axi in enumerate(ax.flat):\\n           axi.imshow(negative_patches[500 * i], cmap='gray')\\n           axi.axis('off')\\nFigure 5-150. Negative image patches, which don’t include faces\\nOur hope is that these would sufficiently cover the space of “nonfaces” that our\\nalgorithm is likely to see.\\n3. Combine sets and extract HOG features.\\nNow that we have these positive samples and negative samples, we can combine\\nthem and compute HOG features. This step takes a little while, because the HOG\\nfeatures involve a nontrivial computation for each image:\\nIn[7]: from itertools import chain\\n       X_train = np.array([feature.hog(im)\\n                           for im in chain(positive_patches,\\n                                           negative_patches)])\\n       y_train = np.zeros(X_train.shape[0])\\n       y_train[:positive_patches.shape[0]] = 1\\nIn[8]: X_train.shape\\nOut[8]: (43233, 1215)\\nApplication: A Face Detection Pipeline \\n| \\n509\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 527}, page_content=\"We are left with 43,000 training samples in 1,215 dimensions, and we now have\\nour data in a form that we can feed into Scikit-Learn!\\n4. Train a support vector machine.\\nNext we use the tools we have been exploring in this chapter to create a classifier\\nof thumbnail patches. For such a high-dimensional binary classification task, a\\nlinear support vector machine is a good choice. We will use Scikit-Learn’s Line\\narSVC, because in comparison to SVC it often has better scaling for large number\\nof samples.\\nFirst, though, let’s use a simple Gaussian naive Bayes to get a quick baseline:\\nIn[9]: from sklearn.naive_bayes import GaussianNB\\n       from sklearn.cross_validation import cross_val_score\\n       cross_val_score(GaussianNB(), X_train, y_train)\\nOut[9]: array([ 0.9408785 ,  0.8752342 ,  0.93976823])\\nWe see that on our training data, even a simple naive Bayes algorithm gets us\\nupward of 90% accuracy. Let’s try the support vector machine, with a grid search\\nover a few choices of the C parameter:\\nIn[10]: from sklearn.svm import LinearSVC\\n        from sklearn.grid_search import GridSearchCV\\n        grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})\\n        grid.fit(X_train, y_train)\\n        grid.best_score_\\nOut[10]: 0.98667684407744083\\nIn[11]: grid.best_params_\\nOut[11]: {'C': 4.0}\\nLet’s take the best estimator and retrain it on the full dataset:\\nIn[12]: model = grid.best_estimator_\\n        model.fit(X_train, y_train)\\nOut[12]: LinearSVC(C=4.0, class_weight=None, dual=True,\\n              fit_intercept=True, intercept_scaling=1,\\n              loss='squared_hinge', max_iter=1000,\\n              multi_class='ovr', penalty='l2',\\n              random_state=None, tol=0.0001, verbose=0)\\n5. Find faces in a new image.\\nNow that we have this model in place, let’s grab a new image and see how the\\nmodel does. We will use one portion of the astronaut image for simplicity (see\\ndiscussion of this in “Caveats and Improvements” on page 512), and run a sliding\\nwindow over it and evaluate each patch (Figure 5-151):\\n510 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 528}, page_content=\"In[13]: test_image = skimage.data.astronaut()\\n        test_image = skimage.color.rgb2gray(test_image)\\n        test_image = skimage.transform.rescale(test_image, 0.5)\\n        test_image = test_image[:160, 40:180]\\n        plt.imshow(test_image, cmap='gray')\\n        plt.axis('off');\\nFigure 5-151. An image in which we will attempt to locate a face\\nNext, let’s create a window that iterates over patches of this image, and compute\\nHOG features for each patch:\\nIn[14]: def sliding_window(img, patch_size=positive_patches[0].shape,\\n                           istep=2, jstep=2, scale=1.0):\\n            Ni, Nj = (int(scale * s) for s in patch_size)\\n            for i in range(0, img.shape[0] - Ni, istep):\\n                for j in range(0, img.shape[1] - Ni, jstep):\\n                    patch = img[i:i + Ni, j:j + Nj]\\n                    if scale != 1:\\n                        patch = transform.resize(patch, patch_size)\\n                    yield (i, j), patch\\n        indices, patches = zip(*sliding_window(test_image))\\n        patches_hog = np.array([feature.hog(patch) for patch in patches])\\n        patches_hog.shape\\nOut[14]: (1911, 1215)\\nFinally, we can take these HOG-featured patches and use our model to evaluate\\nwhether each patch contains a face:\\nIn[15]: labels = model.predict(patches_hog)\\n        labels.sum()\\nApplication: A Face Detection Pipeline \\n| \\n511\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 529}, page_content=\"Out[15]: 33.0\\nWe see that out of nearly 2,000 patches, we have found 30 detections. Let’s use the\\ninformation we have about these patches to show where they lie on our test\\nimage, drawing them as rectangles (Figure 5-152):\\nIn[16]: fig, ax = plt.subplots()\\n        ax.imshow(test_image, cmap='gray')\\n        ax.axis('off')\\n        Ni, Nj = positive_patches[0].shape\\n        indices = np.array(indices)\\n        for i, j in indices[labels == 1]:\\n            ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',\\n                                       alpha=0.3, lw=2,\\n                                       facecolor='none'))\\nFigure 5-152. Windows that were determined to contain a face\\nAll of the detected patches overlap and found the face in the image! Not bad for a\\nfew lines of Python.\\nCaveats and Improvements\\nIf you dig a bit deeper into the preceding code and examples, you’ll see that we still\\nhave a bit of work before we can claim a production-ready face detector. There are\\nseveral issues with what we’ve done, and several improvements that could be made. In\\nparticular:\\n512 \\n| \\nChapter 5: Machine Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 530}, page_content='Our training set, especially for negative features, is not very complete\\nThe central issue is that there are many face-like textures that are not in the train‐\\ning set, and so our current model is very prone to false positives. You can see this\\nif you try out the preceding algorithm on the full astronaut image: the current\\nmodel leads to many false detections in other regions of the image.\\nWe might imagine addressing this by adding a wider variety of images to the neg‐\\native training set, and this would probably yield some improvement. Another\\nway to address this is to use a more directed approach, such as hard negative min‐\\ning. In hard negative mining, we take a new set of images that our classifier has\\nnot seen, find all the patches representing false positives, and explicitly add them\\nas negative instances in the training set before retraining the classifier.\\nOur current pipeline searches only at one scale\\nAs currently written, our algorithm will miss faces that are not approximately\\n62×47 pixels. We can straightforwardly address this by using sliding windows of\\na variety of sizes, and resizing each patch using skimage.transform.resize\\nbefore feeding it into the model. In fact, the sliding_window() utility used here\\nis already built with this in mind.\\nWe should combine overlapped detection patches\\nFor a production-ready pipeline, we would prefer not to have 30 detections of the\\nsame face, but to somehow reduce overlapping groups of detections down to a\\nsingle detection. This could be done via an unsupervised clustering approach\\n(MeanShift Clustering is one good candidate for this), or via a procedural\\napproach such as nonmaximum suppression, an algorithm common in machine\\nvision.\\nThe pipeline should be streamlined\\nOnce we address these issues, it would also be nice to create a more streamlined\\npipeline for ingesting training images and predicting sliding-window outputs.\\nThis is where Python as a data science tool really shines: with a bit of work, we\\ncould take our prototype code and package it with a well-designed object-\\noriented API that gives the user the ability to use this easily. I will leave this as a\\nproverbial “exercise for the reader.”\\nMore recent advances, such as deep learning, should be considered\\nFinally, I should add that HOG and other procedural feature extraction methods\\nfor images are no longer state-of-the-art techniques. Instead, many modern\\nobject detection pipelines use variants of deep neural networks. One way to think\\nof neural networks is that they are an estimator that determines optimal feature\\nextraction strategies from the data, rather than relying on the intuition of\\nthe user. An intro to these deep neural net methods is conceptually (and compu‐\\ntationally!) beyond the scope of this section, although open tools like Google’s\\nApplication: A Face Detection Pipeline \\n| \\n513'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 531}, page_content='TensorFlow have recently made deep learning approaches much more accessible\\nthan they once were. As of the writing of this book, deep learning in Python is\\nstill relatively young, and so I can’t yet point to any definitive resource. That said,\\nthe list of references in the following section should provide a useful place to\\nstart.\\nFurther Machine Learning Resources\\nThis chapter has been a quick tour of machine learning in Python, primarily using\\nthe tools within the Scikit-Learn library. As long as the chapter is, it is still too short\\nto cover many interesting and important algorithms, approaches, and discussions.\\nHere I want to suggest some resources for those who would like to learn more about\\nmachine learning.\\nMachine Learning in Python\\nTo learn more about machine learning in Python, I’d suggest some of the following\\nresources:\\nThe Scikit-Learn website\\nThe Scikit-Learn website has an impressive breadth of documentation and exam‐\\nples covering some of the models discussed here, and much, much more. If you\\nwant a brief survey of the most important and often used machine learning algo‐\\nrithms, this website is a good place to start.\\nSciPy, PyCon, and PyData tutorial videos\\nScikit-Learn and other machine learning topics are perennial favorites in the\\ntutorial tracks of many Python-focused conference series, in particular the\\nPyCon, SciPy, and PyData conferences. You can find the most recent ones via a\\nsimple web search.\\nIntroduction to Machine Learning with Python\\nWritten by Andreas C. Mueller and Sarah Guido, this book includes a fuller treat‐\\nment of the topics in this chapter. If you’re interested in reviewing the fundamen‐\\ntals of machine learning and pushing the Scikit-Learn toolkit to its limits, this is a\\ngreat resource, written by one of the most prolific developers on the Scikit-Learn\\nteam.\\nPython Machine Learning\\nSebastian Raschka’s book focuses less on Scikit-Learn itself, and more on the\\nbreadth of machine learning tools available in Python. In particular, there is\\nsome very useful discussion on how to scale Python-based machine learning\\napproaches to large and complex datasets.\\n514 \\n| \\nChapter 5: Machine Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 532}, page_content='General Machine Learning\\nOf course, machine learning is much broader than just the Python world. There are\\nmany good resources to take your knowledge further, and here I highlight a few that I\\nhave found useful:\\nMachine Learning\\nTaught by Andrew Ng (Coursera), this is a very clearly taught, free online course\\ncovering the basics of machine learning from an algorithmic perspective. It\\nassumes undergraduate-level understanding of mathematics and programming,\\nand steps through detailed considerations of some of the most important\\nmachine learning algorithms. Homework assignments, which are algorithmically\\ngraded, have you actually implement some of these models yourself.\\nPattern Recognition and Machine Learning\\nWritten by Christopher Bishop, this classic technical text covers the concepts of\\nmachine learning discussed in this chapter in detail. If you plan to go further in\\nthis subject, you should have this book on your shelf.\\nMachine Learning: A Probabilistic Perspective\\nWritten by Kevin Murphy, this is an excellent graduate-level text that explores\\nnearly all important machine learning algorithms from a ground-up, unified\\nprobabilistic perspective.\\nThese resources are more technical than the material presented in this book, but to\\nreally understand the fundamentals of these methods requires a deep dive into the\\nmathematics behind them. If you’re up for the challenge and ready to bring your data\\nscience to the next level, don’t hesitate to dive in!\\nFurther Machine Learning Resources \\n| \\n515'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 533}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 534}, page_content='Index\\nSymbols\\n%automagic, 19\\n%cpaste, 11\\n%debug, 22\\n%history, 16\\n%lprun, 28\\n%lsmagic, 13\\n%magic, 13\\n%matplotlib, 219\\n%memit, 29\\n%mode, 20-22\\n%mprun, 29\\n%paste, 11\\n%prun, 27\\n%run, 12\\n%time, 25-27\\n%timeit, 12, 25-27\\n& (ampersand), 77\\n* (asterisk), 7\\n: (colon), 44\\n? (question mark), 3\\n?? (double question mark), 5\\n_ (underscore) shortcut, 15\\n| (operator), 77\\nA\\nabsolute value function, 54\\naggregate() method, 166\\naggregates\\ncomputed directly from object, 57\\nmultidimensional, 60\\nsummarizing set of values with, 61\\naggregation (NumPy), 58-63\\nminimum and maximum, 59\\nmultidimensional aggregates, 60\\npresidents average height example, 61\\nsumming the values in an array, 59\\nvarious functions, 61\\naggregation (Pandas), 158-170\\ngroupby() operation, 161-170\\nMultiIndex, 140\\nPlanets dataset for, 159\\nsimple aggregation, 159-161\\nAkaike information criterion (AIC), 487, 489\\nAlbers equal-area projection, 303\\nalgorithmic efficiency\\nbig-O notation, 92\\ndataset size and, 85\\nampersand (&), 77\\nAnaconda, xiv\\nand keyword, 77\\nannotation of plots, 268-275\\narrows, 272-275\\nholidays/US births example, 269\\ntransforms and text position, 270-272\\nAPIs (see Estimator API)\\nappend() method, Pandas vs. Python, 146\\napply() method, 167\\narithmetic operators, 52\\narrays\\naccessing single rows/columns, 45\\narithmetic operators, 52\\nattributes, 42\\nbasics, 42\\nBoolean, 73-75\\nbroadcasting, 63-69\\ncentering, 68\\ncomputation on, 50-58\\n517'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 535}, page_content='concatenation, 48, 142\\ncreating copies, 46\\ncreating from Python lists, 39\\ncreating from scratch, 39\\ndata as, 33\\nDataFrame object as, 102\\nDataFrame object constructed from, 105\\nfixed-type, 38\\nIndex object as immutable array, 106\\nIndex object vs., 106\\nindexing: accessing single elements, 43\\nreshaping, 47\\nSeries object vs., 99\\nslicing, 44\\nslicing multidimensional subarrays, 45\\nslicing one-dimensional subarrays, 44\\nsorting, 85-96\\nspecifying output to, 56\\nsplitting, 49\\nstandard data types, 41\\nstructured, 92-96\\nsubarrays as no-copy views, 46\\nsumming values in, 59\\nuniversal functions, 50-58\\narrows, 272-275\\nasfreq() method, 197-199\\nasterisk (*), 7\\nautomagic function, 19\\naxes limits, 228-230\\nB\\nbagging, 426\\nbandwidth (see kernel bandwidth)\\nbar (|) operator, 77\\nbar plots, 321\\nBasemap toolkit\\ngeographic data with, 298\\n(see also geographic data)\\ninstallation, 298\\nbasis function regression, 378, 392-396\\nGaussian basis functions, 394-396\\npolynomial basis functions, 393\\nBayesian classification, 383, 501-506\\n(see also naive Bayes classification)\\nBayesian information criterion (BIC), 487\\nBayesian Methods for Hackers stylesheet, 288\\nBayess theorem, 383\\nbias–variance trade-off\\nkernel bandwidth and, 497\\nmodel selection and, 364-366\\nbicycle traffic prediction\\nlinear regression, 400\\ntime series, 202-209\\nbig-O notation, 92\\nbinary ufuncs, 52\\nbinnings, 248\\nbitwise logic operators, 74\\nbogosort, 86\\nBokeh, 330\\nBoolean arrays\\nBoolean operators and, 74\\ncounting entries in, 73\\nworking with, 73-75\\nBoolean masks, 70-78\\nBoolean arrays as, 75-78\\nrainfall statistics, 70\\nworking with Boolean arrays, 73-75\\nBoolean operators, 74\\nbroadcasting, 63-69\\nadding two-dimensional array to one-\\ndimensional array, 66\\nbasics, 63-65\\ncentering an array, 68\\ndefined, 58, 63\\nin practice, 68\\nplotting two-dimensional function, 69\\nrules, 65-68\\ntwo compatible arrays, 66\\ntwo incompatible arrays, 67\\nC\\ncategorical data, 376\\nclass labels (for data point), 334\\nclassification task\\ndefined, 332\\nmachine learning, 333-335\\nclustering, 332\\nbasics, 338-339\\nGMMs, 353, 476-491\\nk-means, 339, 462-476\\ncode\\nmagic commands for determining execu‐\\ntion time, 12\\nmagic commands for pasting blocks, 11\\nmagic commands for running external, 12\\nprofiling and timing, 25-30\\ntiming of snippets, 25-27\\ncoefficient of determination, 365\\n518 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 536}, page_content='colon (:), 44\\ncolor compression, 473-476\\ncolorbars\\ncolormap selection, 256-259\\ncustomizing, 255-262\\ndiscrete, 260\\nhandwritten digit example, 261-262\\ncolormap, 256-259\\ncolumn(s)\\naccessing single, 45\\nindexing, 163\\nMultiIndex for, 133\\nsorting arrays along, 87\\nsuffixes keyword and overlapping names,\\n153\\ncolumn-wise operations, 211-213\\ncommand history shortcuts, 9\\ncomparison operators, 71-73\\nconcatenation\\ndatasets, 141-146\\nof arrays, 48, 142\\nwith pd.concat(), 142-146\\nconfusion matrix, 357\\nconic projections, 303\\ncontour plots, 241-245\\ndensity and, 241-245\\nthree-dimensional function, 241-245\\nthree-dimensional plot, 292\\nConway, Drew, xi\\ncross-validation, 361-370\\ncubehelix colormap, 258\\ncylindrical projections, 301\\nD\\ndata\\nas arrays, 33\\nmissing (see missing data)\\ndata representation (Scikit-Learn package),\\n343-346\\ndata as table, 343\\nfeatures matrix, 344\\ntarget array, 344-345\\ndata science, defining, xi\\ndata types, 34\\nfixed-type arrays, 38\\nintegers, 35\\nlists in, 37-41\\nNumPy, 41\\nDataFrame object (Pandas), 102-105\\nas dictionary, 110-112\\nas generalized NumPy array, 102\\nas specialized dictionary, 103\\nas two-dimensional array, 112-114\\nconstructing, 104\\ndata selection in, 110\\ndefined, 97\\nindex alignment in, 117\\nmasking, 114\\nmultiply indexed, 136\\noperations between Series object and, 118\\nslicing, 114\\nDataFrame.eval() method, 211-213\\nassignment in, 212\\nlocal variables in, 213\\nDataFrame.query() method, 213\\ndatasets\\nappending, 146\\ncombining (Panda), 141-158\\nconcatenation, 141-146\\nmerging/joining, 146-158\\ndatetime module, 189\\ndatetime64 dtype, 189\\ndateutil module, 189\\ndebugging, 22-24\\ndecision trees, 421-426\\n(see also random forests)\\ncreating, 422-425\\noverfitting, 425\\ndeep learning, 513\\ndensity estimator\\nGMM, 484-488\\nhistogram as, 492\\nKDE (see kernel density estimation (KDE))\\ndescribe() method, 164\\ndevelopment, IPython\\nprofiling and timing code, 25-30\\nprofiling full scripts, 27\\ntiming of code snippets, 25-27\\ndictionary(-ies)\\nDataFrame as specialization of, 103\\nDataFrame object constructed from list of,\\n104\\nPandas Series object vs., 100\\ndigits, recognition of (see optical character rec‐\\nognition)\\ndimensionality reduction, 261\\nmachine learning, 340-342\\nPCA and, 433\\nIndex \\n| \\n519'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 537}, page_content='discriminative classification, 405-407\\ndocumentation, accessing\\nIPython, 3-8, 98\\nPandas, 98\\ndouble question mark (??), 5\\ndropna() method, 125\\ndynamic typing, 34\\nE\\neigenfaces, 442-445\\nensemble estimator/method, 421\\n(see also random forests)\\nensemble learner, 421\\nequidistant cylindrical projection, 301\\nerrors, visualizing\\nbasic errorbars, 238\\ncontinuous quantities, 239\\nMatplotlib, 237-240\\nEstimator API, 346-359\\nbasics, 347\\nIris classification example, 351\\nIris clustering example, 353\\nIris dimensionality example, 352\\nsimple linear regression example, 347-354\\neval() function, 210-211\\nDataFrame.eval() method and, 211-213\\npd.eval() function and, 210-211\\nwhen to use, 214\\nexceptions, controlling, 20-22\\nexpectation-maximization (E-M) algorithm\\ncaveats, 467-470\\nGMM as generalization of, 480-484\\nk-means clustering and, 465-476\\nexponentials, 55\\nexternal code, magic commands for running,\\n12\\nF\\nface recognition\\nHOG, 506-514\\nIsomap, 456-460\\nPCA, 442-445\\nSVMs, 416-420\\nfaceted histograms, 318\\nfactor plots, 319\\nfancy indexing, 78-85\\nbasics, 79\\nbinning data, 83\\ncombined with other indexing schemes, 80\\nmodifying values with, 82\\nselection of random points, 81\\nfeature engineering, 375-382\\ncategorical features, 376\\nderived features, 378-380\\nimage features, 378\\nimputation of missing data, 381\\nprocessing pipeline, 381\\ntext features, 377\\nfeature, data point, 334\\nfeatures matrix, 344\\nfillna() method, 126\\nfilter() method, 166\\nFiveThirtyEight stylesheet, 287\\nfixed-type arrays, 38\\nG\\nGaussian basis functions, 394-396\\nGaussian mixture models (GMMs), 476-491\\nchoosing covariance type, 484\\nclustering with, 353\\ndensity estimation algorithm, 484-488\\nE–M generalization, 480-484\\nhandwritten data generation example,\\n488-491\\nk-means weaknesses addressed by, 477-480\\nKDE and, 491\\nGaussian naive Bayes classification, 351, 357,\\n383-386, 510\\nGaussian process regression (GPR), 239\\ngenerative models, 383\\ngeographic data, 298\\nBasemap toolkit for, 298\\nCalifornia city population example, 308\\ndrawing a map background, 304-307\\nmap projections, 300-304\\nplotting data on maps, 307\\nsurface temperature data example, 309\\nget() operation, 183\\nget_dummies() method, 183\\nggplot stylesheet, 287\\ngraphics libraries, 330\\nGroupBy aggregation, 170\\nGroupBy object, 163-165\\naggregate() method, 166\\napply() method, 167\\ncolumn indexing, 163\\ndispatch methods, 164\\nfilter() method, 166\\n520 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 538}, page_content='iteration over groups, 164\\ntransform() method, 167\\ngroupby() operation (Pandas), 161-170\\nGroupBy object and, 163-165\\ngrouping example, 169\\npivot tables vs., 171\\nsplit key specification, 168\\nsplit-apply-combine example, 161-163\\nH\\nhandwritten digits, recognition of (see optical\\ncharacter recognition)\\nhard negative mining, 513\\nhelp\\nIPython, 3-8\\nmagic functions, 13\\nhelp() function, 4\\nhexagonal binnings, 248\\nhierarchical indexing\\nin one-dimensional Series, 128-141\\nMultiIndex, 128-141, 129-131\\n(see also MultiIndex type)\\nrearranging multi-indices, 137-140\\nunstack() method, 130\\nwith Python tuples as keys, 128\\nHistogram of Oriented Gradients (HOG)\\ncaveats and improvements, 512-514\\nfeatures, 506\\nfor face detection pipeline, 506-514\\nsimple face detector, 507-512\\nhistograms, 245-249\\nbinning data to create, 83\\nfaceted, 318\\nKDE and, 248, 491-496\\nmanual customization, 282-284\\nplt.hexbin() function, 248\\nplt.hist2d() function, 247\\nSeaborn, 314-317\\nsimple, 245-246\\ntwo-dimensional, 247-249\\nholdout sets, 360\\nHunter, John, 217\\nhyperparameters, 349\\n(see also model validation)\\nI\\niloc attribute (Pandas), 110\\nimages, encoding for machine learning analy‐\\nsis, 378\\nimmutable array, Index object as, 106\\nimporting, tab completion for, 7\\nIn objects, IPython, 13\\nindex alignment\\nin DataFrame, 117\\nin Series, 116\\nIndex object (Pandas), 105-107\\nas immutable array, 106\\nas ordered set, 106\\nindexing\\nfancy, 78-85\\n(see also fancy indexing)\\nhierarchical (see hierarchical indexing)\\nNumPy arrays: accessing single elements, 43\\nPandas, 107\\nIndexSlice object, 137\\nindicator variables, 183\\ninner join, 153\\ninput/output history, IPython, 13-16\\nIn and Out objects, 13\\nrelated magic commands, 16\\nsuppressing output, 15\\nunderscore shortcuts and previous outputs,\\n15\\ninstallation, Python, xiv\\nintegers, Python, 35\\nIPython, 1\\naccessing documentation with ?, 3\\naccessing source code with ??, 5\\ncommand-line commands in shell, 18\\ncontrolling exceptions, 20-22\\ndebugging, 22-24\\ndocumentation, 3-8, 34\\nerrors handling, 20-24\\nexploring modules with tab completion, 6-7\\nhelp and documentation, 3-8\\ninput/output history, 13-16\\nkeyboard shortcuts in shell, 8\\nlaunching Jupyter notebook, 2\\nlaunching shell, 2\\nmagic commands, 10-13\\nnotebook (see Jupyter notebook)\\nplotting from shell, 219\\nprofiling and timing code, 25-30\\nshell commands, 16-19\\nshell-related magic commands, 19\\nweb resources, 30\\nwildcard matching, 7\\nIris dataset\\nIndex \\n| \\n521'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 539}, page_content='as table, 343\\nclassification, 351\\nclustering, 353\\ndimensionality, 352\\npair plots, 317\\nscatter plots, 236\\nvisualization of, 345\\nisnull() method, 124\\nIsomap\\ndimensionality reduction, 341, 355\\nface data, 456-460\\nix attribute (Pandas), 110\\nJ\\njet colormap, 257\\njoins, 145\\n(see also merging)\\ncategories of, 147-149\\ndatasets, 146-158\\nmany-to-one, 148\\none-to-one, 147\\nset arithmetic for, 152\\njoint distributions, 316, 320\\nJupyter notebook\\nlaunching, 2\\nplotting from, 220\\nK\\nk-means clustering, 339, 462-476\\nbasics, 463-465\\ncolor compression example, 473-476\\nexpectation-maximization algorithm,\\n465-476\\nGMM as means of addressing weaknesses\\nof, 477-480\\nsimple digits data application, 470-473\\nkernel (defined), 496\\nkernel bandwidth\\ndefined, 496\\nselection via cross-validation, 497\\nkernel density estimation (KDE), 491-506\\nbandwidth selection via cross-validation,\\n497\\nBayesian generative classification with,\\n501-506\\ncustom estimator, 501-506\\nhistograms and, 491-496\\nin practice, 496-506\\nMatplotlib, 248\\nSeaborn, 314\\nvisualization of geographic distributions,\\n498-501\\nkernel SVM, 411-414\\nkernel transformation, 413\\nkernel trick, 413\\nkeyboard shortcuts, IPython shell, 8\\ncommand history, 9\\nnavigation, 8\\ntext entry, 9\\nKnuth, Donald, 25\\nL\\nlabels/labeling\\nclassification task, 333-335\\nclustering, 338-339\\ndimensionality reduction and, 340-342\\nregression task, 335-338\\nsimple line plots, 230-232\\nLambert conformal conic projection, 303\\nlasso regularization (L1 regularization), 399\\nlearning curves, computing, 372\\nleft join, 153\\nleft_index keyword, 151-152\\nlegends, plot\\nchoosing elements for, 251\\ncustomizing, 249-255\\nmultiple legends on same axes, 254\\npoint size, 252\\nlevels, naming, 133\\nline plots\\naxes limits for, 228-230\\nlabeling, 230-232\\nline colors and styles, 226-228\\nMatplotlib, 224-232\\nline-by-line profiling, 28\\nlinear regression (in machine learning), 390\\nbasis function regression, 392-396\\nregularization, 396-400\\nSeattle bicycle traffic prediction example,\\n400\\nsimple, 390-392\\nlists, Python, 37-41\\nloc attribute (Pandas), 110\\nlocally linear embedding (LLE), 453-455\\nlogarithms, 55\\nM\\nmachine learning, 331\\n522 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 540}, page_content='basics, 331-342\\ncategories of, 332\\nclassification task, 333-335\\nclustering, 338-339\\ndecision trees and random forests, 421\\ndefined, 332\\ndimensionality reduction, 340-342\\neducational resources, 514\\nface detection pipeline, 506-514\\nfeature engineering, 375-382\\nGMM (see Gaussian mixture models)\\nhyperparameters and model validation,\\n359-375\\nKDE (see kernel density estimation)\\nlinear regression (see linear regression)\\nmanifold learning (see manifold learning)\\nnaive Bayes classification, 382-390\\nPCA (see principal component analysis)\\nqualitative examples, 333-342\\nregression task, 335-338\\nScikit-Learn basics, 343\\nsupervised, 332\\nSVMs (see support vector machines)\\nunsupervised, 332\\nmagic commands\\ncode block pasting, 11\\ncode execution timing, 12\\nhelp commands, 13\\nIPython input/output history, 16\\nrunning external code, 12\\nshell-related, 19\\nmanifold learning, 445-462\\n\"HELLO\" function, 446\\nadvantages/disadvantages, 455\\napplying Isomap on faces data, 456-460\\ndefined, 446\\nk-means clustering (see k-means clustering)\\nmultidimensional scaling, 450-452\\nPCA vs., 455\\nvisualizing structure in digits, 460-462\\nmany-to-one joins, 148\\nmap projections, 300-304\\nconic, 303\\ncylindrical, 301\\nperspective, 302\\npseudo-cylindrical, 302\\nmaps, geographic (see geographic data)\\nmargins, maximizing, 407-416\\nmasking, 114\\n(see also Boolean masks)\\nBoolean arrays, 75-78\\nBoolean masks, 70-78\\nMATLAB-style interface, 222\\nMatplotlib, 217, 329\\naxes limits for line plots, 228-230\\nchanging defaults via rcParams, 284\\ncolorbar customization, 255-262\\nconfigurations and stylesheets, 282-290\\ndensity and contour plots, 241-245\\nerror visualization, 237-240\\ngeneral tips, 218-222\\ngeographic data with Basemap toolkit, 298\\ngotchas, 232\\nhistograms, binnings, and density, 245-249\\nimporting, 218\\ninterfaces, 222\\nlabeling simple line plots, 230-232\\nline colors and styles, 226-228\\nMATLAB-style interfaces, 222\\nmultiple subplots, 262-268\\nobject hierarchy of plots, 275\\nobject-oriented interfaces, 223\\nplot customization, 282-284\\nplot display contexts, 218-220\\nplot legend customization, 249-255\\nplotting from a script, 219\\nplotting from IPython notebook, 220\\nplotting from IPython shell, 219\\nresources and documentation for, 329\\nsaving figures to file, 221\\nSeaborn vs., 311-313\\nsetting styles, 218\\nsimple line plots, 224-232\\nstylesheets, 285-290\\ntext and annotation, 268-275\\nthree-dimensional function visualization,\\n241-245\\nthree-dimensional plotting, 290-298\\ntick customization, 275-282\\nmax() function, 59\\nmaximum margin estimator, 408\\n(see also support vector machines (SVMs))\\nmemory use, profiling, 29\\nmerge key\\non keyword, 149\\nspecification of, 149-152\\nmerging, 146-158\\n(see also joins)\\nIndex \\n| \\n523'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 541}, page_content='key specification, 149-152\\nrelational algebra and, 146\\nUS state population data example, 154-158\\nmin() function, 59\\nMiniconda, xiv\\nmissing data, 120-124\\nfeature engineering and, 381\\nhandling, 119-120\\nNaN and None, 123\\noperating on null values in Pandas, 124-127\\nMöbius strip, 296-298\\nmodel (defined), 334\\nmodel parameters (defined), 334\\nmodel selection\\nbias–variance trade-off, 364-366\\nvalidation curves in Scikit-Learn, 366-370\\nmodel validation, 359-375\\nbias–variance trade-off, 364-366\\ncross-validation, 361-370\\ngrid search example, 373\\nholdout sets, 360\\nlearning curves, 370-373\\nnaive approach to, 359\\nvalidation curves, 366-370\\nmodules, IPython, 6-7\\nMollweide projection, 302\\nmulti-indexing (see hierarchical indexing)\\nmultidimensional scaling (MDS), 450-452\\nbasics, 447-450\\nlocally linear embedding and, 453-455\\nnonlinear embeddings, 452\\nMultiIndex type, 129-131\\ncreation methods, 131-134\\ndata aggregations on, 140\\nexplicit constructors for, 132\\nextra dimension of data with, 130\\nfor columns, 133\\nindex setting/resetting, 139\\nindexing and slicing, 134-137\\nkeys option, 144\\nlevel names, 133\\nmultiply indexed DataFrames, 136\\nmultiply indexed Series, 134\\nrearranging, 137-140\\nsorted/unsorted indices with, 137\\nstacking/unstacking indices, 138\\nmultinomial naive Bayes classification, 386-389\\nN\\nnaive Bayes classification, 382-390\\nadvantages/disadvantages, 389\\nBayesian classification and, 383\\nGaussian, 383-386\\nmultinomial, 386-389\\ntext classification example, 386-389\\nNaN value, 104, 116, 122\\nnavigation shortcuts, 8\\nneural networks, 513\\nnoise filter, PCA as, 440-442\\nNone object, 121, 123\\nnonlinear embeddings, MDS and, 452\\nnotnull() method, 124\\nnp.argsort() function, 86\\nnp.concatenate() function, 48, 143\\nnp.sort() function, 86\\nnull values, 124-127\\ndetecting, 124\\ndropping, 125\\nfilling, 126\\nNumPy, 33\\naggregations, 58-63\\narray attributes, 42\\narray basics, 42\\narray indexing: accessing single elements, 43\\narray slicing: accessing subarrays, 44\\nBoolean masks, 70-78\\nbroadcasting, 63-69\\ncomparison operators as ufuncs, 71-73\\ncomputation on arrays, 50-58\\ndata types in Python, 34\\ndatetime64 dtype, 189\\ndocumentation, 34\\nfancy indexing, 78-85\\nkeywords and/or vs. operators &/|, 77\\nsorting arrays, 85-92\\nstandard data types, 41\\nstructured arrays, 92-96\\nuniversal functions, 50-58\\nO\\nobject-oriented interface, 223\\noffsets, time series, 196\\non keyword, 149\\none-hot encoding, 376\\none-to-one joins, 147\\noptical character recognition\\ndigit classification, 357-358\\n524 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 542}, page_content='GMMs, 488-491\\nk-means clustering, 470-473\\nloading/visualizing digits data, 354\\nMatplotlib, 261-262\\nPCA as noise filtering, 440-442\\nPCA for visualization, 437\\nrandom forests for classifying digits,\\n430-432\\nScikit-Learn application, 354-358\\nvisualizing structure in digits, 460-462\\nor keyword, 77\\nordered set, Index object as, 106\\northographic projection, 302\\nOut objects, IPython, 13\\nouter join, 153\\nouter products, 58\\noutliers, PCA and, 445\\noutput, suppressing, 15\\noverfitting, 371, 425\\nP\\npair plots, 317\\nPandas, 97\\naggregation and grouping, 158-170\\nand compound expressions, 209\\nappending datasets, 146\\nbuilt-in documentation, 98\\ncombining datasets, 141-158\\nconcatenation of datasets, 141-146\\ndata indexing and selection, 107\\ndata selection in DataFrame, 110-215\\ndata selection in Series, 107-110\\nDataFrame object, 102-105\\neval() and query(), 208-209\\nhandling missing data, 119-120\\nhierarchical indexing, 128-141\\nIndex object, 105-107\\ninstallation, 97\\nmerging/joining datasets, 146-158\\nNaN and None in, 123\\nnull values, 124-127\\nobjects, 98-107\\noperating on data in, 115-127\\n(see also universal functions)\\npandas.eval(), 210-211\\nPanel data, 141\\npivot tables, 170-178\\nSeries object, 99-102\\ntime series, 188-214\\nvectorized string operations, 178-188\\npandas.eval() function, 210-211\\nPanel data, 141\\npartial slicing, 135\\npartitioning (partial sorts), 88\\npasting code blocks, magic commands for, 11\\npd.concat() function\\ncatching repeats as error, 144\\nconcatenation with, 142-146\\nconcatenation with joins, 145\\nduplicate indices, 143\\nignoring the index, 144\\nMultiIndex keys, 144\\npd.date_range() function, 193\\npd.eval() function, 210-211\\npd.merge() function, 146-158\\ncategories of joins, 147-149\\nkeywords, 149-152\\nleft_index/right_index keywords, 151-152\\nmerge key specification, 149-152\\nrelational algebra and, 146\\nspecifying set arithmetic for joins, 152\\npdb (Python debugger), 22\\nPerez, Fernando, 1, 217\\nPeriod type, 193\\nperspective projections, 302\\npipelines, 366, 381\\npivot tables, 170-178\\ngroupby() operation vs., 171\\nmulti-level, 172\\nsyntax, 171-173\\nTitanic passengers example, 170\\nUS birthrate data example, 174-178\\nPlanets dataset\\naggregation and grouping, 159\\nbar plots, 321\\nplot legends\\nchoosing elements for, 251\\ncustomizing, 249-255\\nmultiple legends on same axes, 254\\npoints size, 252\\nPlotly, 330\\nplotting\\naxes limits for simple line plots, 228-230\\nbar plots, 321\\nchanging defaults via rcParams, 284\\ncolorbars, 255-262\\ndata on maps, 307-329\\ndensity and contour plots, 241-245\\nIndex \\n| \\n525'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 543}, page_content='display contexts, 218-220\\nfactor plots, 319\\nfrom an IPython shell, 219\\nfrom script, 219\\nhistograms, binnings, and density, 245-249\\nIPython notebook, 220\\njoint distributions, 320\\nlabeling simple line plots, 230-232\\nline colors and styles, 226-228\\nmanual customization, 282-284\\nMatplotlib, 217\\nmultiple subplots, 262-268\\nof errors, 237-240\\npair plots, 317\\nplot legends, 249-255\\nSeaborn, 311-313\\nsimple line plots, 224-232\\nsimple scatter plots, 233-237\\nstylesheets for, 285-290\\ntext and annotation for, 268-275\\nthree-dimensional, 290-298\\nthree-dimensional function, 241-245\\nticks, 275-282\\ntwo-dimensional function, 69\\nvarious Python graphics libraries, 330\\nplt.axes() function, 263-264\\nplt.contour() function, 241-244\\nplt.GridSpec() function, 266-268\\nplt.imshow() function, 243-244\\nplt.legend() command, 249-254\\nplt.plot() function\\ncolor arguments, 226\\nplt.scatter vs., 237\\nscatter plots with, 233-235\\nplt.scatter() function\\nplt.plot vs., 237\\nsimple scatter plots with, 235-237\\nplt.subplot() function, 264\\nplt.subplots() function, 265\\npolynomial basis functions, 393\\npolynomial regression model, 366\\npop() method, 111\\npopulation data, US, merge and join operations\\nwith, 154-158\\nprincipal axes, 434-436\\nprincipal component analysis (PCA), 433-515\\nbasics, 433-442\\nchoosing number of components, 440\\neigenfaces example, 442-445\\nfacial recognition example, 442-445\\nfor dimensionality reduction, 436\\nhandwritten digit example, 437-440,\\n440-442\\nmanifold learning vs., 455\\nmeaning of components, 438-439\\nnoise filtering, 440-442\\nstrengths/weaknesses, 445\\nvisualization with, 437\\nprofiling\\nfull scripts, 27\\nline-by-line, 28\\nmemory use, 29\\nprojections (see map projections)\\npseudo-cylindrical projections, 302\\nPython\\ninstallation considerations, xiv\\nPython 2.x vs. Python 3, xiii\\nreasons for using, xii\\nQ\\nquery() method\\nDataFrame.query() method, 213\\nwhen to use, 214\\nquestion mark (?), accessing IPython documen‐\\ntation with, 3\\nquicksort algorithm, 87\\nR\\nradial basis function, 412\\nrainfall statistics, 70\\nrandom forests\\nadvantages/disadvantages, 432\\nclassifying digits with, 430-432\\ndefined, 426\\nensembles of estimators, 426-428\\nmotivating with decision trees, 421-426\\nregression, 428\\nRandomizedPCA, 442\\nrcParams dictionary, changing defaults via, 284\\nRdBu colormap, 258\\nrecord arrays, 96\\nreduce() method, 57\\nregression, 428-433\\n(see also specific forms, e.g.: linear regres‐\\nsion)\\nregression task\\ndefined, 332\\nmachine learning, 335-338\\n526 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 544}, page_content='regular expressions, 181\\nregularization, 396-400\\nlasso regularization, 399\\nridge regression, 398\\nrelational algebra, 146\\nresample() method, 197-199\\nreset_index() method, 139\\nreshaping, 47\\nridge regression (L2 regularization), 398\\nright join, 153\\nright_index keyword, 151-152\\nrolling statistics, 201\\nruntime configuration (rc), 284\\nS\\nscatter plots (see simple scatter plots)\\nScikit-Learn package, 331, 343-346\\nAPI (see Estimator API)\\nbasics, 343-359\\ndata as table, 343\\ndata representation in, 343-346\\nEstimator API, 346-354\\nfeatures matrix, 344\\nhandwritten digit application, 354-358\\nsupport vector classifier, 408-411\\ntarget array, 344-345\\nscipy.special submodule, 56\\nscript\\nplotting from, 219\\nprofiling, 27\\nSeaborn\\nbar plots, 321\\ndatasets and plot types, 313-329\\nfaceted histograms, 318\\nfactor plots, 319\\nhistograms, KDE, and densities, 314-317\\njoint distributions, 320\\nmarathon finishing times example, 322-329\\nMatplotlib vs., 311-313\\npair plots, 317\\nstylesheet, 289\\nvisualization with, 311-313\\nSeattle, bicycle traffic prediction in\\nlinear regression, 400-405\\ntime series, 202-209\\nSeattle, rainfall statistics in, 70\\nsemi-supervised learning, 333\\nSeries object (Pandas), 99-102\\nas dictionary, 100, 107\\nconstructing, 101\\ndata indexing/selection in, 107-110\\nDataFrame as dictionary of, 110-112\\nDataFrame object constructed from, 104\\nDataFrame object constructed from dictio‐\\nnary of, 105\\ngeneralized NumPy array, 99\\nhierarchical indexing in, 128-141\\nindex alignment in, 116\\nindexer attributes, 109\\nmultiply indexed, 134\\none-dimensional array, 108\\noperations between DataFrame and, 118\\nshell, IPython\\nbasics, 16\\ncommand-line commands, 18\\ncommands, 16-19\\nkeyboard shortcuts in, 8\\nlaunching, 2\\nmagic commands, 19\\npassing values to and from, 18\\nshift() function, 199-201\\nshortcuts\\naccessing previous output, 15\\ncommand history, 9\\nIPython shell, 8-31\\nnavigation, 8\\ntext entry, 9\\nsimple histograms, 245-246\\nsimple line plots\\naxes limits for, 228-230\\nlabeling, 230-232\\nline colors and styles, 226-228\\nMatplotlib, 224-232\\nsimple (Matplotlib), 224-232\\nsimple linear regression, 390-392\\nsimple scatter plots\\nCalifornia city populations, 249-254\\nMatplotlib, 233-237\\nplt.plot, 233-235\\nplt.plot vs. plt.scatter, 237\\nplt.scatter, 235-237\\nslice() operation, 183\\nslicing\\nMultiIndex with sorted/unsorted indices,\\n137\\nNumPy arrays, 44-47\\nNumPy arrays: accessing subarrays, 44\\nIndex \\n| \\n527'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 545}, page_content='NumPy arrays: multidimensional subarrays,\\n45\\nNumPy arrays: one-dimensional subarrays,\\n44\\nNumPy vs. Python, 46\\nPandas conventions, 114\\nsorting arrays, 85-92\\nalong rows or columns, 87\\nbasics, 85\\nfast sorting with np.sort and np.argsort, 86\\nk-nearest neighbors example, 88-92\\npartitioning, 88\\nsource code, accessing, 5\\nsplitting arrays, 49\\nstring operations (see vectorized string opera‐\\ntions)\\nstructured arrays, 92-96\\nadvanced compound types, 95\\ncreating, 94\\nrecord arrays, 96\\nstylesheets\\nBayesian Methods for Hackers, 288\\ndefault style, 286\\nFiveThirtyEight style, 287\\nggplot, 287\\nMatplotlib, 285-290\\nSeaborn, 289\\nsubarrays\\nas no-copy views, 46\\ncreating copies, 46\\nslicing multidimensional, 45\\nslicing one-dimensional, 44\\nsubplots\\nmanual customization, 263-264\\nmultiple, 262-268\\nplt.axes() for, 263-264\\nplt.GridSpec() for, 266-268\\nplt.subplot() for, 264\\nplt.subplots() for, 265\\nsubsets, faceted histograms, 318\\nsuffixes keyword, 153\\nsupervised learning, 332\\nclassification task, 333-335\\nregression task, 335-338\\nsupport vector (defined), 409\\nsupport vector classifier, 408-411\\nsupport vector machines (SVMs), 405\\nadvantages/disadvantages, 420\\nface recognition example, 416-420\\nfitting, 408-411\\nkernels and, 411-414\\nmaximizing the margin, 407-416\\nmotivating, 405-420\\nsimple face detector, 507\\nsoftening margins, 414-416\\nsurface plots, three-dimensional, 293-298\\nT\\nt-distributed stochastic neighbor embedding (t-\\nSNE), 456, 472\\ntab completion\\nexploring IPython modules with, 6-7\\nof object contents, 6\\nwhen importing, 7\\ntable, data as, 343\\ntarget array, 344-345\\nterm frequency-inverse document frequency\\n(TF-IDF), 378\\ntext, 377\\n(see also annotation of plots)\\ntransforms and position of, 270-272\\ntext entry shortcuts, 9\\nthree-dimensional plotting\\ncontour plots, 292\\nMöbius strip visualization, 296-298\\npoints and lines, 291\\nsurface plots, 293-298\\nsurface triangulations, 295-298\\nwireframes, 293\\nwith Matplotlib, 290-298\\nticks (tick marks)\\ncustomizing, 275-282\\nfancy formats, 279-281\\nformatter/locator options, 281\\nmajor and minor, 276\\nreducing/increasing number of, 278\\nTikhonov regularization, 398\\ntime series\\nbar plots, 321\\ndates and times in Pandas, 191\\ndatetime64, 189\\nfrequency codes, 195\\nindexing data by timestamps, 192\\nnative Python dates and times, 189\\noffsets, 196\\nPandas, 188-209\\nPandas data structures for, 192-194\\npd.date_range(), 193\\n528 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 546}, page_content='Python vs. Pandas, 188-192\\nresampling and converting frequencies,\\n197-199\\nrolling statistics, 201\\nSeattle bicycle counts example, 202-209\\ntime-shifts, 199-201\\ntyped arrays, 189\\nTimedelta type, 193\\nTimestamp type, 193\\ntimestamps, indexing data by, 192\\ntiming, of code, 12, 25-27\\ntransform() method, 167\\ntransforms\\nmodifying, 270-272\\ntext position and, 270-272\\ntriangulated surface plots, 295-298\\ntrigonometric functions, 54\\ntshift() function, 199-201\\ntwo-fold cross-validation, 361\\nU\\nufuncs (see universal functions)\\nunary ufuncs, 52\\nunderfitting, 364, 371\\nunderscore (_) shortcut, 15\\nuniversal functions (ufuncs), 50-58\\nabsolute value, 54\\nadvanced features, 56\\naggregates, 57\\narray arithmetic, 52\\nbasics, 51\\ncomparison operators as, 71-73\\nexponentials, 55\\nindex alignment, 116-118\\nindex preservation, 115\\nlogarithms, 55\\noperating on data in Pandas, 115-127\\noperations between DataFrame and Series,\\n118\\nouter products, 58\\nslowness of Python loops, 50\\nspecialized ufuncs, 56\\nspecifying output, 56\\ntrigonometric functions, 54\\nunstack() method, 130\\nunsupervised learning\\nclustering, 338-339, 353\\ndefined, 332\\ndimensionality reduction, 261, 340-342,\\n352, 355\\nPCA (see principal component analysis)\\nV\\nvalidation (see model validation)\\nvalidation curves, 366-370\\nvariables\\ndynamic typing, 34\\npassing to and from shell, 18\\nvariance, in bias–variance trade-off, 364-366\\nvectorized operations, 63\\nvectorized string operations, 178-188\\nbasics, 178\\nindicator variables, 183\\nmethods similar to Python string methods,\\n180\\nmethods using regular expressions, 181\\nrecipe database example, 184-188\\ntables of, 180-184\\nvectorized item access and slicing, 183\\nVega/Vega-Lite, 330\\nviolin plot, 327\\nviridis colormap, 258\\nVispy, 330\\nvisualization software (see Matplotlib) (see Sea‐\\nborn)\\nW\\nWickham, Hadley, 161\\nwildcard matching, 7\\nwireframe plot, 293\\nword counts, 377-378\\nIndex \\n| \\n529'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'source': '../data/pdf/Python_Datascience.pdf', 'file_path': '../data/pdf/Python_Datascience.pdf', 'total_pages': 548, 'format': 'PDF 1.5', 'title': 'Python Data Science Handbook', 'author': 'Jake VanderPlas', 'subject': '', 'keywords': '', 'moddate': '2017-03-20T23:10:55-08:00', 'trapped': '', 'modDate': \"D:20170320231055-08'00'\", 'creationDate': 'D:20161116185813Z', 'page': 547}, page_content='About the Author\\nJake VanderPlas is a long-time user and developer of the Python scientific stack. He\\ncurrently works as an interdisciplinary research director at the University of Wash‐\\nington, conducts his own astronomy research, and spends time advising and consult‐\\ning with local scientists from a wide range of fields.\\nColophon\\nThe animal on the cover of Python Data Science Handbook is a Mexican beaded lizard\\n(Heloderma horridum), a reptile found in Mexico and parts of Guatemala. It and the\\nGila monster (a close relative) are the only venomous lizards in the world. This ani‐\\nmal primarily feeds on eggs, however, so the venom is used as a defense mechanism.\\nWhen it feels threatened, the lizard will bite—and because it cannot release a large\\nquantity of venom at once, it firmly clamps its jaws and uses a chewing motion to\\nmove the toxin deeper into the wound. This bite and the aftereffects of the venom are\\nextremely painful, though rarely fatal to humans.\\nThe Greek word heloderma translates to “studded skin,” referring to the distinctive\\nbeaded texture of the reptile’s skin. These bumps are osteoderms, which each contain\\na small piece of bone and serve as protective armor. The Mexican beaded lizard is\\nblack with yellow patches and bands. It has a broad head and a thick tail that stores\\nfat to help the animal survive during the hot summer months when it is inactive. On\\naverage, these lizards are 22–36 inches long, and weigh around 1.8 pounds.\\nAs with most snakes and lizards, the tongue of the Mexican beaded lizard is its pri‐\\nmary sensory organ. It will flick it out repeatedly to gather scent particles from the\\nenvironment and detect prey (or, during mating season, a potential partner). When\\nthe forked tongue is retracted into the mouth, it touches the Jacobson’s organ, a patch\\nof sensory cells that identify various chemicals and pheromones.\\nThe beaded lizard’s venom contains enzymes that have been synthesized to help treat\\ndiabetes, and further pharmacological research is in progress. It is endangered by loss\\nof habitat, poaching for the pet trade, and being killed by locals who are simply afraid\\nof it. This animal is protected by legislation in both countries where it lives.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.5 (Build 24F74) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250714063433Z00'00'\", 'source': '../data/pdf/ManyaLOR.pdf', 'file_path': '../data/pdf/ManyaLOR.pdf', 'total_pages': 1, 'format': 'PDF 1.4', 'title': 'ManyaLOR', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20250714063433Z00'00'\", 'trapped': '', 'modDate': \"D:20250714063433Z00'00'\", 'creationDate': \"D:20250714063433Z00'00'\", 'page': 0}, page_content='To Whom It May Concern \\nI am pleased to recommend Manya Sharma (Roll No.RA2411026010261), a student of \\nthe SRM Institute of Science and Technology, who successfully completed a 6- week \\nSummer Research Internship under my mentorship as part of the BIT-SIPAR 2025 \\nprogram at Birla Institute of Technology, Mesra. During this period, Manya made \\nnotable contributions to one key projects: \\n“Speech based Gender and Voice disorder detection \\nusing Machine Learning. \" \\nShe displayed excellent technical aptitude, sincerity, and a proactive approach \\nthroughout the internship. Her work exceeded expectations and reflected a strong \\nfoundation in Signal processing and Machine Learning. \\nI am confident that Manya will excel in any academic or professional pursuit She \\nundertakes and strongly recommend her for higher studies, research roles, or \\ntechnical internships. \\nSincerely, \\nDr. Sitanshu Sekhar Sahu \\nAssociate Professor \\nDepartment of Electronics \\nand Communication Engineering \\nBirla Institute of Technology, Mesra'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 0}, page_content='CAR RENTAL\\nS E R V I C E'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 1}, page_content='OBJECTIVE\\nThe objective of this project is to develop a user-friendly car rental website that\\nenables customers to conveniently rent cars with or without drivers, based on\\ntheir preference. The platform will offer transparent cost calculations, pre-\\nbooking facilities, and doorstep delivery for enhanced convenience. In addition,\\nthe service will provide on-demand mechanics in case of breakdowns to ensure a\\nhassle-free travel experience. With features like secure online payments, flexible\\nbooking durations, and real-time availability checks, the system aims to deliver\\nreliable, affordable, and efficient car rental solutions that meet diverse customer\\nneeds.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 2}, page_content='SOFTWARE \\nSPECIFICATIONS\\nFRONT-END (CLIENT SIDE)\\nLANGUAGES & FRAMEWORKS:\\nHTML5, CSS3, JAVASCRIPT (FOR STRUCTURE,\\nSTYLING, AND INTERACTIVITY)\\nREACT.JS (FOR DYNAMIC AND RESPONSIVE\\nUSER INTERFACE)\\nBOOTSTRAP / TAILWIND CSS (FOR FASTER AND\\nCONSISTENT DESIGN)\\nFEATURES SUPPORTED:\\nUSER REGISTRATION/LOGIN\\nCAR BROWSING WITH FILTERS (WITH/WITHOUT\\nDRIVER, TYPE, PRICE, AVAILABILITY)\\nBOOKING AND PRE-BOOKING FORMS\\nCOST CALCULATOR FOR RENTALS\\nPAYMENT GATEWAY INTEGRATION\\n(CREDIT/DEBIT CARD, UPI, WALLETS)\\nMOBILE-RESPONSIVE DESIGN'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 3}, page_content='SOFTWARE \\nSPECIFICATIONS\\nBACK-END (SERVER SIDE)\\nLANGUAGES & FRAMEWORKS:\\nJAVA (JDBC)\\nRESTFUL APIS FOR COMMUNICATION BETWEEN\\nCLIENT AND SERVER\\nFEATURES SUPPORTED:\\nUSER AUTHENTICATION AND AUTHORIZATION\\nCAR AVAILABILITY MANAGEMENT\\nDRIVER ALLOCATION MODULE\\nMECHANIC REQUEST MANAGEMENT SYSTEM\\nBOOKING MANAGEMENT (CREATE, UPDATE,\\nCANCEL)\\nREAL-TIME COST CALCULATION LOGIC\\nNOTIFICATIONS (EMAIL/SMS FOR CONFIRMATIONS\\nAND ALERTS)'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 4}, page_content='SOFTWARE \\nSPECIFICATIONS\\nDATABASE\\nPREFERRED OPTIONS:\\nMYSQL  (FOR STRUCTURED RELATIONAL DATA\\nSUCH AS BOOKINGS, USERS, PAYMENTS)\\nDATA STORED:\\nUSER PROFILES\\nCAR INVENTORY (MODEL, AVAILABILITY, RATES)\\nDRIVERS AND MECHANICS INFORMATION\\nBOOKING AND TRANSACTION HISTORY\\nPAYMENT DETAILS (SECURED, TOKENIZED)\\nADDITIONAL TOOLS & SERVICES\\nPAYMENT GATEWAY: RAZORPAY / PAYPAL /\\nSTRIPE\\nMAPS & LOCATION SERVICES: GOOGLE MAPS API\\n(FOR PICK-UP, DROP-OFF, AND ROUTE DETAILS)\\nNOTIFICATION SERVICES: TWILIO / FIREBASE\\n(FOR SMS AND EMAIL ALERTS)'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 5}, page_content='USE CASE DIAGRAM\\nSALFORD & CO.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 6}, page_content='CLASS DIAGRAM\\nSALFORD & CO.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 7}, page_content='1. USER MANAGEMENT MODULE\\nHANDLES USER REGISTRATION, LOGIN, AND PROFILE MANAGEMENT.\\nSUPPORTS DIFFERENT ROLES: CUSTOMER, ADMIN, DRIVER, MECHANIC.\\nFEATURES: AUTHENTICATION (LOGIN/SIGNUP), AUTHORIZATION (ROLE-BASED ACCESS), PASSWORD RESET.\\n2. CAR MANAGEMENT MODULE\\nMANAGED BY ADMIN.\\nSTORES CAR DETAILS: MODEL, TYPE (SUV, SEDAN, HATCHBACK), AVAILABILITY, RENTAL RATE.\\nUPDATE AVAILABILITY STATUS WHEN A BOOKING IS MADE OR CANCELLED.\\n3. BOOKING & RESERVATION MODULE\\nALLOWS CUSTOMERS TO SEARCH AVAILABLE CARS (WITH/WITHOUT DRIVER).\\nFEATURES: PRE-BOOKING, BOOKING CONFIRMATION, CANCELLATION, EXTENSION OF BOOKING.\\nCOST CALCULATION (BASED ON DURATION, TYPE OF CAR, WITH/WITHOUT DRIVER).\\nMODULES'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 8}, page_content='4. DRIVER & MECHANIC MANAGEMENT MODULE\\nDRIVER ALLOCATION FOR BOOKINGS (OPTIONAL).\\nMECHANIC REQUEST SYSTEM IN CASE OF BREAKDOWNS.\\nFEATURES: ASSIGN DRIVER TO BOOKING, ASSIGN MECHANIC TO SERVICE REQUEST, UPDATE\\nSERVICE/RIDE STATUS.\\n5. PAYMENT & BILLING MODULE\\nINTEGRATES WITH PAYMENT GATEWAY (RAZORPAY/STRIPE/PAYPAL).\\nFEATURES: ONLINE PAYMENT, REFUNDS, BILLING INVOICES, SECURE TRANSACTIONS.\\nMAINTAINS TRANSACTION HISTORY.\\n6. NOTIFICATION & ALERTS MODULE\\nSENDS BOOKING CONFIRMATIONS, PAYMENT RECEIPTS, REMINDERS, AND MECHANIC/DRIVER UPDATES.\\nCHANNELS: EMAIL, SMS, PUSH NOTIFICATIONS.\\n7. ADMIN MANAGEMENT & REPORTING MODULE\\nADMIN DASHBOARD FOR MANAGING USERS, CARS, DRIVERS, MECHANICS, AND BOOKINGS.\\nGENERATES REPORTS (DAILY BOOKINGS, REVENUE, CAR USAGE, CUSTOMER FEEDBACK).\\nPROVIDES SYSTEM-WIDE MONITORING.\\nMODULES'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 9}, page_content='MEET OUR TEAM\\nAarushi Jhawar\\nRA2411026010258\\nAF1\\nShivansh A. Anand\\nRA2411026010276\\nAF1\\nPratistha Chaira\\nRA2411026010271\\nAF1\\nManya Sharma\\nRA2411026010261\\nAF1'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-10-04T10:39:16+00:00', 'source': '../data/pdf/APP Project.pdf', 'file_path': '../data/pdf/APP Project.pdf', 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'APP Project', 'author': 'Aarushi Jhawar', 'subject': '', 'keywords': 'DAGxWWTvI04,BAEcYEX6YSA,0', 'moddate': '2025-10-04T10:39:14+00:00', 'trapped': '', 'modDate': \"D:20251004103914+00'00'\", 'creationDate': \"D:20251004103916+00'00'\", 'page': 10}, page_content='THANKYOU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader , PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\", \n",
    "    glob=\"**/*.pdf\",  ##pattern to match files\n",
    "    loader_cls=PyMuPDFLoader,  ##loader class to use\n",
    "    # loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress= False\n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d85d7f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96476c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
